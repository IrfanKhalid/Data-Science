{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Muhammad Irfan Khalid - Information Retrieval Assignment 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/IrfanKhalid/Data-Science/blob/master/Muhammad_Irfan_Khalid_Information_Retrieval_Assignment_1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "mJtQoVB_HC6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Information and Text Retrieval Systems\n",
        "## Assignment 1"
      ]
    },
    {
      "metadata": {
        "id": "IX3f8hFLHC6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Muhammad Irfan Khalid\n",
        "#### MSDS:18036"
      ]
    },
    {
      "metadata": {
        "id": "HPwWLOCMHC6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We hope you have learned a lot in your classes, now is the time to implement and test your concepts. Since many of you are beginners in python programming puch of the assignment has already been written for you. Your task is to fill the missing lines in the functions, write some new functions etc. There is enough hints and guideline given that you can easily complete this assignment on your own.\n",
        "\n",
        "to give you an over all view, we are going to read some documents and compute tfidf for everyword in the document, then you will be asked to make different plots and run different querries."
      ]
    },
    {
      "metadata": {
        "id": "GGfk7_iZHC6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "let us first import the librarys we will be using."
      ]
    },
    {
      "metadata": {
        "id": "oPnTsfAzHC6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from string import punctuation as puncs\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SI43Lsi1HC61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us now define path of our dataset here, replace path string with the address of dataset in your PC. You can notice here that backward slash is replaced with double backward slashes. This is needed in windows based systems, if you are using *NIX* kernel machines then it may be extra step for you. Since the default address works just fine. Though it might be a good practice to use double backward slashes as yourcode will become robust of environment."
      ]
    },
    {
      "metadata": {
        "id": "ZGbxicZ5HC68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f90b0a82-3331-4198-cb40-fc05776e9e5d"
      },
      "cell_type": "code",
      "source": [
        "## define path of your dataset folder here:\n",
        "path = 'E:\\\\datasets\\\\IR\\\\subset\\\\'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKvk0YsdNwe7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9937a00-04df-4843-e2b7-a9259584b6e7"
      },
      "cell_type": "code",
      "source": [
        "!ls drive/\"My Drive\"/\"ACL txt.zip\"\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/ACL txt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7DUlqo6IOEQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip drive/\"My Drive\"/\"ACL txt.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PK7vCv89Opvi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path= \"ACL txt\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TI4186FrHC7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here you are making list of all the .txt extension files in your path folder. This list will allow you to read all the files in one loop without exclusively giving path to individual txt files."
      ]
    },
    {
      "metadata": {
        "id": "DbZETnY6HC7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## make a list of all txt files path\n",
        "\n",
        "txtFiles = []\n",
        "\n",
        "for fileName in os.listdir(path):\n",
        "    if fileName.endswith('.txt'):\n",
        "        txtFiles.append(path+fileName)\n",
        "#print(len(txtFiles),txtFiles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "820-1lQbHC7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "to help you further you are given with sample inputs at each stage to test your functions, so you have a sample text here that you may use to make sure you have written appropriate functions."
      ]
    },
    {
      "metadata": {
        "id": "mV5waRYMHC7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampleText=\"Association for Computational Linguistics 6 th Applied Natural Language Processing Conference Proceedings\\\n",
        "of the Conference April 29--May 4, 2000 Seattle, Washington, USA ANLP 2000-PREFACE 131 papers were submitted to ANLP-2000. \\\n",
        "46 were accepted for presentation at the conference. Papers came from 24 countries: fifty eight from the \\\n",
        "United States of America, eleven each from Germany and United Kingdom, nine from Canada, eight from Japan, \\\n",
        "four each from Italy and Spain, three each from France, Korea and Switzerland, two each from Australia, \\\n",
        "China, The Netherlands and Sweden and one each from Czech Republic, Denmark, Finland, Greece, India, Hong Kong, \\\n",
        "Malaysia, Norway, Russia and Taiwan. 40 papers were submitted from industry. 85 papers came from academia. \\\n",
        "2 papers were submitted from government organizations and four submissions were combined. The reviewing process \\\n",
        "was supported by a web-based reviewer interface developed by Elisha Kane at New Mexico State University's Computing \\\n",
        "Research Lab. Linda Fresques of CRL coordinated the refereeing process. I would like to express my gratitude \\\n",
        "to and appreciation of the Program Committee members responsible for the six areas: Lynn Carlson, Tools and \\\n",
        "Resources for Developing NLP Systems Subcommittee Eduard Hovy, Integrated NLP Systems Subcommittee Richard Kittredge, \\\n",
        "Multilingual Text Processing Subcommittee Ray Perrault, Spoken Language Systems Subcommittee Oliviero Stock, \\\n",
        "Monolingual Text Processing Systems Subcommittee John White, Evaluation Subcommittee The following colleagues \\\n",
        "did Doug Appelt Fabio Ciravegna Robert Dale Michael Elhadad Ralph G-rishman Lynette Hirschman Yuval Krymolowski \\\n",
        "Inderjeet Mani Zvi Marx Martha Palmer Harold Somers Toshiyuki Takezawa Takehito Utsuro Dekai Wu the bulk of the \\\n",
        "reviewing: Igor Boguslavsky Jim Cowie John Dowding Jim Glass Jan Haji~ Pierre IsabeIIe Alberto Lavelli Daniel Marcu \\\n",
        "David McDonald Owen Rainbow Tomek Strzalkowski Kathryn B. Taylor Pick Vossen R6mi Zajac David Carter Ido Dagan Andreas \\\n",
        "Eiscle Oren Glikman Donna Harman Tanya Korelsky Chin-Yew Lin Paul Martin Teruko Mitamura Norbert Reithinger Beth \\\n",
        "Sundheim Hans Uszkoreit Ralph Weischedel We believe that the quality of the papers selected is rather high and hope \\\n",
        "that the conference will be a success. Sergei Nirenburg Chair, Program Committee ANLP-2000 ANLPi \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MWy_8MF8HC78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is your first task, you need to complete this function in such a way that it takes a txt file as input argument and returns a list of words in your document.\n",
        "\n",
        "so if you enter sample text above, your output will be something like this\n",
        "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', 'Proceedingsof', 'the', 'Conference', 'April', '29--May', '4,', '2000', 'Seattle,', 'Washington,', 'USA', 'ANLP', '2000-PREFACE', '131', 'papers', 'were', 'submitted',.....]"
      ]
    },
    {
      "metadata": {
        "id": "BQ0HqD2FHC8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to read text and return list of words \n",
        "def wordList(doc):\n",
        "    \"\"\"\n",
        "    This function should take text which is a string object and return all \n",
        "    the list of words in it in the same sequece as they appear in the document\n",
        "    NOTE: you have to make sure your text has same case (lower/upper)\n",
        "    \"\"\"\n",
        "    sList=[]\n",
        "    for w in doc.split(\" \"):\n",
        "      sList.append(w)\n",
        "      #print(w)\n",
        "        \n",
        "    return sList\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqCOEujjHC8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eca969e8-b253-4995-eda3-54d43a384be7"
      },
      "cell_type": "code",
      "source": [
        "sList=wordList(sampleText)\n",
        "print(sList)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', 'Proceedingsof', 'the', 'Conference', 'April', '29--May', '4,', '2000', 'Seattle,', 'Washington,', 'USA', 'ANLP', '2000-PREFACE', '131', 'papers', 'were', 'submitted', 'to', 'ANLP-2000.', '46', 'were', 'accepted', 'for', 'presentation', 'at', 'the', 'conference.', 'Papers', 'came', 'from', '24', 'countries:', 'fifty', 'eight', 'from', 'the', 'United', 'States', 'of', 'America,', 'eleven', 'each', 'from', 'Germany', 'and', 'United', 'Kingdom,', 'nine', 'from', 'Canada,', 'eight', 'from', 'Japan,', 'four', 'each', 'from', 'Italy', 'and', 'Spain,', 'three', 'each', 'from', 'France,', 'Korea', 'and', 'Switzerland,', 'two', 'each', 'from', 'Australia,', 'China,', 'The', 'Netherlands', 'and', 'Sweden', 'and', 'one', 'each', 'from', 'Czech', 'Republic,', 'Denmark,', 'Finland,', 'Greece,', 'India,', 'Hong', 'Kong,', 'Malaysia,', 'Norway,', 'Russia', 'and', 'Taiwan.', '40', 'papers', 'were', 'submitted', 'from', 'industry.', '85', 'papers', 'came', 'from', 'academia.', '2', 'papers', 'were', 'submitted', 'from', 'government', 'organizations', 'and', 'four', 'submissions', 'were', 'combined.', 'The', 'reviewing', 'process', 'was', 'supported', 'by', 'a', 'web-based', 'reviewer', 'interface', 'developed', 'by', 'Elisha', 'Kane', 'at', 'New', 'Mexico', 'State', \"University's\", 'Computing', 'Research', 'Lab.', 'Linda', 'Fresques', 'of', 'CRL', 'coordinated', 'the', 'refereeing', 'process.', 'I', 'would', 'like', 'to', 'express', 'my', 'gratitude', 'to', 'and', 'appreciation', 'of', 'the', 'Program', 'Committee', 'members', 'responsible', 'for', 'the', 'six', 'areas:', 'Lynn', 'Carlson,', 'Tools', 'and', 'Resources', 'for', 'Developing', 'NLP', 'Systems', 'Subcommittee', 'Eduard', 'Hovy,', 'Integrated', 'NLP', 'Systems', 'Subcommittee', 'Richard', 'Kittredge,', 'Multilingual', 'Text', 'Processing', 'Subcommittee', 'Ray', 'Perrault,', 'Spoken', 'Language', 'Systems', 'Subcommittee', 'Oliviero', 'Stock,', 'Monolingual', 'Text', 'Processing', 'Systems', 'Subcommittee', 'John', 'White,', 'Evaluation', 'Subcommittee', 'The', 'following', 'colleagues', 'did', 'Doug', 'Appelt', 'Fabio', 'Ciravegna', 'Robert', 'Dale', 'Michael', 'Elhadad', 'Ralph', 'G-rishman', 'Lynette', 'Hirschman', 'Yuval', 'Krymolowski', 'Inderjeet', 'Mani', 'Zvi', 'Marx', 'Martha', 'Palmer', 'Harold', 'Somers', 'Toshiyuki', 'Takezawa', 'Takehito', 'Utsuro', 'Dekai', 'Wu', 'the', 'bulk', 'of', 'the', 'reviewing:', 'Igor', 'Boguslavsky', 'Jim', 'Cowie', 'John', 'Dowding', 'Jim', 'Glass', 'Jan', 'Haji~', 'Pierre', 'IsabeIIe', 'Alberto', 'Lavelli', 'Daniel', 'Marcu', 'David', 'McDonald', 'Owen', 'Rainbow', 'Tomek', 'Strzalkowski', 'Kathryn', 'B.', 'Taylor', 'Pick', 'Vossen', 'R6mi', 'Zajac', 'David', 'Carter', 'Ido', 'Dagan', 'Andreas', 'Eiscle', 'Oren', 'Glikman', 'Donna', 'Harman', 'Tanya', 'Korelsky', 'Chin-Yew', 'Lin', 'Paul', 'Martin', 'Teruko', 'Mitamura', 'Norbert', 'Reithinger', 'Beth', 'Sundheim', 'Hans', 'Uszkoreit', 'Ralph', 'Weischedel', 'We', 'believe', 'that', 'the', 'quality', 'of', 'the', 'papers', 'selected', 'is', 'rather', 'high', 'and', 'hope', 'that', 'the', 'conference', 'will', 'be', 'a', 'success.', 'Sergei', 'Nirenburg', 'Chair,', 'Program', 'Committee', 'ANLP-2000', 'ANLPi', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dEW49hbLHC8n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weldone! you must feel very happy! let us now implement each function below, you have been given sample inputs and your outputs  must be something corresponding to the examples given."
      ]
    },
    {
      "metadata": {
        "id": "Hj_BOe92HC8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampleList=[\"Association\",\"for\",\"Computational\",\"Linguistics\",\"6\",\"th\",\"Applied\",\n",
        "            \"Natural\",\"Language\",\"Processing\",\"Conference\"\"Proceedingsof\",\"the\",\n",
        "            \"Conference\",\"April\",\"29--May\",\"4,\",\"2000\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8uULE81vHC89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "complete this function"
      ]
    },
    {
      "metadata": {
        "id": "ECSbx_O6HC9C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###  function to remove puntuation marks from words\n",
        "# import string.maketrans as textfilter\n",
        "from string import punctuation as puncs\n",
        "def removePuncs(wordList):\n",
        "    \"\"\"\n",
        "    This function will take a list of words, iterate over the list and remove punctation marks that appear in the word\n",
        "    \"\"\"\n",
        "    slist=[]\n",
        "    print('punctuation marks are: ', puncs)\n",
        "    for w in wordList:\n",
        "        slist.append( w.translate(str.maketrans({key: None for key in puncs})))\n",
        "        #print(word)\n",
        "    return slist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fACWwlwOHC9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You are now required to write down exlplanation of the function above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "."
      ]
    },
    {
      "metadata": {
        "id": "iKwCbLpuHC9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d63fb456-84a2-404e-e9fd-0080471a237b"
      },
      "cell_type": "code",
      "source": [
        "sList=removePuncs(sampleList)\n",
        "print(sList)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "punctuation marks are:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'ConferenceProceedingsof', 'the', 'Conference', 'April', '29May', '4', '2000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "64pIo73PHC9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function below has already been implemented for you, understand it well. It might help you ahead!"
      ]
    },
    {
      "metadata": {
        "id": "BHF75kHYHC9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### function to calculate term frequency in the doc\n",
        "def termFrequencyInDoc(wordList):\n",
        "    \"\"\"\n",
        "    This function should take a list of words as input argument, and output a dictionary of words such that\n",
        "    each word that appears in the document is key in the dictionary and it's value is term frequency\n",
        "    \"\"\"\n",
        "    termFrequency_dic={}\n",
        "    for w in wordList:\n",
        "        if w in termFrequency_dic.keys():\n",
        "            termFrequency_dic[w]+=1\n",
        "        else:\n",
        "            termFrequency_dic[w]=1\n",
        "    return termFrequency_dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltjlbnm4HC98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0887a593-5577-4f7f-836d-bf395188abac"
      },
      "cell_type": "code",
      "source": [
        "termFrequencyInDoc(sList)\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2000': 1,\n",
              " '29May': 1,\n",
              " '4': 1,\n",
              " '6': 1,\n",
              " 'Applied': 1,\n",
              " 'April': 1,\n",
              " 'Association': 1,\n",
              " 'Computational': 1,\n",
              " 'Conference': 1,\n",
              " 'ConferenceProceedingsof': 1,\n",
              " 'Language': 1,\n",
              " 'Linguistics': 1,\n",
              " 'Natural': 1,\n",
              " 'Processing': 1,\n",
              " 'for': 1,\n",
              " 'th': 1,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "EXmp8T0uHC-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to calculate word Document frequency\n",
        "def wordDocFre(dicList):\n",
        "    \"\"\"\n",
        "    This function takes list of dictionary as input arguemnt, each dictionary in this list is the word that appears in the\n",
        "    given document as keys and the no. of times the word appears as value. This function should construct a dictionary which\n",
        "    has all the words that appear in the corpus as keys and no. of docs that contain this word as value\n",
        "    \"\"\"\n",
        "    vocan={}\n",
        "    for docDic in dicList:\n",
        "        for keys in docDic.keys():\n",
        "            if keys in vocan.keys():\n",
        "                vocan[keys]+=1\n",
        "                #print(\"hint: see termFrequencyInDoc() function\")\n",
        "            else:\n",
        "                vocan[keys]=1\n",
        "                #print(\"hint: see termFrequencyInDoc() function\")\n",
        "    return vocan\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ebQESoIlY86Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c888064b-6316-462c-c8b2-044ccc22d790"
      },
      "cell_type": "code",
      "source": [
        "lista= []\n",
        "lista.append(DF)\n",
        "WordDF=wordDocFre(lista)\n",
        "WordDF"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2000': 1,\n",
              " '29May': 1,\n",
              " '4': 1,\n",
              " '6': 1,\n",
              " 'Applied': 1,\n",
              " 'April': 1,\n",
              " 'Association': 1,\n",
              " 'Computational': 1,\n",
              " 'Conference': 1,\n",
              " 'ConferenceProceedingsof': 1,\n",
              " 'Language': 1,\n",
              " 'Linguistics': 1,\n",
              " 'Natural': 1,\n",
              " 'Processing': 1,\n",
              " 'for': 1,\n",
              " 'th': 1,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "e0o7Gas3HC_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## you should be well versed in syntax of creating functions by now !!\n",
        "## construct a function named inverseDocFre() that takes dictionary returned from wordDocFre functions above\n",
        "## and outputs inverse document frequency of each word. You can do it!\n",
        "\n",
        "def inverseDocFre(dicList,DocumentCount):\n",
        "  IMD={}\n",
        "  for keys in dicList:\n",
        "    IMD[keys]=math.log(DocumentCount+1/dicList[keys])\n",
        "  \n",
        "  return IMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8VK2JQJijU3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "25646820-d61d-4b70-d0fc-e78ea9691356"
      },
      "cell_type": "code",
      "source": [
        "inverseDocFre(WordDF,1)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2000': 0.6931471805599453,\n",
              " '29May': 0.6931471805599453,\n",
              " '4': 0.6931471805599453,\n",
              " '6': 0.6931471805599453,\n",
              " 'Applied': 0.6931471805599453,\n",
              " 'April': 0.6931471805599453,\n",
              " 'Association': 0.6931471805599453,\n",
              " 'Computational': 0.6931471805599453,\n",
              " 'Conference': 0.6931471805599453,\n",
              " 'ConferenceProceedingsof': 0.6931471805599453,\n",
              " 'Language': 0.6931471805599453,\n",
              " 'Linguistics': 0.6931471805599453,\n",
              " 'Natural': 0.6931471805599453,\n",
              " 'Processing': 0.6931471805599453,\n",
              " 'for': 0.6931471805599453,\n",
              " 'th': 0.6931471805599453,\n",
              " 'the': 0.6931471805599453}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "-IKy02nLHC_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### function to calculate tf-idf for everyword in doc\n",
        "## this is your main function which calls the function above in appropriate fasion \n",
        "def tfidf(docList):\n",
        "    \"\"\"\n",
        "    This function takes list of documents it calls the function wordList to split the document in list of words and remove\n",
        "    stopwords and punctuation marks from them, then calls termFrequencyInDoc() uses its output to create \n",
        "    dictionary of vocabulary using the function wordDocFre(), it then should call inverseDocFre() function.\n",
        "    it then outputs a list of dictionary, where each document corresponds to one dictionary, its words should be keys\n",
        "    values should be tf-idf score\n",
        "    \"\"\"\n",
        "    dicList=[]\n",
        "    for i in range(0,len(docList)):\n",
        "        dicList=wordList(i)\n",
        "        dicList=removePuncs(dicList)\n",
        "        dicList=termFrequencyInDoc(dicList)        \n",
        "        print(\"This is easy-peasy, unless you were sleeping in class!\")\n",
        "    \n",
        "    #wordDocFre()\n",
        "    #inverseDocFre(,len(docList))\n",
        "        \n",
        "    return tfidf_Dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFEjG8fWHC_o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### sort vocabulary according to IDF-against K (number of documents containing that word) values and plot using matplotlib.pyplot"
      ]
    },
    {
      "metadata": {
        "id": "cNzOK_b-HC_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "IvKkiNaPHC_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mow2odeKHDAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct a plot that shows how IDF-K relation changes as base of logarithm changes from 10 to -1."
      ]
    },
    {
      "metadata": {
        "id": "U1K1OAcfHDAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### yout code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zxxcf1vWHDAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct a plot Term Frequency weight transformation such as this one\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "BPkMQK-gHDAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdhcmYmXHDAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct plot of BM25 as shown here\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "iI0UaMuSHDAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyesJy4EHDA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### write code for SVM and run the following querry, your must show the top 5 documents ranked according to the score.\n",
        "    1 Text Mining\n",
        "    2 LDA\n",
        "    3 topic modelling\n",
        "    4 Natural language Processing\n",
        "    5 generative models"
      ]
    },
    {
      "metadata": {
        "id": "wTrhPWvGHDA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}