{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Muhammad Irfan Khalid - Information Retrieval Assignment 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/IrfanKhalid/Data-Science/blob/master/Copy_of_Muhammad_Irfan_Khalid_Information_Retrieval_Assignment_1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "mJtQoVB_HC6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Information and Text Retrieval Systems\n",
        "## Assignment 1"
      ]
    },
    {
      "metadata": {
        "id": "IX3f8hFLHC6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Muhammad Irfan Khalid\n",
        "#### MSDS:18036"
      ]
    },
    {
      "metadata": {
        "id": "HPwWLOCMHC6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We hope you have learned a lot in your classes, now is the time to implement and test your concepts. Since many of you are beginners in python programming puch of the assignment has already been written for you. Your task is to fill the missing lines in the functions, write some new functions etc. There is enough hints and guideline given that you can easily complete this assignment on your own.\n",
        "\n",
        "to give you an over all view, we are going to read some documents and compute tfidf for everyword in the document, then you will be asked to make different plots and run different querries."
      ]
    },
    {
      "metadata": {
        "id": "GGfk7_iZHC6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "let us first import the librarys we will be using."
      ]
    },
    {
      "metadata": {
        "id": "oPnTsfAzHC6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from string import punctuation as puncs\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SI43Lsi1HC61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us now define path of our dataset here, replace path string with the address of dataset in your PC. You can notice here that backward slash is replaced with double backward slashes. This is needed in windows based systems, if you are using *NIX* kernel machines then it may be extra step for you. Since the default address works just fine. Though it might be a good practice to use double backward slashes as yourcode will become robust of environment."
      ]
    },
    {
      "metadata": {
        "id": "ZGbxicZ5HC68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48cf0e40-5ccf-4d63-e83f-a1a9c56507a3"
      },
      "cell_type": "code",
      "source": [
        "## define path of your dataset folder here:\n",
        "path = 'E:\\\\datasets\\\\IR\\\\subset\\\\'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKvk0YsdNwe7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d2c182c-df50-4ebc-be7d-cd062916a6ef"
      },
      "cell_type": "code",
      "source": [
        "!ls drive/\"My Drive\"/\"ACL txt.zip\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'drive/My Drive/ACL txt.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7DUlqo6IOEQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bb12f693-cdbd-4c69-c884-74f76391808f"
      },
      "cell_type": "code",
      "source": [
        "!unzip drive/\"My Drive\"/\"ACL txt.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/ACL txt.zip\n",
            "replace ACL txt/A00-1000.pdf.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ACL txt/A00-1001.pdf.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PK7vCv89Opvi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path= \"ACL txt\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TI4186FrHC7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here you are making list of all the .txt extension files in your path folder. This list will allow you to read all the files in one loop without exclusively giving path to individual txt files."
      ]
    },
    {
      "metadata": {
        "id": "DbZETnY6HC7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## make a list of all txt files path\n",
        "\n",
        "txtFiles = []\n",
        "\n",
        "for fileName in os.listdir(path):\n",
        "    if fileName.endswith('.txt'):\n",
        "        txtFiles.append(path+\"/\"+fileName)\n",
        "#print(len(txtFiles),txtFiles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VAJ1cOzpYwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc4f3ad1-869b-48b6-83c5-6e697497508d"
      },
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "for \n",
        "with open(txtFiles[0], 'r',encoding='unicode_escape') as myfile:\n",
        "    data=myfile.read().replace('\\n', '')\n",
        "    \n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ITERATION, HABITUALITY AND VERB FORM SEMANTICS Frank v~n Eynoe. Un ivers i ty  oi Leuven Mar=a-Theresiastraat, 21 3000 Leuven Belgium ABSTRACT The veto forms are o4ten claimeo to convey two ;inds o+ information : I. w~et\\'~er the event Oeecr ibed in a sentence is present, past or future (= oe ic t i c  in fo rmat ion  2. whether the event described in a sentence is oresente~ as completed, going on, just  s ta r t ing  or being , in ished (= espectual information) \\\\ [ t  w i l l  be ~emonstrated in th i s  paper that one has t l  ado a rhino component to  the analysis of verb ~orm meanings, namely w~e~ner or no~ they e>press habltual i tv.  The 4ramewor~ 04 the analysis is mo~el- theoret ic  semantics. BACKGROUND The ana lw ls  of iteration and ha~ituality in th is  ~aper is part of a comprehensive semantic ar~Ivs is  of temporal expressions in natural kanguage. The research on th i s  topic is carr ied ob~ in ~he framework of EUROTRA, the MT project o4 the European Community. It is reporteo on e, tensi~,eiv in Van Eynde (lqBT). The or ig ina l  motive for s~arting th is  research ~as the fact that verbal tenses ann temporal a,:~:i l i~ries do not corresponO one-to-one in toe ienguages that EUROTRA has to deal with. Compare for in~taqce , i  EN ne has l ived in Copenhagen for 20 years ,Z, Dk nan nan boer i KmOenhavn i 20 ~r ~it~ tne l r  equlvaients in the fokiowlng languages \\x95 S~ DE er wonnt se i t  20 Jahren in Kopenhaoen ~i  FR i i  haDite ~ Copenhaoue Oepuis 20 ans ~5, NL n i j  woont sinds tw int lg  jaar in Kopenhagen When t rans la t ing  from Engl ieh or Danish to German, ~rench or Dutch the present perfect has to Pe replaceO by a simple present, Di&~ererces like these can be handled In one o; two eaVs either by Oef inlng complex mappings from source language to target  language forms in t rans fer  or Oy Oeflnlng mappings Oetween language spec i f i c  forms and In ter l ingua l  meanings in the monolingual components. SL ~orm ) TL form complex mmpc~ngs meanlng ) meanlng ,\\\\[ identity I mapping mapping SL ~orm TL form Because c* EUROTR~ s ao~erence to the pr inc ip le  o~ \"simPle t~ansfe ~\" it was quite OOVlOUS ~rom the s tar t  that the interlingual approa~ was the one to opt ~or. I t  w i l l ,  hence, be adopted in th l s  paper a~ wel l .  The paper consists  of t~ree parts.  In .ths flrst I will present a formal ism for the representat ion of time meanings,  together with mooel for the interpr~tat lon o~ those representat ions.  In the seconp th is  forma|ism wi i l  be extenOeO so that i t  can also Pe use~ for the ana\\\\]~slS o~ i terat ion an¢ habitL, alit~. Ann In th~ th i rd  pert I ~i~i show how the extendeo formalis~ can be l,~\\' ~or a~ !n\\\\[erl lnoua~ a~alvs is  O~ the ver~ fo~.S, THE CORE FORMALISM A Temporal Model T~e formal ist  tha: ~ill oe use: here has oee~ de~ineo e~pi lc i t iy  i~, van Eynce, aes TomDe Q Maes ~5) .  irk th !s  p~per i wi l l  on!y  g ive  a s~or~ In~ormai present~zion of tDe fo rmai lS~ CO%Cemtratlrlg On th~se partS ~ICh  will De neeOe~ In the se~onO pert ,  270 The model COnSIStS  of a set  o f  l inearly oroerep irtsrvals. An in terva l  ~s a continuous set of time points on the time axis : I , ) A la l im i te  i t  might consist of one moment o6 t lme : I For an~ pair of ~ntervaie one can Oeflne tne l r  i n te r lec t ion  as the set of tlme points which the\\'/ share :  i J In j  Tn~s set m~g~t also be empty, as in I J it I s ,  furthermore, possible to define some b~narv re la t ions  between in terva is ,  such ae l preceoence \\' , , ~} I be4ore O (..~i,J: O J a f te r  i ; .~ J , i ;  I ~dent~ty \" t  I ) I simui .\\\\] =( l ,a )  O I contain , , \\' ) I par t -o f  J c~I,J., d 0 contain I ~( J , I ;  I overlap \\' , ~ , ) I l e f tover  J ~<( I , J )  O J r ightover  I >>(O,l) T~ese re la t ions  are also useO in  Bruce (1972). A Format for Representation For the semantic analysis of the temporal expressions i w i l i  s ta r t  from tne assumption t~at every sentence can be analvseo in two parts : the temporal informal:on expressa~ by the tenses. aux i l i a r ies  anO ao~erbials on t~e one hano. anp ~as~o atempora! proposit ion on the ot~er hand. (b; the cat sat on t~e mat w~.i. for instance, be analyseo in a basic proposit ion \"the cat s i t  on the met\" and the ~n~ormation conveyed Dv the past tense. The re la t ion  between both is established in two steps : the basic proposit ion is f i r s t  relateO to the in terva l  ~or whicn i t  is said to be true,  the socalled time of event (E), and then th is  in terva l  is re lated to the time Of speech ~S) : 3 E : ,E ,S )  ~ AT(E,the cat s i t  on the mat)3 This formula states that \"the cat s i t  on the mat\" i~ true at an in terva l  E which precedes the tlme of speech S. Following Reichenbach (1947) I w i l l  furthermore assume that the relat ion between the time of event and the time of speech is mediated by a th i ro  kind o~ in terva l ,  namely the time of reference (R), So, instead of the simple ReI(E,S) we w111 have a composite ReI~E,R) & RefeR,S). Ne.t to th l s  re la t iona l  information tn~ tempore: expreeslons can also give spec i f ic  informatlon about the iocat lon or the length of the reie~ant in terva ls .  This is typ ica l ly  Oone by means o~ t:me aOverbiais, such as \"next year\",  \" in the  spring\\':, \" for  t~o years\", \" t i l l  Christmas\", etc. T~is in~ormatlon w i l l  be represented bY means o~ one-place preOicates over in tervs l s  : Freo(E) and Pred~) ,  ~n exception ~s tc be ~ade here +or the time o, speec~. ~nose precise locat ion or length is never spec~fleo b,  i inQo~stic means, bu\\\\[ rather bv pragmatic factors .  A possible way to reelect  tn~s- In the &oc~,allsm is to t res t  i t  as an unbouno var iab le .  In sum, the general format for the representat lon of temporal information looks as fol lows : 3 R,E \\\\[Rei(R.S) ~ Pred~R; & ~eI(E,R) ~ Prep(E) AT(E,p)\\\\] where p is a basic atemporal proposit ion An example ; ~T we wi l l  v ie l t  Moscow next year 3 P,E \\\\[,~R,S~ & ne~t vear~R) ~ =~E,R) & A ~ E,we v i s i t  Moscowi\\\\] 271 As i t  stanos th is  format is not adequate yet fo ~ the ~epresentatlon of sentences l i ke  (8~ last year they played chess every week (e~ he was always late The basic propoe~tions \"they play chess\" and \"he oe la te\"  do not hold for one par t i cu la r  time of event E, but rather for a set of in terva ls  wnicn are spread in time in some way speci f ieo by \"every week\" in (8) and \"always\" in (9). In the fo l lowing part I w i l l  introduce an exter.oeO formalism which can OeaI with these typos 04 iteration. THE EXTENDED FORMALISM Cycl ic Iteration Cyclic i te ra t ion  is marked by aoverpials  l i ke  \"ca i iv\" ,  \"every Monday\", \"each year\" ,  etc. In ~virk e.a. (1972) they are cal lao per iodic  frequency adverbials. For the analysis of these adverbials I f i r s t  IntroOuce the notion Crams time. The frame time ie the in terva l  which contains a l l  the instances of the event describeo in the basic propos i t ion.  In (8~ last  year they played chess every week t~e ~rame time is last  year. In the general forma~ t .e frame time occupies the same place as the time c~ event in non- i te ra t ive  in terpretat ions  (= the E - i~terva i~,  ~ext, I de~ine a set of d i s t inc t ,  non- overlapping subinterva ls  ~I~ which are a l l  part o+ the frame time. In (8~, these in terva ls  have a length of one week each. This gives the fo l lowing ,pre l iminary)  representat ion :5 R,E \\\\[ (R,S) & last  year(R~ & =(E,R) & I \\\\ [ c \\' i ,E )  & nI :~ & week,i) - - -x  AT~i,they play chess; \\\\ ] \\\\ ]  R S s imi lar  analysis can be found in Stump (198i where t~e aoverbial frequency ad~ectlvee (P) ere given the fo l lowing t ru th  condit ion : F~\\' is true in a world w at an interval I i~4 ,~m is true in w at non-over lapp ing subintervals  o$ i distriOuteO throughout i ~t perioOs of a speci$ieo length I .  \" \\\\[Stump 1981, 226\\\\] 5t~mp s i - interval cor responds  to my frame time. and his non-over lapp lng sub interva ls  correspond to my I - in terva ls .  As a representat ion of (B) th i s  formula is not su f f i c ient ,  though, since the instances of chess pla~ing do not have to take a whole week for  (B~ to. be true.  A more adequate paraphrase is to say that every week contained at least one subinterval  (e~ during which they played chess : , , o  l \\\\ [ c ( l ,E~ & nl=~ & week!i) - - -> e \\\\ [c~e, l )  & AT(e, they play cness)\\\\] \\\\ ]  An argument in favor of th i s  refinement is that languages have special means for speci fy ing the e- times. In ~I(\\' last year she arr ived at ~ c clock every da~ the aoverbia2 \"at eight o ¢ioc~\" denotes the locat lOn 04 t~e e- intervai  ; B Not ice tha~ the pro~art lee of e are constant within \\'the 4tame time : the aoverDial \"st eight o c lo t ! \"  spec i t ies  t~e time of each o¢ her arr lvals  cf last year. The general format for the representat ion of cycl ic i terat ion is, hence~ 3 R,E \\\\[ReI~R,S) & PreO~R~ & Rei(E,Ri & Pred~E} & I \\\\ [c( l .E~ & ni=O & P(1) - - - .  e ~:~e~I~ ~ M(e) - - -2  AT~e,p; \\\\ ] \\\\ ] \\\\ ]  where P is replacec ov the head o4 a per iooic  ~requencv aoverbial ,  spec i fy ing the l ocat ion  or the  iengtn o~ I Io -opt lona}l~i  replaced ov ~ ti~,a advero~6i, sPecifYin~ the length cr the igcatlon C.f e ~n im\\\\ [ ,o r ta r~t  property of th i s  format is it ~. cha in - l i ke  s t ructure  : 272 R is oef~neo with respect to S : ReI~R,S~ E as defined with respect to R : ReI(E,R~ I is defineo w~th respect to E : ~( I ,E)  and e is oefineo with respect to I : c(e. I~ As it stands, the format does not provioe any means for stat ing a d i rect  re la t ionsh ip  between the in terva ls  inside the frame time ~I and e~ ano the in terva ls  outside the frame time (S anO R~. As consequence, the formal~sm predicts  that temporal adverbials w~ich are in the scope o~ a frequency adverbial (:  the e-speci f iers~ cannot refer ba~K to the speech t~me or the eeference time: * Rei(e,S) and * Rel(e,R~, gooo p;ece of evidence for th is  hypothesis ~s pr~ioed  by the WHEN-aoveroiais. In general one can d is t inguish two kinde of those adverbials : t~e re la t iona l  ones, which express a re la t ion  Oetween the reference time and the speech time, such as \"~esterday\" a\\'nd \"tomorrow\", and the non- re la t iona l  ones, which ident i fy  the locat ion o~ an :nterva l  without any reference to the speech t~me, suc~ as \"between 8 and 9\" and \"at two o c lock\".  The in terest ing  thing now is t~at only the la t te r  adverbials can occur in the scope of a frequency adverbia l .  Compare :iI~ she arr ived every day between 8 anq 9 e *(12~ she arr ived every day yesterday e The fact that the re la t iona l  WHEN-adverbials cannot occur in the scope of a frequency aoverb~al prcviOes some pos i t ive  evioence ~or not inciuoln§ d i rect  re la t ions  between e ano S in the formal~em. The cha ln - l i ke  structure of the representat ion format Is ,  hence, i~ngu is t i ca l ly  motivated. Temporal Quantifiers The format Oeveloped for the analysis of cvc l ic  i te ra t lon  can also be useo for the analysis o~ the temporal ~uantifier$, such as \"miway~\", \"scmetlmes\", \"never\",  \"seldom\" ano \"o f ten\" .  The ~rmet ion  they proviOe is less spec i f i c  than the ona p~ovioed by the period frequency aOverb~ais, ar, d t~s  should be ref iecteO in the i r  representat ion.  As a s tar t ing  point I take the general ~ormat ~or the representation o~ sentences w~th a periodic frequency adverbial : . . .  ~ i \\\\[c( l .E~ & nI=~ & P( l i  - - -> 3 e \\\\ [c ie , l i  &Mie) & AT(e,p) \\\\ ] \\\\ ]  For a semantlc analysis of the temporal quant i f le rs  th i s  format has to be generalieeo. The most important change is the replacement of the universal ouant i ; ie r  bv a var iable : ... Q I C=(I,E) . . .  where Q can be any of the fo l lo~ing  quant i f le rs  always 3 eometimes -3 never  Few selOom, rare ly ,  now ano then Many o~ten, f requent ly  Most usual lv ,  mostly, general ly .=,is s ix fo ld  dzvis:on is taken beer from Lewis ~1975). This analysis account~ for the anomaly of sentences l i ke  o ,13} we sometimes played chess every wee~ 3 ? (141 they often met every month Many (15p we always plaveO chess every week 9 These sentences are eemant lce i iy  anomalous oecauee t~e sa~e ~ino o* In*ormation. namely the v~iue o~ ~. is epec~lec twice. This leaps to :~cons~etenc~ ~ ~13) and (14} where the Q- ve~ia~ie IB s~l~ to be both universal anO non- ~r;vers~i at tme same time, and i t  leaos to pleonasm in (15~ where the Q-variable is twice sago to Oe u~,iversal. The ne, t question is whethe,\" thP temporal quant!~iers introduce any ext ra -conq i t ions  on those In terva ls ,  ouch ms c~l ,E) ,  ~I=~ and P~i~. The f~rst  of t~ese condit ions appears to Pe relevant : the temporal quant i f ie rs  are ~ndeeo interpreteO wi~ respect to some given frame time. In ~x he was al~ays late \"a lways\"  ooesnot oenote AL~ possible in terva ls .  but onl~ a l l  possibie in terva ls  ~n the past. The conoit~on that the subintervals  may no~ overlap does not seem to be re levant ,  though, in (16, quaOratlc equations are aIweye s~mple 273 the Instances for whlon \"quadratic equations Pe ~imple\" are true are no~ temporal ly ordereo at all. it, is m~gnt indicate, Ov the way, that the i- objects ~re not necessari ly in terva ls ,  but rather cases or occasions wnlcn can but need no: be given m temporal in terpretat ion  (of.  Lewis 1975i. The th i rd  conOition concerns the propert ies of t~e I -ob jec ts .  In the case of the per iodic \\x95 ,equency aOverblals the relevant propert ies  concern the locat ion or the length of the in terva l .  In the case of the temporal guant i f le rs  one could think of speci fy ing a relevance conoiticn~ for a sentence l i ke  ~ he was always late ones not mea= that he was late at any possible occasion in the past, Put rather that he was late on al l  occasions on which his being late or timel~ could nave mattered. in Aqv~st, Hoepelman & Rohrer (1980) one can ~ind a proposal to incorporate th is  information in the semantic representat ion,  but I w i l l  not adopt t~is  proposal here, since the condit ions o~ the ,non)relevance of the occas ions are typica l iv  determined O~ pragmatic factors ,  in ~:\" he always leaves o~-~ twelve the relevant occasions (1) could just  as well oe all occasions on which he leaves as a l l  occasions on Wnlch ne leaves for work as a!i occas ions on ~hish he leaves for watching the home game of nls ~avour l te  footOaii  team. As a resu l t  of the foregoing reductions ar~o changes the general format for analysing tempo, al cuant i f ie rs  looks as fo l lows : 3 ~,E \\\\[ReI(R,S) & Pred(R) & ReI(E,R) & F\\'reoiE) & Q I \\\\[c( l~E) - - ->/& 3 e \\\\ [c~e, I ;  & M~ei & AT(e ,p! \\\\ ] \\\\ ] \\\\ ]  ,here O is replaced by any of {V, 3, \"3, Most, Few, Many} M is replaced by some time adverbial which spec i f ies  the locat ion or the length of e ( i f  there is anv~ Habitual i ty The sentences oiscusse~ so far al l  contain an exp l i c i t  ind icat ion  of i te ra t ion .  !he presence of SL~Ch an Ind lCat lOn  I s ,  however, not necessary for der iv ing an i te ra t ive  in terpretat ion .  Take, ÷or instance, (in~ he leaves at twel~e This sentence cannot only mean tnat he w i l l  leave at twelve, but also that he has the habit  of leaving ~-* twelve. in the representat ion of in terpretat ion  the time adverbial spec i f i ss  the t~me of reference : the former \"at twelve\\' 3 ~,E \\\\[ :(R,S) & at twelve(R) & :(E,R) & AT(E. he leave~\\\\] E S R in the representat ion of tne habitual i~terpretat ion~ on tne other hand, tne time adverolal shouls be tal~en to speci fy the mult ip le e-tlme, for the sentence Ooes no~ report on one o~ his ieavzngs at twelve, out rather on several of socn :ea,es. As a representat ion of th i s  in terpretat ion  I propose : ~,=st ; \\\\[--~I.fJ ---,  _ e Lc~e,I) ,~ at twelve~e & AT~e, he leave, l\\\\] R (19~ he leaves at twelve is t -eaten as synonymous with (20, he usual ly leaves at twelve I f  th i s  is fe l t  to be undesirable,  one cam introGuce a special quant i f ie r  for marking hab i tua i i tv ,  but at th i s  moment ~ do not see an~ reason fo r  SUCh a move. 274 The general format for the representation of habitual ~nterpretat~one Is,  hence, 3 R,E \\\\[ReI(R,S) ~ Pred(R) & Rel~E.R> ~ Preo(E) Most i \\\\[c~I,E) ---> 3 e \\\\[c~e,I) ~ Pred(e) & AT~e,p)\\\\]\\\\]\\\\] The Assignment of Representations to Sentences On t~e basis of the given analyses one O:stinguls~ three kinds of sentence meanings : no i terat ion no ~ i \\\\[ l /per iod ic  cyclic i~eration \\\\ Q I \\\\[ \\\\] \\\\ indef i - , te  can is specified F is not specified Q is any of {~,3, \"3,~ost,Manv,Few} The assl~nment of these meanings to particuiar sentences is fa i r l y  straightforward when the sentence contains a frequency adverOial or a temporal quanti f ier ,  but i f  there is none o~ those~ then the sentence is amOiguous Oetween a non-lterative and an habitual interpretation ~cf. the two interpretations of \"he leaves at tweive\"~. It, practice there are some oisambiguatlng ~. I* the basic proposition (p) denotes a state, ~r. er, the sentence can not have an habitual ir~erpreta~ior~ Compare :i;~ ne leaves at twelve ,21 ne is in je i !  ~1~ can be interpreted as meaning that he has the naPlt of leaving at twelve, bu~ (21i cannot Oe interpreted ms meaning that he has the habit of bel=g in jail. ~, Certain verb forms can biock the Oerivation o~ one of t~s two possiole interpretations. Compare ~2~ he is drinking coffee 12\\\\]) he drinks coffee (22, can Oenote a single instance of drinking as wei\" as a recent habit of him to drink:: coffee ~cf. in the sense of \"he is. drinklng coffee nowadays\"). (2; , ,  on the other hand, can only denote a habit; i t  cmnnot be used to report on a single instance o~ drinking. This demonstrates the need to distingulsn oi4ferent types of verb forms : the ones that wi l l  aiways e l i c i t  an habitual interpretat ion,  the ones that block the derivation o~ an habitual interpretat ion,  and the ones that admit both kinds of interpretmtions. The firs~ are unequivocall~ \\\\[+habitual\\\\],  the second C-habitual\\\\[ and the last wi l l  be given the feature \\\\ [+/-habitual \\\\ ] .  THE INTERLINSUAL ANALYSIS OF THE VERB FORMS The Meanings of the Verb Forme In the previous parts i have presente¢ a formaliem for the representation of temporal information in sentences. This formallsm is especially deeigned for the anaiyeis of natural language, but not for the analysis o~ any particular natural language, such ae English, Dutch or Kiswahili. I ts  mmin purpose is to provide a conceptuall~ well-defined language for de;ining and comparln~ the ~eanings of te~poral expressions in di f ferent natural l~nguagee. In order to serve this purpose i t  is not s~ff ic lent ~o have a formalism, ~nouon. What is also needed is a general specif ication o4 now the semmntic representations relate to tnelr imnguage specific co~nterpmrts, i .e .  the tenses, the temporal aux l i : r ies  and t~e time aoveroials. The ÷orme~ two wi i l  furcner de caileO veto forms, For c { \\' is~ ~n~, those verb forms are summec up in the followlng rL~ie : Vero form ---~. \\\\[+/-F\\'ast\\\\] (wi11+ir.f) (have+EP) ({be+iNS to+fri l l )  ~e going T, hi_\\'¢ rule ylelds 24 (=2x2x2x3) \\'verb forms. Their role in the semantic interpretation of sentences .:an easily de expressed in terms of the given formalism. They specify i .  the relat ion Petween reference time anO speech time : ~eI(R,S) (= oeict ic information) 2. the relation between event time and reference time : ReI,E,R) (= aspectual information~ 5. whether the sentence has an habitual and!or ; non-iterative ~nterpretaZlon 275 The meaning of a verb form can, hence, be representeO as a t r ip le  ~x,y,z> where x and v are substi~uteO for one of the possible dinar, -elations oe~ween interva ls ,  and where z is one of the three poesible habituali~y values. The aame verb  ~orm can,  of  course ,  have oifferent meanings and will, hence, Oe assoclateO ~th  a set of such t r ip les .  The detai ls o~ this association have  been discL:ssed elsewhere~ at \\\\]east for the x ann ¥ values ~cf. Van Eynde, des Tombe & Maes 1985i. In tnls paper I wi l l  only discuss the z values in some deta i l .  The Mabituality Value A good start ing point for demonstrating the relevance of the hab i tua l i ty  value is provided by the following i i s t  of sentences. They are taken from hess (1985). ~)  a text editor makes modifications to a text f i l e  ~25) a text editor is makin~ modifications to a text f i l e  ~26) a text editor made mooiflcatione to a text f i l e  \\x95 27~ a text editor has made modifications to a text f i l e  In L24) i t  is said \"that a text editor ma~es modifications to a text f i l e  in general, almost by Oef in i t ion.  We might read this  sentence in a re~erence manual\" (Hess 1985, 10). In (25-27), on the other hand, i t  is said \"that there i s ,  or was, a case of a text editor mankind modifications to a text f i l e .  These remarks might ~e made by a system operator, watcnlng ~is screen\\' (lb.). Hess concludes from these observations that the quant i f ier  of the subject is universal in (24) and e~:isten~ial in (25-27), However~ th is  conclusion does not foliow automatically. In terms of the formalism presented in this  paper one could sa~ that (24) has an habitual in terpretat ion ,  whereas the other sentences have a non- i terat ive interpretat ion,  In the former case the ex is tent ia l  quant i f ier  of the subject wi l l  be in the scope o~ the Most-quantif ier,  whereas in the la t te r  case i t  wl i i  not be in the scope of any non-existential  quant i f ie r ,  and this accounts for the difference in interpretat ion without havinq to postulate two possiole meanings for the indef in i te  a r t i c le .  Hess s examples are useful in this  context, t~ough, because they clear ly i l l us t ra te  the roie of the vend for~ in the interpretat ion.  Since i t  is the only variable part in the sentences, the ~ifferences in interpretat ion can only be ascribeo to them, more spec i f i ca l ly  to their hab l tua i i ty  value. ;or the assignment of an hab l tua l i ty  value to a given verb form one has to test whether i t  can or cannot e l i c i t  an habitual interpretat ion in some given context. In testing this  one should i. always use sentences with a non-stative basic proposit ion, for i~ the la t te r  is stat ive the sentence can never be habitual (of. supra) ; 2. pay attention to the other inter i lngual  values of the verb form. The English simple present. for instance, is uneouivocally \\\\[+habitual\\\\]  in i t s  sim~Itaneoue meaning, but in i t s  posterior meaning i t  can be \\\\ [ -nao i tua l \\\\ ]  too (of. the non- i terat ive interpretat ion of \"he leaves at twelve\"~. The relevance of the \\\\ [+/ -Hab i tua l i tv \\\\ ] -  d is t inct ion  has so far only been demonstrated from a monolln~ual semantic point of view. I t  is ,  however, possible to give some translat ional  evidence for this o let inct lo= as well. The relevant cas~s are tne ones where the corresponding verb forms have  Oi~ferent hab i :~a l l ty  values. A good example of th is  is the translat ion of the Dutch simple present in En~ilsh. The Dutch simple present can be both habitua} and ~on-hacitual in It~ simultaneous meaning : 28; hi~ o,\\'inxt aileen whisky <simui,y,~haOitual~ \"he drinks only whisky\\' 29, Liji~, h i j  dr!nit k~4 ie  . ,s imul, / , -habltuai> \"look, he Orinks co,fee\" The English simple present, on the other hand, s always habitual in i t s  simultaneous meaning unless in sentences Oee:ribing states, of course (~0~ he only drinks whisky <slmui,y,+habitua~. *~31) iooi:, he drinks ~o~fee <s imul ,y , -ha~i tua l  Pot the expression of slmul~aneous non- i te ra t iv i ty  one has to use She progressive : 32) look, De is crinking coffee As a conseoue~ce. ~e mapping of (29) to ~32) in~ol~es a non-~riviai  tense replacement, and i t  i l  o~e of the merits o~ the given formaliem that i t  car handle this i r  an lnter i ingual  way. 276 REFERENCES ~qviet Lennart, Hoepelman Jaap & Rohre? Ch~-istiah (19BO~, \"Adverbs of frequency:, in Rohr~r (ed.), Time~ tense and quantifiere. Niemever. T~oingen, 1-17. Bruce Bertram (1972), \"A model for temporal reference and its application in a question- answering program\", in Ar t i f i c ia l  Intelligence 3, 1-25. Hess M~chael (I~B5), \"How does natural language quantify ?\". in Proceedings of the Secono Cmnferenc~ of the European Cnapter of the ACL, Geneva, B-15. Lemis David ~1975~ \"Adverbs of ouantification\", in Keenan (ed.), Formal semantics of natural language. Cambrioge University Press, Cambridge, ~-15. ~u!rk  Randolph, Greenbaum Sioney, Leech Geoffrey Svartvik Jan (1972J, A grammar of contemporar~ English. Longman~ London. Relcnenoach Hans (1947~ Elements of symDollc logic. University of California Press, Berkeley. Stump Gregor. ~19BI~, \"The Interpretation of frequenc~ ~ adjectives\",  In L~nguist ics ant ~nilosophy 4. 221-257. Van Eynde Frank~ des Tombe Louis & Maes Fons ~1985)~ \"The specification of time meaning ~or machine translation\", In Proceedings of the Second Conference of the European chapter of the ACL, Geneva, 35-40. Van Eynde Frank (1987), Time. A unified theory of tense, asoect and Aktionsart, An internal Eurotra Ooeument (78 pages). Leuven. 277 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "820-1lQbHC7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "to help you further you are given with sample inputs at each stage to test your functions, so you have a sample text here that you may use to make sure you have written appropriate functions."
      ]
    },
    {
      "metadata": {
        "id": "mV5waRYMHC7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampleText=\"Association for Computational Linguistics 6 th Applied Natural Language Processing Conference Proceedings\\\n",
        "of the Conference April 29--May 4, 2000 Seattle, Washington, USA ANLP 2000-PREFACE 131 papers were submitted to ANLP-2000. \\\n",
        "46 were accepted for presentation at the conference. Papers came from 24 countries: fifty eight from the \\\n",
        "United States of America, eleven each from Germany and United Kingdom, nine from Canada, eight from Japan, \\\n",
        "four each from Italy and Spain, three each from France, Korea and Switzerland, two each from Australia, \\\n",
        "China, The Netherlands and Sweden and one each from Czech Republic, Denmark, Finland, Greece, India, Hong Kong, \\\n",
        "Malaysia, Norway, Russia and Taiwan. 40 papers were submitted from industry. 85 papers came from academia. \\\n",
        "2 papers were submitted from government organizations and four submissions were combined. The reviewing process \\\n",
        "was supported by a web-based reviewer interface developed by Elisha Kane at New Mexico State University's Computing \\\n",
        "Research Lab. Linda Fresques of CRL coordinated the refereeing process. I would like to express my gratitude \\\n",
        "to and appreciation of the Program Committee members responsible for the six areas: Lynn Carlson, Tools and \\\n",
        "Resources for Developing NLP Systems Subcommittee Eduard Hovy, Integrated NLP Systems Subcommittee Richard Kittredge, \\\n",
        "Multilingual Text Processing Subcommittee Ray Perrault, Spoken Language Systems Subcommittee Oliviero Stock, \\\n",
        "Monolingual Text Processing Systems Subcommittee John White, Evaluation Subcommittee The following colleagues \\\n",
        "did Doug Appelt Fabio Ciravegna Robert Dale Michael Elhadad Ralph G-rishman Lynette Hirschman Yuval Krymolowski \\\n",
        "Inderjeet Mani Zvi Marx Martha Palmer Harold Somers Toshiyuki Takezawa Takehito Utsuro Dekai Wu the bulk of the \\\n",
        "reviewing: Igor Boguslavsky Jim Cowie John Dowding Jim Glass Jan Haji~ Pierre IsabeIIe Alberto Lavelli Daniel Marcu \\\n",
        "David McDonald Owen Rainbow Tomek Strzalkowski Kathryn B. Taylor Pick Vossen R6mi Zajac David Carter Ido Dagan Andreas \\\n",
        "Eiscle Oren Glikman Donna Harman Tanya Korelsky Chin-Yew Lin Paul Martin Teruko Mitamura Norbert Reithinger Beth \\\n",
        "Sundheim Hans Uszkoreit Ralph Weischedel We believe that the quality of the papers selected is rather high and hope \\\n",
        "that the conference will be a success. Sergei Nirenburg Chair, Program Committee ANLP-2000 ANLPi \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MWy_8MF8HC78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is your first task, you need to complete this function in such a way that it takes a txt file as input argument and returns a list of words in your document.\n",
        "\n",
        "so if you enter sample text above, your output will be something like this\n",
        "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', 'Proceedingsof', 'the', 'Conference', 'April', '29--May', '4,', '2000', 'Seattle,', 'Washington,', 'USA', 'ANLP', '2000-PREFACE', '131', 'papers', 'were', 'submitted',.....]"
      ]
    },
    {
      "metadata": {
        "id": "BQ0HqD2FHC8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to read text and return list of words \n",
        "def wordList(doc):\n",
        "    \"\"\"\n",
        "    This function should take text which is a string object and return all \n",
        "    the list of words in it in the same sequece as they appear in the document\n",
        "    NOTE: you have to make sure your text has same case (lower/upper)\n",
        "    \"\"\"\n",
        "    sList=[]\n",
        "    for w in doc.split(\" \"):\n",
        "      sList.append(w)\n",
        "      #print(w)\n",
        "        \n",
        "    return sList\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqCOEujjHC8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eca969e8-b253-4995-eda3-54d43a384be7"
      },
      "cell_type": "code",
      "source": [
        "sList=wordList(sampleText)\n",
        "print(sList)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'Conference', 'Proceedingsof', 'the', 'Conference', 'April', '29--May', '4,', '2000', 'Seattle,', 'Washington,', 'USA', 'ANLP', '2000-PREFACE', '131', 'papers', 'were', 'submitted', 'to', 'ANLP-2000.', '46', 'were', 'accepted', 'for', 'presentation', 'at', 'the', 'conference.', 'Papers', 'came', 'from', '24', 'countries:', 'fifty', 'eight', 'from', 'the', 'United', 'States', 'of', 'America,', 'eleven', 'each', 'from', 'Germany', 'and', 'United', 'Kingdom,', 'nine', 'from', 'Canada,', 'eight', 'from', 'Japan,', 'four', 'each', 'from', 'Italy', 'and', 'Spain,', 'three', 'each', 'from', 'France,', 'Korea', 'and', 'Switzerland,', 'two', 'each', 'from', 'Australia,', 'China,', 'The', 'Netherlands', 'and', 'Sweden', 'and', 'one', 'each', 'from', 'Czech', 'Republic,', 'Denmark,', 'Finland,', 'Greece,', 'India,', 'Hong', 'Kong,', 'Malaysia,', 'Norway,', 'Russia', 'and', 'Taiwan.', '40', 'papers', 'were', 'submitted', 'from', 'industry.', '85', 'papers', 'came', 'from', 'academia.', '2', 'papers', 'were', 'submitted', 'from', 'government', 'organizations', 'and', 'four', 'submissions', 'were', 'combined.', 'The', 'reviewing', 'process', 'was', 'supported', 'by', 'a', 'web-based', 'reviewer', 'interface', 'developed', 'by', 'Elisha', 'Kane', 'at', 'New', 'Mexico', 'State', \"University's\", 'Computing', 'Research', 'Lab.', 'Linda', 'Fresques', 'of', 'CRL', 'coordinated', 'the', 'refereeing', 'process.', 'I', 'would', 'like', 'to', 'express', 'my', 'gratitude', 'to', 'and', 'appreciation', 'of', 'the', 'Program', 'Committee', 'members', 'responsible', 'for', 'the', 'six', 'areas:', 'Lynn', 'Carlson,', 'Tools', 'and', 'Resources', 'for', 'Developing', 'NLP', 'Systems', 'Subcommittee', 'Eduard', 'Hovy,', 'Integrated', 'NLP', 'Systems', 'Subcommittee', 'Richard', 'Kittredge,', 'Multilingual', 'Text', 'Processing', 'Subcommittee', 'Ray', 'Perrault,', 'Spoken', 'Language', 'Systems', 'Subcommittee', 'Oliviero', 'Stock,', 'Monolingual', 'Text', 'Processing', 'Systems', 'Subcommittee', 'John', 'White,', 'Evaluation', 'Subcommittee', 'The', 'following', 'colleagues', 'did', 'Doug', 'Appelt', 'Fabio', 'Ciravegna', 'Robert', 'Dale', 'Michael', 'Elhadad', 'Ralph', 'G-rishman', 'Lynette', 'Hirschman', 'Yuval', 'Krymolowski', 'Inderjeet', 'Mani', 'Zvi', 'Marx', 'Martha', 'Palmer', 'Harold', 'Somers', 'Toshiyuki', 'Takezawa', 'Takehito', 'Utsuro', 'Dekai', 'Wu', 'the', 'bulk', 'of', 'the', 'reviewing:', 'Igor', 'Boguslavsky', 'Jim', 'Cowie', 'John', 'Dowding', 'Jim', 'Glass', 'Jan', 'Haji~', 'Pierre', 'IsabeIIe', 'Alberto', 'Lavelli', 'Daniel', 'Marcu', 'David', 'McDonald', 'Owen', 'Rainbow', 'Tomek', 'Strzalkowski', 'Kathryn', 'B.', 'Taylor', 'Pick', 'Vossen', 'R6mi', 'Zajac', 'David', 'Carter', 'Ido', 'Dagan', 'Andreas', 'Eiscle', 'Oren', 'Glikman', 'Donna', 'Harman', 'Tanya', 'Korelsky', 'Chin-Yew', 'Lin', 'Paul', 'Martin', 'Teruko', 'Mitamura', 'Norbert', 'Reithinger', 'Beth', 'Sundheim', 'Hans', 'Uszkoreit', 'Ralph', 'Weischedel', 'We', 'believe', 'that', 'the', 'quality', 'of', 'the', 'papers', 'selected', 'is', 'rather', 'high', 'and', 'hope', 'that', 'the', 'conference', 'will', 'be', 'a', 'success.', 'Sergei', 'Nirenburg', 'Chair,', 'Program', 'Committee', 'ANLP-2000', 'ANLPi', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dEW49hbLHC8n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weldone! you must feel very happy! let us now implement each function below, you have been given sample inputs and your outputs  must be something corresponding to the examples given."
      ]
    },
    {
      "metadata": {
        "id": "Hj_BOe92HC8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampleList=[\"Association\",\"for\",\"Computational\",\"Linguistics\",\"6\",\"th\",\"Applied\",\n",
        "            \"Natural\",\"Language\",\"Processing\",\"Conference\"\"Proceedingsof\",\"the\",\n",
        "            \"Conference\",\"April\",\"29--May\",\"4,\",\"2000\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8uULE81vHC89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "complete this function"
      ]
    },
    {
      "metadata": {
        "id": "ECSbx_O6HC9C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###  function to remove puntuation marks from words\n",
        "# import string.maketrans as textfilter\n",
        "from string import punctuation as puncs\n",
        "def removePuncs(wordList):\n",
        "    \"\"\"\n",
        "    This function will take a list of words, iterate over the list and remove punctation marks that appear in the word\n",
        "    \"\"\"\n",
        "    slist=[]\n",
        "    #print('punctuation marks are: ', puncs)\n",
        "    for w in wordList:\n",
        "        slist.append( w.translate(str.maketrans({key: None for key in puncs})))\n",
        "        #print(word)\n",
        "    return slist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fACWwlwOHC9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You are now required to write down exlplanation of the function above.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "."
      ]
    },
    {
      "metadata": {
        "id": "iKwCbLpuHC9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d63fb456-84a2-404e-e9fd-0080471a237b"
      },
      "cell_type": "code",
      "source": [
        "sList=removePuncs(sampleList)\n",
        "print(sList)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "punctuation marks are:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['Association', 'for', 'Computational', 'Linguistics', '6', 'th', 'Applied', 'Natural', 'Language', 'Processing', 'ConferenceProceedingsof', 'the', 'Conference', 'April', '29May', '4', '2000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "64pIo73PHC9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function below has already been implemented for you, understand it well. It might help you ahead!"
      ]
    },
    {
      "metadata": {
        "id": "BHF75kHYHC9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### function to calculate term frequency in the doc\n",
        "def termFrequencyInDoc(wordList):\n",
        "    \"\"\"\n",
        "    This function should take a list of words as input argument, and output a dictionary of words such that\n",
        "    each word that appears in the document is key in the dictionary and it's value is term frequency\n",
        "    \"\"\"\n",
        "    termFrequency_dic={}\n",
        "    for w in wordList:\n",
        "        if w in termFrequency_dic.keys():\n",
        "            termFrequency_dic[w]+=1\n",
        "        else:\n",
        "            termFrequency_dic[w]=1\n",
        "    return termFrequency_dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltjlbnm4HC98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0887a593-5577-4f7f-836d-bf395188abac"
      },
      "cell_type": "code",
      "source": [
        "termFrequencyInDoc(sList)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2000': 1,\n",
              " '29May': 1,\n",
              " '4': 1,\n",
              " '6': 1,\n",
              " 'Applied': 1,\n",
              " 'April': 1,\n",
              " 'Association': 1,\n",
              " 'Computational': 1,\n",
              " 'Conference': 1,\n",
              " 'ConferenceProceedingsof': 1,\n",
              " 'Language': 1,\n",
              " 'Linguistics': 1,\n",
              " 'Natural': 1,\n",
              " 'Processing': 1,\n",
              " 'for': 1,\n",
              " 'th': 1,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "EXmp8T0uHC-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to calculate word Document frequency\n",
        "def wordDocFre(dicList):\n",
        "    \"\"\"\n",
        "    This function takes list of dictionary as input arguemnt, each dictionary in this list is the word that appears in the\n",
        "    given document as keys and the no. of times the word appears as value. This function should construct a dictionary which\n",
        "    has all the words that appear in the corpus as keys and no. of docs that contain this word as value\n",
        "    \"\"\"\n",
        "    vocan={}\n",
        "    for docDic in dicList:\n",
        "        for keys in docDic.keys():\n",
        "            if keys in vocan.keys():\n",
        "                vocan[keys]+=1\n",
        "                #print(\"hint: see termFrequencyInDoc() function\")\n",
        "            else:\n",
        "                vocan[keys]=1\n",
        "                #print(\"hint: see termFrequencyInDoc() function\")\n",
        "    return vocan\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ebQESoIlY86Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lista= []\n",
        "lista.append(DF)\n",
        "WordDF=wordDocFre(lista)\n",
        "WordDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0o7Gas3HC_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## you should be well versed in syntax of creating functions by now !!\n",
        "## construct a function named inverseDocFre() that takes dictionary returned from wordDocFre functions above\n",
        "## and outputs inverse document frequency of each word. You can do it!\n",
        "\n",
        "def inverseDocFre(dicList,DocumentCount):\n",
        "  IMD={}\n",
        "  for keys in dicList:\n",
        "    IMD[keys]=math.log(DocumentCount+1/dicList[keys])\n",
        "  \n",
        "  return IMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8VK2JQJijU3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "25646820-d61d-4b70-d0fc-e78ea9691356"
      },
      "cell_type": "code",
      "source": [
        "inverseDocFre(WordDF,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2000': 0.6931471805599453,\n",
              " '29May': 0.6931471805599453,\n",
              " '4': 0.6931471805599453,\n",
              " '6': 0.6931471805599453,\n",
              " 'Applied': 0.6931471805599453,\n",
              " 'April': 0.6931471805599453,\n",
              " 'Association': 0.6931471805599453,\n",
              " 'Computational': 0.6931471805599453,\n",
              " 'Conference': 0.6931471805599453,\n",
              " 'ConferenceProceedingsof': 0.6931471805599453,\n",
              " 'Language': 0.6931471805599453,\n",
              " 'Linguistics': 0.6931471805599453,\n",
              " 'Natural': 0.6931471805599453,\n",
              " 'Processing': 0.6931471805599453,\n",
              " 'for': 0.6931471805599453,\n",
              " 'th': 0.6931471805599453,\n",
              " 'the': 0.6931471805599453}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "-IKy02nLHC_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### function to calculate tf-idf for everyword in doc\n",
        "## this is your main function which calls the function above in appropriate fasion \n",
        "def tfidf(docList):\n",
        "    \"\"\"\n",
        "    This function takes list of documents it calls the function wordList to split the document in list of words and remove\n",
        "    stopwords and punctuation marks from them, then calls termFrequencyInDoc() uses its output to create \n",
        "    dictionary of vocabulary using the function wordDocFre(), it then should call inverseDocFre() function.\n",
        "    it then outputs a list of dictionary, where each document corresponds to one dictionary, its words should be keys\n",
        "    values should be tf-idf score\n",
        "    \"\"\"   \n",
        "    \n",
        "    #len(docList)\n",
        "    data={}\n",
        "    DFrequency=[]\n",
        "    DFreq={}\n",
        "    dicList=[]\n",
        "    for i in range(0,3):\n",
        "        with open(txtFiles[i], 'r',encoding='iso-8859-1') as myfile:\n",
        "          data[txtFiles[i]]=myfile.read().replace('\\n', '')\n",
        "        #print(data)        \n",
        "        dicList=wordList(str(data))       \n",
        "        dicList=removePuncs(dicList)\n",
        "        dicList=termFrequencyInDoc(dicList) \n",
        "        IDFDictionary=wordDocFre(data)\n",
        "        #print(\"This is easy-peasy, unless you were sleeping in class!\")\n",
        "    \n",
        "    #Reading the Full  Corpus\n",
        "    for i in range(0,3):\n",
        "        with open(txtFiles[i], 'r',encoding='iso-8859-1') as myfile:\n",
        "          DFrequency.append(myfile.read().replace('\\n', ''))\n",
        "     \n",
        "    #Constructing Dictionary\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #IDFDictionary=wordDocFre(data)\n",
        "    \n",
        "    \n",
        "    print(IDFDictionary)\n",
        "    #tfidf_Dic=inverseDocFre(IDFDictionary,len(docList))\n",
        "        \n",
        "    return #tfidf_Dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7e3Tz1fsnTfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "d28cf648-4b28-4140-a4d3-6b5bb05ab8ea"
      },
      "cell_type": "code",
      "source": [
        "tfidf(txtFiles)\n",
        "#txtFiles"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e43b41dd4c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtFiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#txtFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-ebe6b0d05a8c>\u001b[0m in \u001b[0;36mtfidf\u001b[0;34m(docList)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdicList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremovePuncs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdicList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermFrequencyInDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mIDFDictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordDocFre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(\"This is easy-peasy, unless you were sleeping in class!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c857a3852a3a>\u001b[0m in \u001b[0;36mwordDocFre\u001b[0;34m(dicList)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvocan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocDic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocDic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mvocan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KFEjG8fWHC_o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### sort vocabulary according to IDF-against K (number of documents containing that word) values and plot using matplotlib.pyplot"
      ]
    },
    {
      "metadata": {
        "id": "cNzOK_b-HC_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "IvKkiNaPHC_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "92229f92-09c6-4bd1-c791-f0e7a2fe6aa3"
      },
      "cell_type": "code",
      "source": [
        "### your code here\n",
        "tfidf(txtFiles)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'ITERATION, HABITUALITY AND VERB FORM SEMANTICS Frank v~n Eynoe. Un ivers i ty  oi Leuven Mar=a-Theresiastraat, 21 3000 Leuven Belgium ABSTRACT The veto forms are o4ten claimeo to convey two ;inds o+ information : I. w~et\\'~er the event Oeecr ibed in a sentence is present, past or future (= oe ic t i c  in fo rmat ion  2. whether the event described in a sentence is oresente~ as completed, going on, just  s ta r t ing  or being , in ished (= espectual information) \\\\\\\\ [ t  w i l l  be ~emonstrated in th i s  paper that one has t l  ado a rhino component to  the analysis of verb ~orm meanings, namely w~e~ner or no~ they e>press habltual i tv.  The 4ramewor~ 04 the analysis is mo~el- theoret ic  semantics. BACKGROUND The ana lw ls  of iteration and ha~ituality in th is  ~aper is part of a comprehensive semantic ar~Ivs is  of temporal expressions in natural kanguage. The research on th i s  topic is carr ied ob~ in ~he framework of EUROTRA, the MT project o4 the European Community. It is reporteo on e, tensi~,eiv in Van Eynde (lqBT). The or ig ina l  motive for s~arting th is  research ~as the fact that verbal tenses ann temporal a,:~:i l i~ries do not corresponO one-to-one in toe ienguages that EUROTRA has to deal with. Compare for in~taqce , i  EN ne has l ived in Copenhagen for 20 years ,Z, Dk nan nan boer i KmOenhavn i 20 ~r ~it~ tne l r  equlvaients in the fokiowlng languages \\\\x95 S~ DE er wonnt se i t  20 Jahren in Kopenhaoen ~i  FR i i  haDite ~ Copenhaoue Oepuis 20 ans ~5, NL n i j  woont sinds tw int lg  jaar in Kopenhagen When t rans la t ing  from Engl ieh or Danish to German, ~rench or Dutch the present perfect has to Pe replaceO by a simple present, Di&~ererces like these can be handled In one o; two eaVs either by Oef inlng complex mappings from source language to target  language forms in t rans fer  or Oy Oeflnlng mappings Oetween language spec i f i c  forms and In ter l ingua l  meanings in the monolingual components. SL ~orm ) TL form complex mmpc~ngs meanlng ) meanlng ,\\\\\\\\[ identity I mapping mapping SL ~orm TL form Because c* EUROTR~ s ao~erence to the pr inc ip le  o~ \"simPle t~ansfe ~\" it was quite OOVlOUS ~rom the s tar t  that the interlingual approa~ was the one to opt ~or. I t  w i l l ,  hence, be adopted in th l s  paper a~ wel l .  The paper consists  of t~ree parts.  In .ths flrst I will present a formal ism for the representat ion of time meanings,  together with mooel for the interpr~tat lon o~ those representat ions.  In the seconp th is  forma|ism wi i l  be extenOeO so that i t  can also Pe use~ for the ana\\\\\\\\]~slS o~ i terat ion an\\\\xa2 habitL, alit~. Ann In th~ th i rd  pert I ~i~i show how the extendeo formalis~ can be l,~\\' ~or a~ !n\\\\\\\\[erl lnoua~ a~alvs is  O~ the ver~ fo~.S, THE CORE FORMALISM A Temporal Model T~e formal ist  tha: ~ill oe use: here has oee~ de~ineo e~pi lc i t iy  i~, van Eynce, aes TomDe Q Maes ~5) .  irk th !s  p~per i wi l l  on!y  g ive  a s~or~ In~ormai present~zion of tDe fo rmai lS~ CO%Cemtratlrlg On th~se partS ~ICh  will De neeOe~ In the se~onO pert ,  270 The model COnSIStS  of a set  o f  l inearly oroerep irtsrvals. An in terva l  ~s a continuous set of time points on the time axis : I , ) A la l im i te  i t  might consist of one moment o6 t lme : I For an~ pair of ~ntervaie one can Oeflne tne l r  i n te r lec t ion  as the set of tlme points which the\\'/ share :  i J In j  Tn~s set m~g~t also be empty, as in I J it I s ,  furthermore, possible to define some b~narv re la t ions  between in terva is ,  such ae l preceoence \\' , , ~} I be4ore O (..~i,J: O J a f te r  i ; .~ J , i ;  I ~dent~ty \" t  I ) I simui .\\\\\\\\] =( l ,a )  O I contain , , \\' ) I par t -o f  J c~I,J., d 0 contain I ~( J , I ;  I overlap \\' , ~ , ) I l e f tover  J ~<( I , J )  O J r ightover  I >>(O,l) T~ese re la t ions  are also useO in  Bruce (1972). A Format for Representation For the semantic analysis of the temporal expressions i w i l i  s ta r t  from tne assumption t~at every sentence can be analvseo in two parts : the temporal informal:on expressa~ by the tenses. aux i l i a r ies  anO ao~erbials on t~e one hano. anp ~as~o atempora! proposit ion on the ot~er hand. (b; the cat sat on t~e mat w~.i. for instance, be analyseo in a basic proposit ion \"the cat s i t  on the met\" and the ~n~ormation conveyed Dv the past tense. The re la t ion  between both is established in two steps : the basic proposit ion is f i r s t  relateO to the in terva l  ~or whicn i t  is said to be true,  the socalled time of event (E), and then th is  in terva l  is re lated to the time Of speech ~S) : 3 E : ,E ,S )  ~ AT(E,the cat s i t  on the mat)3 This formula states that \"the cat s i t  on the mat\" i~ true at an in terva l  E which precedes the tlme of speech S. Following Reichenbach (1947) I w i l l  furthermore assume that the relat ion between the time of event and the time of speech is mediated by a th i ro  kind o~ in terva l ,  namely the time of reference (R), So, instead of the simple ReI(E,S) we w111 have a composite ReI~E,R) & RefeR,S). Ne.t to th l s  re la t iona l  information tn~ tempore: expreeslons can also give spec i f ic  informatlon about the iocat lon or the length of the reie~ant in terva ls .  This is typ ica l ly  Oone by means o~ t:me aOverbiais, such as \"next year\",  \" in the  spring\\':, \" for  t~o years\", \" t i l l  Christmas\", etc. T~is in~ormatlon w i l l  be represented bY means o~ one-place preOicates over in tervs l s  : Freo(E) and Pred~) ,  ~n exception ~s tc be ~ade here +or the time o, speec~. ~nose precise locat ion or length is never spec~fleo b,  i inQo~stic means, bu\\\\\\\\[ rather bv pragmatic factors .  A possible way to reelect  tn~s- In the &oc~,allsm is to t res t  i t  as an unbouno var iab le .  In sum, the general format for the representat lon of temporal information looks as fol lows : 3 R,E \\\\\\\\[Rei(R.S) ~ Pred~R; & ~eI(E,R) ~ Prep(E) AT(E,p)\\\\\\\\] where p is a basic atemporal proposit ion An example ; ~T we wi l l  v ie l t  Moscow next year 3 P,E \\\\\\\\[,~R,S~ & ne~t vear~R) ~ =~E,R) & A ~ E,we v i s i t  Moscowi\\\\\\\\] 271 As i t  stanos th is  format is not adequate yet fo ~ the ~epresentatlon of sentences l i ke  (8~ last year they played chess every week (e~ he was always late The basic propoe~tions \"they play chess\" and \"he oe la te\"  do not hold for one par t i cu la r  time of event E, but rather for a set of in terva ls  wnicn are spread in time in some way speci f ieo by \"every week\" in (8) and \"always\" in (9). In the fo l lowing part I w i l l  introduce an exter.oeO formalism which can OeaI with these typos 04 iteration. THE EXTENDED FORMALISM Cycl ic Iteration Cyclic i te ra t ion  is marked by aoverpials  l i ke  \"ca i iv\" ,  \"every Monday\", \"each year\" ,  etc. In ~virk e.a. (1972) they are cal lao per iodic  frequency adverbials. For the analysis of these adverbials I f i r s t  IntroOuce the notion Crams time. The frame time ie the in terva l  which contains a l l  the instances of the event describeo in the basic propos i t ion.  In (8~ last  year they played chess every week t~e ~rame time is last  year. In the general forma~ t .e frame time occupies the same place as the time c~ event in non- i te ra t ive  in terpretat ions  (= the E - i~terva i~,  ~ext, I de~ine a set of d i s t inc t ,  non- overlapping subinterva ls  ~I~ which are a l l  part o+ the frame time. In (8~, these in terva ls  have a length of one week each. This gives the fo l lowing ,pre l iminary)  representat ion :5 R,E \\\\\\\\[ (R,S) & last  year(R~ & =(E,R) & I \\\\\\\\ [ c \\' i ,E )  & nI :~ & week,i) - - -x  AT~i,they play chess; \\\\\\\\ ] \\\\\\\\ ]  R S s imi lar  analysis can be found in Stump (198i where t~e aoverbial frequency ad~ectlvee (P) ere given the fo l lowing t ru th  condit ion : F~\\' is true in a world w at an interval I i~4 ,~m is true in w at non-over lapp ing subintervals  o$ i distriOuteO throughout i ~t perioOs of a speci$ieo length I .  \" \\\\\\\\[Stump 1981, 226\\\\\\\\] 5t~mp s i - interval cor responds  to my frame time. and his non-over lapp lng sub interva ls  correspond to my I - in terva ls .  As a representat ion of (B) th i s  formula is not su f f i c ient ,  though, since the instances of chess pla~ing do not have to take a whole week for  (B~ to. be true.  A more adequate paraphrase is to say that every week contained at least one subinterval  (e~ during which they played chess : , , o  l \\\\\\\\ [ c ( l ,E~ & nl=~ & week!i) - - -> e \\\\\\\\ [c~e, l )  & AT(e, they play cness)\\\\\\\\] \\\\\\\\ ]  An argument in favor of th i s  refinement is that languages have special means for speci fy ing the e- times. In ~I(\\' last year she arr ived at ~ c clock every da~ the aoverbia2 \"at eight o \\\\xa2ioc~\" denotes the locat lOn 04 t~e e- intervai  ; B Not ice tha~ the pro~art lee of e are constant within \\'the 4tame time : the aoverDial \"st eight o c lo t ! \"  spec i t ies  t~e time of each o\\\\xa2 her arr lvals  cf last year. The general format for the representat ion of cycl ic i terat ion is, hence~ 3 R,E \\\\\\\\[ReI~R,S) & PreO~R~ & Rei(E,Ri & Pred~E} & I \\\\\\\\ [c( l .E~ & ni=O & P(1) - - - .  e ~:~e~I~ ~ M(e) - - -2  AT~e,p; \\\\\\\\ ] \\\\\\\\ ] \\\\\\\\ ]  where P is replacec ov the head o4 a per iooic  ~requencv aoverbial ,  spec i fy ing the l ocat ion  or the  iengtn o~ I Io -opt lona}l~i  replaced ov ~ ti~,a advero~6i, sPecifYin~ the length cr the igcatlon C.f e ~n im\\\\\\\\ [ ,o r ta r~t  property of th i s  format is it ~. cha in - l i ke  s t ructure  : 272 R is oef~neo with respect to S : ReI~R,S~ E as defined with respect to R : ReI(E,R~ I is defineo w~th respect to E : ~( I ,E)  and e is oefineo with respect to I : c(e. I~ As it stands, the format does not provioe any means for stat ing a d i rect  re la t ionsh ip  between the in terva ls  inside the frame time ~I and e~ ano the in terva ls  outside the frame time (S anO R~. As consequence, the formal~sm predicts  that temporal adverbials w~ich are in the scope o~ a frequency adverbial (:  the e-speci f iers~ cannot refer ba~K to the speech t~me or the eeference time: * Rei(e,S) and * Rel(e,R~, gooo p;ece of evidence for th is  hypothesis ~s pr~ioed  by the WHEN-aoveroiais. In general one can d is t inguish two kinde of those adverbials : t~e re la t iona l  ones, which express a re la t ion  Oetween the reference time and the speech time, such as \"~esterday\" a\\'nd \"tomorrow\", and the non- re la t iona l  ones, which ident i fy  the locat ion o~ an :nterva l  without any reference to the speech t~me, suc~ as \"between 8 and 9\" and \"at two o c lock\".  The in terest ing  thing now is t~at only the la t te r  adverbials can occur in the scope of a frequency adverbia l .  Compare :iI~ she arr ived every day between 8 anq 9 e *(12~ she arr ived every day yesterday e The fact that the re la t iona l  WHEN-adverbials cannot occur in the scope of a frequency aoverb~al prcviOes some pos i t ive  evioence ~or not inciuoln\\\\xa7 d i rect  re la t ions  between e ano S in the formal~em. The cha ln - l i ke  structure of the representat ion format Is ,  hence, i~ngu is t i ca l ly  motivated. Temporal Quantifiers The format Oeveloped for the analysis of cvc l ic  i te ra t lon  can also be useo for the analysis o~ the temporal ~uantifier$, such as \"miway~\", \"scmetlmes\", \"never\",  \"seldom\" ano \"o f ten\" .  The ~rmet ion  they proviOe is less spec i f i c  than the ona p~ovioed by the period frequency aOverb~ais, ar, d t~s  should be ref iecteO in the i r  representat ion.  As a s tar t ing  point I take the general ~ormat ~or the representation o~ sentences w~th a periodic frequency adverbial : . . .  ~ i \\\\\\\\[c( l .E~ & nI=~ & P( l i  - - -> 3 e \\\\\\\\ [c ie , l i  &Mie) & AT(e,p) \\\\\\\\ ] \\\\\\\\ ]  For a semantlc analysis of the temporal quant i f le rs  th i s  format has to be generalieeo. The most important change is the replacement of the universal ouant i ; ie r  bv a var iable : ... Q I C=(I,E) . . .  where Q can be any of the fo l lo~ing  quant i f le rs  always 3 eometimes -3 never  Few selOom, rare ly ,  now ano then Many o~ten, f requent ly  Most usual lv ,  mostly, general ly .=,is s ix fo ld  dzvis:on is taken beer from Lewis ~1975). This analysis account~ for the anomaly of sentences l i ke  o ,13} we sometimes played chess every wee~ 3 ? (141 they often met every month Many (15p we always plaveO chess every week 9 These sentences are eemant lce i iy  anomalous oecauee t~e sa~e ~ino o* In*ormation. namely the v~iue o~ ~. is epec~lec twice. This leaps to :~cons~etenc~ ~ ~13) and (14} where the Q- ve~ia~ie IB s~l~ to be both universal anO non- ~r;vers~i at tme same time, and i t  leaos to pleonasm in (15~ where the Q-variable is twice sago to Oe u~,iversal. The ne, t question is whethe,\" thP temporal quant!~iers introduce any ext ra -conq i t ions  on those In terva ls ,  ouch ms c~l ,E) ,  ~I=~ and P~i~. The f~rst  of t~ese condit ions appears to Pe relevant : the temporal quant i f ie rs  are ~ndeeo interpreteO wi~ respect to some given frame time. In ~x he was al~ays late \"a lways\"  ooesnot oenote AL~ possible in terva ls .  but onl~ a l l  possibie in terva ls  ~n the past. The conoit~on that the subintervals  may no~ overlap does not seem to be re levant ,  though, in (16, quaOratlc equations are aIweye s~mple 273 the Instances for whlon \"quadratic equations Pe ~imple\" are true are no~ temporal ly ordereo at all. it, is m~gnt indicate, Ov the way, that the i- objects ~re not necessari ly in terva ls ,  but rather cases or occasions wnlcn can but need no: be given m temporal in terpretat ion  (of.  Lewis 1975i. The th i rd  conOition concerns the propert ies of t~e I -ob jec ts .  In the case of the per iodic \\\\x95 ,equency aOverblals the relevant propert ies  concern the locat ion or the length of the in terva l .  In the case of the temporal guant i f le rs  one could think of speci fy ing a relevance conoiticn~ for a sentence l i ke  ~ he was always late ones not mea= that he was late at any possible occasion in the past, Put rather that he was late on al l  occasions on which his being late or timel~ could nave mattered. in Aqv~st, Hoepelman & Rohrer (1980) one can ~ind a proposal to incorporate th is  information in the semantic representat ion,  but I w i l l  not adopt t~is  proposal here, since the condit ions o~ the ,non)relevance of the occas ions are typica l iv  determined O~ pragmatic factors ,  in ~:\" he always leaves o~-~ twelve the relevant occasions (1) could just  as well oe all occasions on which he leaves as a l l  occasions on Wnlch ne leaves for work as a!i occas ions on ~hish he leaves for watching the home game of nls ~avour l te  footOaii  team. As a resu l t  of the foregoing reductions ar~o changes the general format for analysing tempo, al cuant i f ie rs  looks as fo l lows : 3 ~,E \\\\\\\\[ReI(R,S) & Pred(R) & ReI(E,R) & F\\'reoiE) & Q I \\\\\\\\[c( l~E) - - ->/& 3 e \\\\\\\\ [c~e, I ;  & M~ei & AT(e ,p! \\\\\\\\ ] \\\\\\\\ ] \\\\\\\\ ]  ,here O is replaced by any of {V, 3, \"3, Most, Few, Many} M is replaced by some time adverbial which spec i f ies  the locat ion or the length of e ( i f  there is anv~ Habitual i ty The sentences oiscusse~ so far al l  contain an exp l i c i t  ind icat ion  of i te ra t ion .  !he presence of SL~Ch an Ind lCat lOn  I s ,  however, not necessary for der iv ing an i te ra t ive  in terpretat ion .  Take, \\\\xf7or instance, (in~ he leaves at twel~e This sentence cannot only mean tnat he w i l l  leave at twelve, but also that he has the habit  of leaving ~-* twelve. in the representat ion of in terpretat ion  the time adverbial spec i f i ss  the t~me of reference : the former \"at twelve\\' 3 ~,E \\\\\\\\[ :(R,S) & at twelve(R) & :(E,R) & AT(E. he leave~\\\\\\\\] E S R in the representat ion of tne habitual i~terpretat ion~ on tne other hand, tne time adverolal shouls be tal~en to speci fy the mult ip le e-tlme, for the sentence Ooes no~ report on one o~ his ieavzngs at twelve, out rather on several of socn :ea,es. As a representat ion of th i s  in terpretat ion  I propose : ~,=st ; \\\\\\\\[--~I.fJ ---,  _ e Lc~e,I) ,~ at twelve~e & AT~e, he leave, l\\\\\\\\] R (19~ he leaves at twelve is t -eaten as synonymous with (20, he usual ly leaves at twelve I f  th i s  is fe l t  to be undesirable,  one cam introGuce a special quant i f ie r  for marking hab i tua i i tv ,  but at th i s  moment ~ do not see an~ reason fo r  SUCh a move. 274 The general format for the representation of habitual ~nterpretat~one Is,  hence, 3 R,E \\\\\\\\[ReI(R,S) ~ Pred(R) & Rel~E.R> ~ Preo(E) Most i \\\\\\\\[c~I,E) ---> 3 e \\\\\\\\[c~e,I) ~ Pred(e) & AT~e,p)\\\\\\\\]\\\\\\\\]\\\\\\\\] The Assignment of Representations to Sentences On t~e basis of the given analyses one O:stinguls~ three kinds of sentence meanings : no i terat ion no ~ i \\\\\\\\[ l /per iod ic  cyclic i~eration \\\\\\\\ Q I \\\\\\\\[ \\\\\\\\] \\\\\\\\ indef i - , te  can is specified F is not specified Q is any of {~,3, \"3,~ost,Manv,Few} The assl~nment of these meanings to particuiar sentences is fa i r l y  straightforward when the sentence contains a frequency adverOial or a temporal quanti f ier ,  but i f  there is none o~ those~ then the sentence is amOiguous Oetween a non-lterative and an habitual interpretation ~cf. the two interpretations of \"he leaves at tweive\"~. It, practice there are some oisambiguatlng ~. I* the basic proposition (p) denotes a state, ~r. er, the sentence can not have an habitual ir~erpreta~ior~ Compare :i;~ ne leaves at twelve ,21 ne is in je i !  ~1~ can be interpreted as meaning that he has the naPlt of leaving at twelve, bu~ (21i cannot Oe interpreted ms meaning that he has the habit of bel=g in jail. ~, Certain verb forms can biock the Oerivation o~ one of t~s two possiole interpretations. Compare ~2~ he is drinking coffee 12\\\\\\\\]) he drinks coffee (22, can Oenote a single instance of drinking as wei\" as a recent habit of him to drink:: coffee ~cf. in the sense of \"he is. drinklng coffee nowadays\"). (2; , ,  on the other hand, can only denote a habit; i t  cmnnot be used to report on a single instance o~ drinking. This demonstrates the need to distingulsn oi4ferent types of verb forms : the ones that wi l l  aiways e l i c i t  an habitual interpretat ion,  the ones that block the derivation o~ an habitual interpretat ion,  and the ones that admit both kinds of interpretmtions. The firs~ are unequivocall~ \\\\\\\\[+habitual\\\\\\\\],  the second C-habitual\\\\\\\\[ and the last wi l l  be given the feature \\\\\\\\ [+/-habitual \\\\\\\\ ] .  THE INTERLINSUAL ANALYSIS OF THE VERB FORMS The Meanings of the Verb Forme In the previous parts i have presente\\\\xa2 a formaliem for the representation of temporal information in sentences. This formallsm is especially deeigned for the anaiyeis of natural language, but not for the analysis o~ any particular natural language, such ae English, Dutch or Kiswahili. I ts  mmin purpose is to provide a conceptuall~ well-defined language for de;ining and comparln~ the ~eanings of te~poral expressions in di f ferent natural l~nguagee. In order to serve this purpose i t  is not s~ff ic lent ~o have a formalism, ~nouon. What is also needed is a general specif ication o4 now the semmntic representations relate to tnelr imnguage specific co~nterpmrts, i .e .  the tenses, the temporal aux l i : r ies  and t~e time aoveroials. The \\\\xf7orme~ two wi i l  furcner de caileO veto forms, For c { \\' is~ ~n~, those verb forms are summec up in the followlng rL~ie : Vero form ---~. \\\\\\\\[+/-F\\'ast\\\\\\\\] (wi11+ir.f) (have+EP) ({be+iNS to+fri l l )  ~e going T, hi_\\'\\\\xa2 rule ylelds 24 (=2x2x2x3) \\'verb forms. Their role in the semantic interpretation of sentences .:an easily de expressed in terms of the given formalism. They specify i .  the relat ion Petween reference time anO speech time : ~eI(R,S) (= oeict ic information) 2. the relation between event time and reference time : ReI,E,R) (= aspectual information~ 5. whether the sentence has an habitual and!or ; non-iterative ~nterpretaZlon 275 The meaning of a verb form can, hence, be representeO as a t r ip le  ~x,y,z> where x and v are substi~uteO for one of the possible dinar, -elations oe~ween interva ls ,  and where z is one of the three poesible habituali~y values. The aame verb  ~orm can,  of  course ,  have oifferent meanings and will, hence, Oe assoclateO ~th  a set of such t r ip les .  The detai ls o~ this association have  been discL:ssed elsewhere~ at \\\\\\\\]east for the x ann \\\\xa5 values ~cf. Van Eynde, des Tombe & Maes 1985i. In tnls paper I wi l l  only discuss the z values in some deta i l .  The Mabituality Value A good start ing point for demonstrating the relevance of the hab i tua l i ty  value is provided by the following i i s t  of sentences. They are taken from hess (1985). ~)  a text editor makes modifications to a text f i l e  ~25) a text editor is makin~ modifications to a text f i l e  ~26) a text editor made mooiflcatione to a text f i l e  \\\\x95 27~ a text editor has made modifications to a text f i l e  In L24) i t  is said \"that a text editor ma~es modifications to a text f i l e  in general, almost by Oef in i t ion.  We might read this  sentence in a re~erence manual\" (Hess 1985, 10). In (25-27), on the other hand, i t  is said \"that there i s ,  or was, a case of a text editor mankind modifications to a text f i l e .  These remarks might ~e made by a system operator, watcnlng ~is screen\\' (lb.). Hess concludes from these observations that the quant i f ier  of the subject is universal in (24) and e~:isten~ial in (25-27), However~ th is  conclusion does not foliow automatically. In terms of the formalism presented in this  paper one could sa~ that (24) has an habitual in terpretat ion ,  whereas the other sentences have a non- i terat ive interpretat ion,  In the former case the ex is tent ia l  quant i f ier  of the subject wi l l  be in the scope o~ the Most-quantif ier,  whereas in the la t te r  case i t  wl i i  not be in the scope of any non-existential  quant i f ie r ,  and this accounts for the difference in interpretat ion without havinq to postulate two possiole meanings for the indef in i te  a r t i c le .  Hess s examples are useful in this  context, t~ough, because they clear ly i l l us t ra te  the roie of the vend for~ in the interpretat ion.  Since i t  is the only variable part in the sentences, the ~ifferences in interpretat ion can only be ascribeo to them, more spec i f i ca l ly  to their hab l tua i i ty  value. ;or the assignment of an hab l tua l i ty  value to a given verb form one has to test whether i t  can or cannot e l i c i t  an habitual interpretat ion in some given context. In testing this  one should i. always use sentences with a non-stative basic proposit ion, for i~ the la t te r  is stat ive the sentence can never be habitual (of. supra) ; 2. pay attention to the other inter i lngual  values of the verb form. The English simple present. for instance, is uneouivocally \\\\\\\\[+habitual\\\\\\\\]  in i t s  sim~Itaneoue meaning, but in i t s  posterior meaning i t  can be \\\\\\\\ [ -nao i tua l \\\\\\\\ ]  too (of. the non- i terat ive interpretat ion of \"he leaves at twelve\"~. The relevance of the \\\\\\\\ [+/ -Hab i tua l i tv \\\\\\\\ ] -  d is t inct ion  has so far only been demonstrated from a monolln~ual semantic point of view. I t  is ,  however, possible to give some translat ional  evidence for this o let inct lo= as well. The relevant cas~s are tne ones where the corresponding verb forms have  Oi~ferent hab i :~a l l ty  values. A good example of th is  is the translat ion of the Dutch simple present in En~ilsh. The Dutch simple present can be both habitua} and ~on-hacitual in It~ simultaneous meaning : 28; hi~ o,\\'inxt aileen whisky <simui,y,~haOitual~ \"he drinks only whisky\\' 29, Liji~, h i j  dr!nit k~4 ie  . ,s imul, / , -habltuai> \"look, he Orinks co,fee\" The English simple present, on the other hand, s always habitual in i t s  simultaneous meaning unless in sentences Oee:ribing states, of course (~0~ he only drinks whisky <slmui,y,+habitua~. *~31) iooi:, he drinks ~o~fee <s imul ,y , -ha~i tua l  Pot the expression of slmul~aneous non- i te ra t iv i ty  one has to use She progressive : 32) look, De is crinking coffee As a conseoue~ce. ~e mapping of (29) to ~32) in~ol~es a non-~riviai  tense replacement, and i t  i l  o~e of the merits o~ the given formaliem that i t  car handle this i r  an lnter i ingual  way. 276 REFERENCES ~qviet Lennart, Hoepelman Jaap & Rohre? Ch~-istiah (19BO~, \"Adverbs of frequency:, in Rohr~r (ed.), Time~ tense and quantifiere. Niemever. T~oingen, 1-17. Bruce Bertram (1972), \"A model for temporal reference and its application in a question- answering program\", in Ar t i f i c ia l  Intelligence 3, 1-25. Hess M~chael (I~B5), \"How does natural language quantify ?\". in Proceedings of the Secono Cmnferenc~ of the European Cnapter of the ACL, Geneva, B-15. Lemis David ~1975~ \"Adverbs of ouantification\", in Keenan (ed.), Formal semantics of natural language. Cambrioge University Press, Cambridge, ~-15. ~u!rk  Randolph, Greenbaum Sioney, Leech Geoffrey Svartvik Jan (1972J, A grammar of contemporar~ English. Longman~ London. Relcnenoach Hans (1947~ Elements of symDollc logic. University of California Press, Berkeley. Stump Gregor. ~19BI~, \"The Interpretation of frequenc~ ~ adjectives\",  In L~nguist ics ant ~nilosophy 4. 221-257. Van Eynde Frank~ des Tombe Louis & Maes Fons ~1985)~ \"The specification of time meaning ~or machine translation\", In Proceedings of the Second Conference of the European chapter of the ACL, Geneva, 35-40. Van Eynde Frank (1987), Time. A unified theory of tense, asoect and Aktionsart, An internal Eurotra Ooeument (78 pages). Leuven. 277 '\n",
            "b'Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298\\\\x961307,Uppsala, Sweden, 11-16 July 2010. c\\\\xa92010 Association for Computational LinguisticsImproved Unsupervised POS Induction through Prototype DiscoveryOmri Abend1? Roi Reichart2 Ari Rappoport11Institute of Computer Science, 2ICNCHebrew University of Jerusalem{omria01|roiri|arir}@cs.huji.ac.ilAbstractWe present a novel fully unsupervised al-gorithm for POS induction from plain text,motivated by the cognitive notion of proto-types. The algorithm first identifies land-mark clusters of words, serving as thecores of the induced POS categories. Therest of the words are subsequently mappedto these clusters. We utilize morpho-logical and distributional representationscomputed in a fully unsupervised manner.We evaluate our algorithm on English andGerman, achieving the best reported re-sults for this task.1 IntroductionPart-of-speech (POS) tagging is a fundamentalNLP task, used by a wide variety of applications.However, there is no single standard POS tag-ging scheme, even for English. Schemes varysignificantly across corpora and even more soacross languages, creating difficulties in usingPOS tags across domains and for multi-lingualsystems (Jiang et al., 2009). Automatic inductionof POS tags from plain text can greatly alleviatethis problem, as well as eliminate the efforts in-curred by manual annotations. It is also a problemof great theoretical interest. Consequently, POSinduction is a vibrant research area (see Section 2).In this paper we present an algorithm basedon the theory of prototypes (Taylor, 2003), whichposits that some members in cognitive categoriesare more central than others. These practically de-fine the category, while the membership of otherelements is based on their association with the? Omri Abend is grateful to the Azrieli Foundation forthe award of an Azrieli Fellowship.central members. Our algorithm first clusterswords based on a fine morphological representa-tion. It then clusters the most frequent words,defining landmark clusters which constitute thecores of the categories. Finally, it maps the restof the words to these categories. The last twostages utilize a distributional representation thathas been shown to be effective for unsupervisedparsing (Seginer, 2007).We evaluated the algorithm in both English andGerman, using four different mapping-based andinformation theoretic clustering evaluation mea-sures. The results obtained are generally betterthan all existing POS induction algorithms.Section 2 reviews related work. Sections 3 and4 detail the algorithm. Sections 5, 6 and 7 describethe evaluation, experimental setup and results.2 Related WorkUnsupervised and semi-supervised POS tagginghave been tackled using a variety of methods.Schu\\\\xa8tze (1995) applied latent semantic analysis.The best reported results (when taking into ac-count all evaluation measures, see Section 5) aregiven by (Clark, 2003), which combines dis-tributional and morphological information withthe likelihood function of the Brown algorithm(Brown et al., 1992). Clark\\\\x92s tagger is very sen-sitive to its initialization. Reichart et al. (2010b)propose a method to identify the high quality runsof this algorithm. In this paper, we show thatour algorithm outperforms not only Clark\\\\x92s meanperformance, but often its best among 100 runs.Most research views the task as a sequential la-beling problem, using HMMs (Merialdo, 1994;Banko and Moore, 2004; Wang and Schuurmans,2005) and discriminative models (Smith and Eis-ner, 2005; Haghighi and Klein, 2006). Several1298techniques were proposed to improve the HMMmodel. A Bayesian approach was employed by(Goldwater and Griffiths, 2007; Johnson, 2007;Gao and Johnson, 2008). Van Gael et al. (2009)used the infinite HMM with non-parametric pri-ors. Grac\\\\xb8a et al. (2009) biased the model to inducea small number of possible tags for each word.The idea of utilizing seeds and expanding themto less reliable data has been used in several pa-pers. Haghighi and Klein (2006) use POS \\\\x91pro-totypes\\\\x92 that are manually provided and tailoredto a particular POS tag set of a corpus. Fre-itag (2004) and Biemann (2006) induce an ini-tial clustering and use it to train an HMM model.Dasgupta and Ng (2007) generate morphologicalclusters and use them to bootstrap a distributionalmodel. Goldberg et al. (2008) use linguistic con-siderations for choosing a good starting point forthe EM algorithm. Zhao and Marcus (2009) ex-pand a partial dictionary and use it to learn dis-ambiguation rules. Their evaluation is only at thetype level and only for half of the words. Raviand Knight (2009) use a dictionary and an MDL-inspired modification to the EM algorithm.Many of these works use a dictionary provid-ing allowable tags for each or some of the words.While this scenario might reduce human annota-tion efforts, it does not induce a tagging schemebut remains tied to an existing one. It is furthercriticized in (Goldwater and Griffiths, 2007).Morphological representation. Many POS in-duction models utilize morphology to some ex-tent. Some use simplistic representations of termi-nal letter sequences (e.g., (Smith and Eisner, 2005;Haghighi and Klein, 2006)). Clark (2003) modelsthe entire letter sequence as an HMM and uses itto define a morphological prior. Dasgupta and Ng(2007) use the output of the Morfessor segmenta-tion algorithm for their morphological representa-tion. Morfessor (Creutz and Lagus, 2005), whichwe use here as well, is an unsupervised algorithmthat segments words and classifies each segmentas being a stem or an affix. It has been tested onseveral languages with strong results.Our work has several unique aspects. First,our clustering method discovers prototypes in afully unsupervised manner, mapping the rest ofthe words according to their association with theprototypes. Second, we use a distributional repre-sentation which has been shown to be effective forunsupervised parsing (Seginer, 2007). Third, weuse a morphological representation based on sig-natures, which are sets of affixes that represent afamily of words sharing an inflectional or deriva-tional morphology (Goldsmith, 2001).3 Distributional AlgorithmOur algorithm is given a plain text corpus and op-tionally a desired number of clusters k. Its outputis a partitioning of words into clusters. The al-gorithm utilizes two representations, distributionaland morphological. Although eventually the latteris used before the former, for clarity of presenta-tion we begin by detailing the base distributionalalgorithm. In the next section we describe the mor-phological representation and its integration intothe base algorithm.Overview. The algorithm consists of two mainstages: landmark clusters discovery, and wordmapping. For the former, we first compute a dis-tributional representation for each word. We thencluster the coordinates corresponding to high fre-quency words. Finally, we define landmark clus-ters. In the word mapping stage we map each wordto the most similar landmark cluster.The rationale behind using only the high fre-quency words in the first stage is twofold. First,prototypical members of a category are frequent(Taylor, 2003), and therefore we can expect thesalient POS tags to be represented in this smallsubset. Second, higher frequency implies more re-liable statistics. Since this stage determines thecores of all resulting clusters, it should be as accu-rate as possible.Distributional representation. We use a sim-plified form of the elegant representation of lexi-cal entries used by the Seginer unsupervised parser(Seginer, 2007). Since a POS tag reflects thegrammatical role of the word and since this rep-resentation is effective to parsing, we were moti-vated to apply it to the present task.Let W be the set of word types in the corpus.The right context entry of a word x ? W is a pairof mappings r intx : W ? [0, 1] and r adjx :W ? [0, 1]. For each w ? W , r adjx(w) is anadjacency score of w to x, reflecting w\\\\x92s tendencyto appear on the right hand side of x.For each w ? W , r intx(w) is an interchange-ability score of x with w, reflecting the tendencyof w to appear to the left of words that tend to ap-pear to the right of x. This can be viewed as a1299similarity measure between words with respect totheir right context. The higher the scores the morethe words tend to be adjacent/interchangeable.Left context parameters l intx and l adjx aredefined analogously.There are important subtleties in these defini-tions. First, for two words x,w ? W , r adjx(w)is generally different from l adjw(x). For exam-ple, if w is a high frequency word and x is a lowfrequency word, it is likely that w appears manytimes to the right of x, yielding a high r adjx(w),but that x appears only a few times to the left of wyielding a low l adjw(x). Second, from the defi-nition of r intx(w) and r intw(x), it is clear thatthey need not be equal.These functions are computed incrementally bya bootstrapping process. We initialize all map-pings to be identically 0. We iterate over the wordsin the training corpus. For every word instance x,we take the word immediately to its right y andupdate x\\\\x92s right context using y\\\\x92s left context:?w ? W : r intx(w) +=l adjy(w)N(y)?w ? W : r adjx(w) +={1 w = yl inty(w)N(y) w 6= yThe division by N(y) (the number of times yappears in the corpus before the update) is done inorder not to give a disproportional weight to highfrequency words. Also, r intx(w) and r adjx(w)might become larger than 1. We therefore nor-malize them after all updates are performed by thenumber of occurrences of x in the corpus.We update l intx and l adjx analogously usingthe word z immediately to the left of x. The up-dates of the left and right functions are done inparallel.We define the distributional representation of aword type x to be a 4|W | + 2 dimensional vectorvx. Each word w yields four coordinates, one foreach direction (left/right) and one for each map-ping type (int/adj). Two additional coordinatesrepresent the frequency in which the word appearsto the left and to the right of a stopping punc-tuation. Of the 4|W | coordinates correspondingto words, we allow only 2n to be non-zero: then top scoring among the right side coordinates(those of r intx and r adjx), and the n top scoringamong the left side coordinates (those of l intxand l adjx). We used n = 50.The distance between two words is defined tobe one minus the cosine of the angle between theirrepresentation vectors.Coordinate clustering. Each of our landmarkclusters will correspond to a set of high frequencywords (HFWs). The number of HFWs is muchlarger than the number of expected POS tags.Hence we should cluster HFWs. Our algorithmdoes that by unifying some of the non-zero coordi-nates corresponding to HFWs in the distributionalrepresentation defined above.We extract the words that appear more than Ntimes per million1 and apply the following proce-dure I times (5 in our experiments).We run average link clustering with a threshold? (AVGLINK?, (Jain et al., 1999)) on these words,in each iteration initializing every HFW to haveits own cluster. AVGLINK? means running the av-erage link algorithm until the two closest clustershave a distance larger than ?. We then use the in-duced clustering to update the distributional rep-resentation, by collapsing all coordinates corre-sponding to words appearing in the same clusterinto a single coordinate whose value is the sumof the collapsed coordinates\\\\x92 values. In order toproduce a conservative (fine) clustering, we used arelatively low ? value of 0.25.Note that the AVGLINK? initialization in eachof the I iterations assigns each HFW to a sepa-rate cluster. The iterations differ in the distribu-tional representation of the HFWs, resulting fromthe previous iterations.In our English experiments, this process re-duced the dimension of the HFWs set (the num-ber of coordinates that are non-zero in at least oneof the HFWs) from 14365 to 10722. The aver-age number of non-zero coordinates per word de-creased from 102 to 55.Since all eventual POS categories correspond toclusters produced at this stage, to reduce noise wedelete clusters of less than five elements.Landmark detection. We define landmark clus-ters using the clustering obtained in the final iter-ation of the coordinate clustering stage. However,the number of clusters might be greater than thedesired number k, which is an optional parame-ter of the algorithm. In this case we select a sub-set of k clusters that best covers the HFW space.We use the following heuristic. We start from themost frequent cluster, and greedily select the clus-1We used N = 100, yielding 1242 words for English and613 words for German.1300ter farthest from the clusters already selected. Thedistance between two clusters is defined to be theaverage distance between their members. A clus-ter\\\\x92s distance from a set of clusters is defined tobe its minimal distance from the clusters in theset. The final set of clusters {L1, ..., Lk} and theirmembers are referred to as landmark clusters andprototypes, respectively.Mapping all words. Each word w ? W is as-signed the cluster Li that contains its nearest pro-totype:d(w,Li) = minx?Li{1 ? cos(vw, vx)}Map(w) = argminLi{d(w,Li)}Words that appear less than 5 times are consid-ered as unknown words. We consider two schemesfor handling unknown words. One randomly mapseach such word to a cluster, using a probabil-ity proportional to the number of unique knownwords already assigned to that cluster. However,when the number k of landmark clusters is rela-tively large, it is beneficial to assign all unknownwords to a separate new cluster (after running thealgorithm with k? 1). In our experiments, we usethe first option when k is below some threshold(we used 15), otherwise we use the second.4 Morphological ModelThe morphological model generates another wordclustering, based on the notion of a signature.This clustering is integrated with the distributionalmodel as described below.4.1 Morphological RepresentationWe use the Morfessor (Creutz and Lagus, 2005)word segmentation algorithm. First, all words inthe corpus are segmented. Then, for each stem,the set of all affixes with which it appears (its sig-nature, (Goldsmith, 2001)) is collected. The mor-phological representation of a word type is thendefined to be its stem\\\\x92s signature in conjunctionwith its specific affixes2 (See Figure 1).We now collect all words having the same rep-resentation. For instance, if the words joined andpainted are found to have the same signature, theywould share the same cluster since both have theaffix \\\\x91 ed\\\\x92. The word joins does not share the samecluster with them since it has a different affix, \\\\x91 s\\\\x92.This results in coarse-grained clusters exclusivelydefined according to morphology.2A word may contain more than a single affix.Types join joins joined joiningStem join join join joinAffixes ? s ed ingSignature {?, ed, s, ing}Figure 1: An example for a morphological representation,defined to be the conjunction of its affix(es) with the stem\\\\x92ssignature.In addition, we incorporate capitalization infor-mation into the model, by constraining all wordsthat appear capitalized in more than half of theirinstances to belong to a separate cluster, regard-less of their morphological representation. Themotivation for doing so is practical: capitalizationis used in many languages to mark grammaticalcategories. For instance, in English capitalizationmarks the category of proper names and in Ger-man it marks the noun category . We report En-glish results both with and without this modifica-tion.Words that contain non-alphanumeric charac-ters are represented as the sequence of the non-alphanumeric characters they include, e.g., \\\\x91vis-a`-vis\\\\x92 is represented as (\\\\x93-\\\\x94, \\\\x93-\\\\x94). We do not as-sign a morphological representation to words in-cluding more than one stem (like weatherman), towords that have a null affix (i.e., where the wordis identical to its stem) and to words whose stemis not shared by any other word (signature of size1). Words that were not assigned a morphologi-cal representation are included as singletons in themorphological clustering.4.2 Distributional-Morphological AlgorithmWe detail the modifications made to our basedistributional algorithm given the morphologicalclustering defined above.Coordinate clustering and landmarks. Weconstrain AVGLINK? to begin by forming links be-tween words appearing in the same morphologi-cal cluster. Only when the distance between thetwo closest clusters gets above ? we remove thisconstraint and proceed as before. This is equiv-alent to performing AVGLINK? separately withineach morphological cluster and then using the re-sult as an initial condition for an AVGLINK? coor-dinate clustering. The modified algorithm in thisstage is otherwise identical to the distributional al-gorithm.Word mapping. In this stage words that are notprototypes are mapped to one of the landmark1301clusters. A reasonable strategy would be to mapall words sharing a morphological cluster as a sin-gle unit. However, these clusters are too coarse-grained. We therefore begin by partitioning themorphological clusters into sub-clusters accordingto their distributional behavior. We do so by apply-ing AVGLINK? (the same as AVGLINK? but with adifferent parameter) to each morphological clus-ter. Since our goal is cluster refinement, we use a? that is considerably higher than ? (0.9).We then find the closest prototype to each suchsub-cluster (averaging the distance across all ofthe latter\\\\x92s members) and map it as a single unitto the cluster containing that prototype.5 Clustering EvaluationWe evaluate the clustering produced by our algo-rithm using an external quality measure: we takea corpus tagged by gold standard tags, tag it usingthe induced tags, and compare the two taggings.There is no single accepted measure quantifyingthe similarity between two taggings. In order tobe as thorough as possible, we report results usingfour known measures, two mapping-based mea-sures and two information theoretic ones.Mapping-based measures. The induced clus-ters have arbitrary names. We define two map-ping schemes between them and the gold clus-ters. After the induced clusters are mapped, wecan compute a derived accuracy. The Many-to-1measure finds the mapping between the gold stan-dard clusters and the induced clusters which max-imizes accuracy, allowing several induced clustersto be mapped to the same gold standard cluster.The 1-to-1 measure finds the mapping betweenthe induced and gold standard clusters which max-imizes accuracy such that no two induced clus-ters are mapped to the same gold cluster. Com-puting this mapping is equivalent to finding themaximal weighted matching in a bipartite graph,whose weights are given by the intersection sizesbetween matched classes/clusters. As in (Reichartand Rappoport, 2008), we use the Kuhn-Munkresalgorithm (Kuhn, 1955; Munkres, 1957) to solvethis problem.Information theoretic measures. These arebased on the observation that a good clustering re-duces the uncertainty of the gold tag given the in-duced cluster, and vice-versa. Several such mea-sures exist; we use V (Rosenberg and Hirschberg,2007) and NVI (Reichart and Rappoport, 2009),VI\\\\x92s (Meila, 2007) normalized version.6 Experimental SetupSince a goal of unsupervised POS tagging is in-ducing an annotation scheme, comparison to anexisting scheme is problematic. To address thisproblem we compare to three different schemesin two languages. In addition, the two Englishschemes we compare with were designed to tagcorpora contained in our training set, and havebeen widely and successfully used with these cor-pora by a large number of applications.Our algorithm was run with the exact same pa-rameters on both languages: N = 100 (high fre-quency threshold), n = 50 (the parameter thatdetermines the effective number of coordinates),? = 0.25 (cluster separation during landmarkcluster generation), ? = 0.9 (cluster separationduring refinement of morphological clusters).The algorithm we compare with in most detailis (Clark, 2003), which reports the best currentresults for this problem (see Section 7). SinceClark\\\\x92s algorithm is sensitive to its initialization,we ran it a 100 times and report its average andstandard deviation in each of the four measures.In addition, we report the percentile in which ourresult falls with respect to these 100 runs.Punctuation marks are very frequent in corporaand are easy to cluster. As a result, including themin the evaluation greatly inflates the scores. Forthis reason we do not assign a cluster to punctua-tion marks and we report results using this policy,which we recommend for future work. However,to be able to directly compare with previous work,we also report results for the full POS tag set.We do so by assigning a singleton cluster to eachpunctuation mark (in addition to the k requiredclusters). This simple heuristic yields very highperformance on punctuation, scoring (when allother words are assumed perfect tagging) 99.6%(99.1%) 1-to-1 accuracy when evaluated againstthe English fine (coarse) POS tag sets, and 97.2%when evaluated against the German POS tag set.For English, we trained our model on the39832 sentences which constitute sections 2-21 ofthe PTB-WSJ and on the 500K sentences fromthe NYT section of the NANC newswire corpus(Graff, 1995). We report results on the WSJ partof our data, which includes 950028 words tokensin 44389 types. Of the tokens, 832629 (87.6%)1302English Fine k=13 Coarse k=13 Fine k=34Prototype Clark Prototype Clark Prototype ClarkTagger \\\\xb5 ? % Tagger \\\\xb5 ? % Tagger \\\\xb5 ? %Many\\\\x96to\\\\x961 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 9055.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 901\\\\x96to\\\\x961 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 10054.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 1000.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 1000.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98German k=17 k=26Prototype Clark Prototype ClarkTagger \\\\xb5 ? % Tagger \\\\xb5 ? %Many\\\\x96to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 6058.9 59.1 1.4 40 63.2 62.8 1.2 601\\\\x96to\\\\x961 53.7 52.0 1.8 77 56.0 52.0 2.1 9948.0 46.0 2.3 78 50.7 45.9 2.6 99NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 1000.819 0.829 0.025 66 0.785 0.839 0.025 100V 0.646 0.645 0.010 50 0.675 0.657 0.008 1000.552 0.553 0.013 48 0.596 0.574 0.010 100Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark\\\\x92s average score (\\\\xb5),Clark\\\\x92s standard deviation (?) and the fraction of Clark\\\\x92s results that scored worse than our model (%). For the mapping basedmeasures, results are accuracy percentage. For V ? [0, 1], higher is better. For high quality output, NV I ? [0, 1] as well, andlower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the scorewhen excluding it. In English, our results are always better than Clark\\\\x92s. In German, they are almost always better.are not punctuation. The percentage of unknownwords (those appearing less than five times) is1.6%. There are 45 clusters in this annotationscheme, 34 of which are not punctuation.We ran each algorithm both with k=13 andk=34 (the number of desired clusters). We com-pare the output to two annotation schemes: the finegrained PTB WSJ scheme, and the coarse grainedtags defined in (Smith and Eisner, 2005). Theoutput of the k=13 run is evaluated both againstthe coarse POS tag annotation (the \\\\x91Coarse k=13\\\\x92scenario) and against the full PTB-WSJ annotationscheme (the \\\\x91Fine k=13\\\\x92 scenario). The k=34 runis evaluated against the full PTB-WSJ annotationscheme (the \\\\x91Fine k=34\\\\x92 scenario).The POS cluster frequency distribution tends tobe skewed: each of the 13 most frequent clustersin the PTB-WSJ cover more than 2.5% of the to-kens (excluding punctuation) and together 86.3%of them. We therefore chose k=13, since it is boththe number of coarse POS tags (excluding punctu-ation) as well as the number of frequent POS tagsin the PTB-WSJ annotation scheme. We chosek=34 in order to evaluate against the full 34 tagsPTB-WSJ annotation scheme (excluding punctua-tion) using the same number of clusters.For German, we trained our model on the 20296sentences of the NEGRA corpus (Brants, 1997)and on the first 450K sentences of the DeWACcorpus (Baroni et al., 2009). DeWAC is a cor-pus extracted by web crawling and is thereforeout of domain. We report results on the NEGRApart, which includes 346320 word tokens of 49402types. Of the tokens, 289268 (83.5%) are notpunctuation. The percentage of unknown words(those appearing less than five times) is 8.1%.There are 62 clusters in this annotation scheme,51 of which are not punctuation.We ran the algorithms with k=17 and k=26.k=26 was chosen since it is the number of clus-ters that cover each more than 0.5% of the NE-GRA tokens, and in total cover 96% of the (non-punctuation) tokens. In order to test our algo-rithm in another scenario, we conducted experi-ments with k=17 as well, which covers 89.9% ofthe tokens. All outputs are compared against NE-GRA\\\\x92s gold standard scheme.We do not report results for k=51 (where thenumber of gold clusters is the same as the numberof induced clusters), since our algorithm producedonly 42 clusters in the landmark detection stage.We could of course have modified the parame-ters to allow our algorithm to produce 51 clusters.However, we wanted to use the exact same param-eters as those used for the English experiments tominimize the issue of parameter tuning.In addition to the comparisons described above,we present results of experiments (in the \\\\x91Fine1303B B+M B+C F(I=1) FM-to-1 53.3 54.8 58.2 57.3 61.01-to-1 50.2 51.7 55.1 54.8 60.0NVI 0.782 0.720 0.710 0.742 0.652V 0.569 0.598 0.615 0.597 0.636Table 2: A comparison of partial versions of the model inthe \\\\x91Fine k=13\\\\x92 WSJ scenario. M-to-1 and 1-to-1 results arereported in accuracy percentage. Lower NVI is better. B is thestrictly distributional algorithm, B+M adds the morphologi-cal model, B+C adds capitalization to B, F(I=1) consists ofall components, where only one iteration of coordinate clus-tering is performed, and F is the full model.M-to-1 1-to-1 V VIPrototype 71.6 63.5 0.677 2.00Clark 69.8 54.5 0.659 2.18HK \\\\x96 41.3 \\\\x96 \\\\x96J 43\\\\x9662 37\\\\x9647 \\\\x96 4.23\\\\x965.74GG \\\\x96 \\\\x96 \\\\x96 2.8GJ \\\\x96 40\\\\x9649.9 \\\\x96 4.03\\\\x964.47VG \\\\x96 \\\\x96 0.54-0.59 2.5\\\\x962.9GGTP-45 65.4 44.5 \\\\x96 \\\\x96GGTP-17 70.2 49.5 \\\\x96 \\\\x96Table 4: Comparison of our algorithms with the recent fullyunsupervised POS taggers for which results are reported. Themodels differ in the annotation scheme, the corpus size andthe number of induced clusters (k) that they used. HK:(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.GG: (Goldwater and Griffiths, 2007), 24K tokens, coarsetags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,k=25\\\\x9650. GJ: (Gao and Johnson, 2008), 1.17M tokens, finetags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, finetags, k=47\\\\x96192. GGTP-45: (Grac\\\\xb8a et al., 2009), 1.17M to-kens, fine tags, k=45. GGTP-17: (Grac\\\\xb8a et al., 2009), 1.17Mtokens, coarse tags, k=17. Lower VI values indicate betterclustering. VI is computed using e as the base of the loga-rithm. Our algorithm gives the best results.k=13\\\\x92 scenario) that quantify the contribution ofeach component of the algorithm. We ran the basedistributional algorithm, a variant which uses onlycapitalization information (i.e., has only one non-singleton morphological class, that of words ap-pearing capitalized in most of their instances) anda variant which uses no capitalization information,defining the morphological clusters according tothe morphological representation alone.7 ResultsTable 1 presents results for the English and Ger-man experiments. For English, our algorithm ob-tains better results than Clark\\\\x92s in all measures andscenarios. It is without exception better than theaverage score of Clark\\\\x92s and in most cases betterthan the maximal Clark score obtained in 100 runs.A significant difference between our algorithmand Clark\\\\x92s is that the latter, like most algorithmswhich addressed the task, induces the clustering0 5 10 15 20 25 30 35 40 4500.20.40.60.81  Gold StandardInducedFigure 2: POS class frequency distribution for our modeland the gold standard, in the \\\\x91Fine k=34\\\\x92 scenario. The dis-tributions are similar.by maximizing a non-convex function. Thesefunctions have many local maxima and the specificsolution to which algorithms that maximize themconverge strongly depends on their (random) ini-tialization. Therefore, their output\\\\x92s quality oftensignificantly diverges from the average. This issueis discussed in depth in (Reichart et al., 2010b).Our algorithm is deterministic3.For German, in the k=26 scenario our algorithmoutperforms Clark\\\\x92s, often outperforming even itsmaximum in 100 runs. In the k=17 scenario, ouralgorithm obtains a higher score than Clark withprobability 0.4 to 0.78, depending on the measureand scenario. Clark\\\\x92s average score is slightly bet-ter in the Many-to-1 measure, while our algorithmperforms somewhat better than Clark\\\\x92s average inthe 1-to-1 and NVI measures.The DeWAC corpus from which we extractedstatistics for the German experiments is out of do-main with respect to NEGRA. The correspond-ing corpus in English, NANC, is a newswire cor-pus and therefore clearly in-domain with respectto WSJ. This is reflected by the percentage of un-known words, which was much higher in Germanthan in English (8.1% and 1.6%), lowering results.Table 2 shows the effect of each of our algo-rithm\\\\x92s components. Each component providesan improvement over the base distributional algo-rithm. The full coordinate clustering stage (sev-eral iterations, F) considerably improves the scoreover a single iteration (F(I=1)). Capitalization in-formation increases the score more than the mor-phological information, which might stem fromthe granularity of the POS tag set with respect tonames. This analysis is supported by similar ex-periments we made in the \\\\x91Coarse k=13\\\\x92 scenario(not shown in tables here). There, the decrease inperformance was only of 1%\\\\x962% in the mapping3The fluctuations inflicted on our algorithm by the randommapping of unknown words are of less than 0.1% .1304Excluding Punctuation Including Punctuation Perfect PunctuationM-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI VVan Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:perfect assignment of punctuation is assumed.based measures and 3.5% in the V measure.Finally, Table 4 presents reported results for allrecent algorithms we are aware of that tackled thetask of unsupervised POS induction from plaintext. Results for our algorithm\\\\x92s and Clark\\\\x92s arereported for the \\\\x91Fine, k=34\\\\x92 scenario. The set-tings of the various experiments vary in terms ofthe exact annotation scheme used (coarse or finegrained) and the size of the test set. However, thescore differences are sufficiently large to justifythe claim that our algorithm is currently the bestperforming algorithm on the PTB-WSJ corpus forPOS induction from plain text4.Since previous works provided results only forthe scenario in which punctuation is included, thereported results are not directly comparable. Inorder to quantify the effect various punctuationschemes have on the results, we evaluated the\\\\x91iHMM: PY-fixed\\\\x92 model (Van Gael et al., 2009)and ours when punctuation is excluded, includedor perfectly tagged5. The results (Table 3) indi-cate that most probably even after an appropriatecorrection for punctuation, our model remains thebest performing one.8 DiscussionIn this work we presented a novel unsupervised al-gorithm for POS induction from plain text. The al-gorithm first generates relatively accurate clustersof high frequency words, which are subsequentlyused to bootstrap the entire clustering. The dis-tributional and morphological representations thatwe use are novel for this task.We experimented on two languages with map-ping and information theoretic clustering evalua-tion measures. Our algorithm obtains the best re-ported results on the English PTB-WSJ corpus. Inaddition, our results are almost always better thanClark\\\\x92s on the German NEGRA corpus.4Grac\\\\xb8a et al. (2009) report very good results for 17 tags inthe M-1 measure. However, their 1-1 results are quite poor,and results for the common IT measures were not reported.Their results for 45 tags are considerably lower.5We thank the authors for sending us their data.We have also performed a manual error anal-ysis, which showed that our algorithm performsmuch better on closed classes than on openclasses. In order to asses this quantitatively, letus define a random variable for each of the goldclusters, which receives a value corresponding toeach induced cluster with probability proportionalto their intersection size. For each gold cluster,we compute the entropy of this variable. In ad-dition, we greedily map each induced cluster to agold cluster and compute the ratio between theirintersection size and the size of the gold cluster(mapping accuracy).We experimented in the \\\\x91Fine k=34\\\\x92 scenario.The clusters that obtained the best scores were(brackets indicate mapping accuracy and entropyfor each of these clusters) coordinating conjunc-tions (95%, 0.32), prepositions (94%, 0.32), de-terminers (94%, 0.44) and modals (93%, 0.45).These are all closed classes.The classes on which our algorithm performedworst consist of open classes, mostly verb types:past tense verbs (47%, 2.2), past participle verbs(44%, 2.32) and the morphologically unmarkednon-3rd person singular present verbs (32%, 2.86).Another class with low performance is the propernouns (37%, 2.9). The errors there are mostlyof three types: confusions between common andproper nouns (sometimes due to ambiguity), un-known words which were put in the unknownwords cluster, and abbreviations which were givena separate class by our algorithm. Finally, the al-gorithm\\\\x92s performance on the heterogeneous ad-verbs class (19%, 3.73) is the lowest.Clark\\\\x92s algorithm exhibits6 a similar patternwith respect to open and closed classes. Whilehis algorithm performs considerably better on ad-verbs (15% mapping accuracy difference and 0.71entropy difference), our algorithm scores consid-erably better on prepositions (17%, 0.77), su-perlative adjectives (38%, 1.37) and plural propernames (45%, 1.26).6Using average mapping accuracy and entropy over the100 runs.1305Naturally, this analysis might reflect the arbi-trary nature of a manually design POS tag setrather than deficiencies in automatic POS induc-tion algorithms. In future work we intend to ana-lyze the output of such algorithms in order to im-prove POS tag sets.Our algorithm and Clark\\\\x92s are monosemous(i.e., they assign each word exactly one tag), whilemost other algorithms are polysemous. In order toassess the performance loss caused by the monose-mous nature of our algorithm, we took the M-1greedy mapping computed for the entire datasetand used it to compute accuracy over the monose-mous and polysemous words separately. Resultsare reported for the English \\\\x91Fine k=34\\\\x92 scenario(without punctuation). We define a word to bemonosemous if more than 95% of its tokens areassigned the same gold standard tag. For English,there are approximately 255K polysemous tokensand 578K monosemous ones. As expected, ouralgorithm is much more accurate on the monose-mous tokens, achieving 76.6% accuracy, com-pared to 47.1% on the polysemous tokens.The evaluation in this paper is done at the tokenlevel. Type level evaluation, reflecting the algo-rithm\\\\x92s ability to detect the set of possible POStags for each word type, is important as well. Itcould be expected that a monosemous algorithmsuch as ours would perform poorly in a type levelevaluation. In (Reichart et al., 2010a) we discusstype level evaluation at depth and propose typelevel evaluation measures applicable to the POSinduction problem. In that paper we compare theperformance of our Prototype Tagger with lead-ing unsupervised POS tagging algorithms (Clark,2003; Goldwater and Griffiths, 2007; Gao andJohnson, 2008; Van Gael et al., 2009). Our al-gorithm obtained the best results in 4 of the 6measures in a margin of 4\\\\x966%, and was secondbest in the other two measures. Our results werebetter than Clark\\\\x92s (the only other monosemousalgorithm evaluated there) on all measures in amargin of 5\\\\x9621%. The fact that our monose-mous algorithm was better than good polysemousalgorithms in a type level evaluation can be ex-plained by the prototypical nature of the POS phe-nomenon (a longer discussion is given in (Reichartet al., 2010a)). However, the quality upper boundfor monosemous algorithms is obviously muchlower than that for polysemous algorithms, andwe expect polysemous algorithms to outperformmonosemous algorithms in the future in both typelevel and token level evaluations.The skewed (Zipfian) distribution of POS classfrequencies in corpora is a problem for many POSinduction algorithms, which by default tend to in-duce a clustering having a balanced distribution.Explicit modifications to these algorithms were in-troduced in order to bias their model to producesuch a distribution (see (Clark, 2003; Johnson,2007; Reichart et al., 2010b)). An appealing prop-erty of our model is its ability to induce a skeweddistribution without being explicitly tuned to doso, as seen in Figure 2.Acknowledgements. We would like to thankYoav Seginer for his help with his parser.ReferencesMichele Banko and Robert C. Moore, 2004. Part ofSpeech Tagging in Context. COLING \\\\x9204.Marco Baroni, Silvia Bernardini, Adriano Ferraresi andEros Zanchetta, 2009. The WaCky Wide Web: ACollection of Very Large Linguistically ProcessedWeb-Crawled Corpora. Language Resources andEvaluation.Chris Biemann, 2006. Unsupervised Part-of-Speech Tagging Employing Efficient Graph Cluster-ing. COLING-ACL \\\\x9206 Student Research Work-shop.Thorsten Brants, 1997. The NEGRA Export Format.CLAUS Report, Saarland University.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouze, Jenifer C. Lai and Robert Mercer, 1992.Class-Based N-Gram Models of Natural Language.Computational Linguistics, 18(4):467\\\\x96479.Alexander Clark, 2003. Combining Distributional andMorphological Information for Part of Speech In-duction. EACL \\\\x9203.Mathias Creutz and Krista Lagus, 2005. Inducing theMorphological Lexicon of a Natural Language fromUnannotated Text. AKRR \\\\x9205.Sajib Dasgupta and Vincent Ng, 2007. Unsu-pervised Part-of-Speech Acquisition for Resource-Scarce Languages. EMNLP-CoNLL \\\\x9207.Dayne Freitag, 2004. Toward Unsupervised Whole-Corpus Tagging. COLING \\\\x9204.Jianfeng Gao and Mark Johnson, 2008. A Compar-ison of Bayesian Estimators for Unsupervised Hid-den Markov Model POS Taggers. EMNLP \\\\x9208.Yoav Goldberg, Meni Adler and Michael Elhadad,2008. EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start). ACL \\\\x9208.1306John Goldsmith, 2001. Unsupervised Learning of theMorphology of a Natural Language. ComputationalLinguistics, 27(2):153\\\\x96198.Sharon Goldwater and Tom Griffiths, 2007. FullyBayesian Approach to Unsupervised Part-of-SpeechTagging. ACL \\\\x9207.Joa\\\\x98o Grac\\\\xb8a, Kuzman Ganchev, Ben Taskar and Fre-nando Pereira, 2009. Posterior vs. Parameter Spar-sity in Latent Variable Models. NIPS \\\\x9209.David Graff, 1995. North American News Text Cor-pus. Linguistic Data Consortium. LDC95T21.Aria Haghighi and Dan Klein, 2006. Prototype-drivenLearning for Sequence Labeling. HLT\\\\x96NAACL \\\\x9206.Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,1999. Data Clustering: A Review. ACM ComputingSurveys 31(3):264\\\\x96323.Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-tomatic Adaptation of Annotation Standards: Chi-nese Word Segmentation and POS Tagging \\\\x96 A CaseStudy. ACL \\\\x9209.Mark Johnson, 2007. Why Doesnt EM Find GoodHMM POS-Taggers? EMNLP-CoNLL \\\\x9207.Harold W. Kuhn, 1955. The Hungarian method forthe Assignment Problem. Naval Research LogisticsQuarterly, 2:83-97.Marina Meila, 2007. Comparing Clustering \\\\x96 an In-formation Based Distance. Journal of MultivariateAnalysis, 98:873\\\\x96895.Bernard Merialdo, 1994. Tagging English Text witha Probabilistic Model. Computational Linguistics,20(2):155\\\\x96172.James Munkres, 1957. Algorithms for the Assignmentand Transportation Problems. Journal of the SIAM,5(1):32\\\\x9638.Sujith Ravi and Kevin Knight, 2009. Minimized Mod-els for Unsupervised Part-of-Speech Tagging. ACL\\\\x9209.Roi Reichart and Ari Rappoport, 2008. UnsupervisedInduction of Labeled Parse Trees by Clustering withSyntactic Features. COLING \\\\x9208.Roi Reichart and Ari Rappoport, 2009. The NVI Clus-tering Evaluation Measure. CoNLL \\\\x9209.Roi Reichart, Omri Abend and Ari Rappoport, 2010a.Type Level Clustering Evaluation: New Measuresand a POS Induction Case Study. CoNLL \\\\x9210.Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.Improved Unsupervised POS Induction Using In-trinsic Clustering Quality and a Zipfian Constraint.CoNLL \\\\x9210.Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A Conditional Entropy-Based ExternalCluster Evaluation Measure. EMNLP \\\\x9207.Hinrich Schu\\\\xa8tze, 1995. Distributional part-of-speechtagging. EACL \\\\x9295.Yoav Seginer, 2007. Fast Unsupervised IncrementalParsing. ACL \\\\x9207.Noah A. Smith and Jason Eisner, 2005. ContrastiveEstimation: Training Log-Linear Models on Unla-beled Data. ACL \\\\x9205.John R. Taylor, 2003. Linguistic Categorization: Pro-totypes in Linguistic Theory, Third Edition. OxfordUniversity Press.Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-mani, 2009. The Infinite HMM for UnsupervisedPOS Tagging. EMNLP \\\\x9209.Qin Iris Wang and Dale Schuurmans, 2005. Im-proved Estimation for Unsupervised Part-of-SpeechTagging. IEEE NLP\\\\x96KE \\\\x9205.Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-supervised Learner for POS Disambiguation RulesGiven Only a Minimal Lexicon. EMNLP \\\\x9209.1307'\n",
            "b'Word Sense Induction:Triplet-Based Clustering and Automatic EvaluationStefan BordagNatural Language Processing DepartmentUniversity of LeipzigGermanysbordag@informatik.uni-leipzig.deAbstractIn this paper a novel solution to auto-matic and unsupervised word sense induc-tion (WSI) is introduced. It represents aninstantiation of the \\\\x91one sense per colloca-tion\\\\x92 observation (Gale et al., 1992). Likemost existing approaches it utilizes clus-tering of word co-occurrences. This ap-proach differs from other approaches toWSI in that it enhances the effect of theone sense per collocation observation byusing triplets of words instead of pairs.The combination with a two-step cluster-ing process using sentence co-occurrencesas features allows for accurate results. Ad-ditionally, a novel and likewise automaticand unsupervised evaluation method in-spired by Schu\\\\xa8tze\\\\x92s (1992) idea of evalu-ation of word sense disambiguation algo-rithms is employed. Offering advantageslike reproducability and independency ofa given biased gold standard it also en-ables automatic parameter optimization ofthe WSI algorithm.1 IntroductionThe aim of word sense induction1 (WSI) is to findsenses of a given target word (Yarowski, 1995)automatically and if possible in an unsupervisedmanner. WSI is akin to word sense disambiguation(WSD) both in methods employed and in prob-lems encountered, such as vagueness of sense dis-tinctions (Kilgarriff, 1997). The input to a WSI al-gorithm is a target word to be disambiguated, e.g.1Sometimes called word sense discovery (Dorow andWiddows, 2003) or word sense discrimination (Purandare,2004; Velldal, 2005)space, and the output is a number of word sets rep-resenting the various senses, e.g. (3-dimensional,expanse, locate) and (office, building, square).Such results can be at the very least used as empir-ically grounded suggestions for lexicographers oras input for WSD algorithms. Other possible usesinclude automatic thesaurus or ontology construc-tion, machine translation or information retrieval.But the usefulness of WSI in real-world applica-tions has yet to be tested and proved.2 Related workA substantial number of different approaches toWSI has been proposed so far. They are all basedon co-occurrence statistics, albeit using differ-ent context representations such as co-occurrenceof words within phrases (Pantel and Lin, 2002;Dorow and Widdows, 2003; Velldal, 2005), bi-grams (Schu\\\\xa8tze, 1998; Neill, 2002; Udani et al.,2005), small windows around a word (Gauch andFutrelle, 1993), or larger contexts such as sen-tences (Bordag, 2003; Rapp, 2004) or large win-dows of up to 20 words (Ferret, 2004). Moreoverthey all employ clustering methods to partition theco-occurring words into sets describing conceptsor senses. Some algorithms aim for a global clus-tering of words into concepts (Yarowski, 1995;Pantel and Lin, 2002; Velldal, 2005). But the ma-jority of algorithms are based on a local cluster-ing: Words co-occurring with the target word aregrouped into the various senses the target wordhas. It is not immediately clear which approachto favor, however aiming at global senses has theinherent property to produce a uniform granular-ity of distinctions between senses that might notbe desired (Rapp, 2004).Graph-based algorithms differ from the ma-jority of algorithms in several aspects. Words137can be taken as nodes and co-occurrence of twowords defines an edge between the two respec-tive nodes. Activation spreading on the resultinggraph can be employed (Barth, 2004) in order toobtain most distinctly activated areas in the vicin-ity of the target word. It is also possible to usegraph-based clustering techniques to obtain senserepresentations based on sub-graph density mea-sures (Dorow and Widdows, 2003; Bordag, 2003).However, it is not yet clear, whether this kind ofapproach differs qualitatively from the standardclustering approaches. Generally though, the no-tion of sub-graph density seems to be more intu-itive compared to the more abstract clustering.There are different types of polysemy, themost significant distinction probably being be-tween syntactic classes of the word (e.g. to plantvs. a plant) and conceptually different senses (e.g.power plant vs. green plant). As known fromwork on unsupervised part-of-speech tagging (Ro-hwer and Freitag, 2004; Rapp, 2005), the size ofthe window in which words will be found simi-lar to the target word plays a decisive role. Us-ing most significant direct neighbours as contextrepresentations to compare words results in pre-dominantly syntactical similarity to be found. Onthe other hand, using most significant sentence co-occurrences results in mostly semantical similarity(Curran, 2003). However, whereas various contextrepresentations, similarity measures and cluster-ing methods have already been compared againsteach other (Purandare, 2004), there is no evidenceso far, whether the various window sizes or otherparameters have influence on the type of ambigu-ity found, see also (Manning and Schu\\\\xa8tze, 1999,p. 259).Pantel & Lin (2002) introduced an evalua-tion method based on comparisons of the ob-tained word senses with senses provided in Word-Net. This method has been successfully used byother authors as well (Purandare, 2004; Ferret,2004) because it is straightforward and producesintuitive numbers that help to directly estimatewhether the output of a WSI algorithm is mean-ingful. On the other hand, any gold standard suchasWordNet is biased and hence also lacks domain-specific sense definitions while providing an abun-dance of sense definitions that occur too rarely inmost corpora. For example in the British NationalCorpus (BNC), the sense #2 of MALE ([n] thecapital of Maldives) from WordNet is representedby a single sentence only. Furthermore, compar-ing results of an algorithm to WordNet automat-ically implies another algorithm that matches thefound senses with the senses in WordNet. This isvery similar to the task of WSD and therefore canbe assumed to be similarly error prone. These rea-sons have led some researchers to opt for a man-ual evaluation of their algorithms (Neill, 2002;Rapp, 2004; Udani et al., 2005). Manual evalu-ation, however, has its own disadvantages, mostnotably the poor reproducability of results. In thiswork a pseudoword based evaluation method simi-lar to Schu\\\\xa8tze\\\\x92s (1992) pseudoword method is em-ployed. It is automatic, easy to reproduce andadapts well to domain specificity of a given cor-pus.3 Triplet-based algorithmThe algorithm proposed in this work is based onthe one sense per collocation observation (Galeet al., 1992). That essentially means that when-ever a pair of words co-occurs significantly of-ten in a corpus (hence a collocation), the con-cept referenced by that pair is unambiguous, e.g.growing plant vs. power plant. However, asalso pointed out by Yarowsky (1995), this ob-servation does not hold uniformly over all possi-ble co-occurrences of two words. It is strongerfor adjacent co-occurrences or for word pairs in apredicate-argument relationship than for arbitraryassociations at equivalent distance, e.g. a plantis much less clear-cut. To alleviate this problem,the first step of the presented algorithm is to buildtriplets of words (target word and two of it\\\\x92s co-occurrences) instead of pairs (target word and oneco-occurrence). This means that a plant is furtherrestricted by another word and even a stop wordsuch as on rules several possibilities of interpreta-tion of a plant out or at least makes them a lot lessimprobable.The algorithm was applied to two types of co-occurrence data. In order to show the influence ofwindow size, both the most significant sentence-wide co-occurrences and direct neighbour co-occurrences were computed for each word. Thesignificance values are obtained using the log-likelihood measure assuming a binomial distrib-ution for the unrelatedness hypothesis (Dunning,1993). For each word, only the 200 most signifi-cant co-occurrences were kept. This threshold andall others to follow were chosen after experiment-138ing with the algorithm. However, as will be shownin section 4, the exact set-up of these numbersdoes not matter. The presented evaluation methodenables to find the optimal configuration of para-meters automatically using a genetic algorithm.The core assumption of the triplet-based al-gorithm is, that any three (or more) words ei-ther uniquely identify a topic, concept or sense.Using the previously acquired most significantco-occurrences (of both types), the lists of co-occurrences for all three words of a triplet areintersected to retain words contained in all threelists. If the three words cover a topic, e.g. space,NASA, Mars, then the intersection will not beempty, e.g. launch, probe, cosmonaut, .... If thethree words do not identify a meaningful topic,e.g. space, NASA, cupboard, then the intersectionwill most likely contain few to no words at all. In-tersections of triplets built from function words arevery likely to contain many co-occurrences evenif they do not identify a unique topic. These so-called \\\\x91stop words\\\\x92 are thus removed both from theco-occurrences from which triplets are built andfrom the co-occurrences which are used as fea-tures.It is straightforward then to create all possibletriplets of the co-occurrences of the target wordw and to compute the intersection of their co-occurrence lists. Using these intersections as fea-tures of the triplets, it is possible to group tripletsof words together that have similar features bymeans of any standard clustering algorithm. How-ever, in order to \\\\x92tie\\\\x92 the referenced meanings ofthe triplets to the target word w, the resultingset of triplets can be restricted only to those thatalso contain the target word. This has the usefulside-effect that it reduces the number of tripletsto cluster. To further reduce the remaining num-ber of(2002)= 19900 items to be clustered, aniterative incremental windowing mechanism hasbeen added. Instead of clustering all triplets inone step, 30 co-occurrences beginning from themost significant ones are taken in each step tobuild(302)= 435 triplets and their intersections.The resulting elements (triplets and intersectionsof their respective co-occurrences as features) arethen clustered with the clusters remaining from theprevious step.In each step of the clustering algorithm, thewords from the triplets and the features aremerged, if the overlap factor similarity measure(Curran, 2003) found them to be similar enough(over 80% overlapping words out of 200). Thus, ifthe element (space, NASA, Mars) : (orbital, satel-lite, astronauts,...) and (space, launch, Mars) :(orbit, satellite, astronaut, ...) were found to besimilar, they are merged to (space=2, NASA=1,Mars=1, launch=1) : (orbital=1, satellite=2, as-tronauts=1, orbit=1, astronaut=1, ...). Since themeasure utilizes only the features for comparisons,the result can contain two or more clusters havingalmost identical key sets (which result from merg-ing triplets). A post-clustering step is thereforeapplied in order to compare clusters by the for-merly triplet words and merge spurious sense dis-tinctions. After having thus established the finalclusters, the words that remain unclustered can beclassified to the resulting clusters. Classificationis performed by comparing the co-occurrences ofeach remaining word to the agglomerated featurewords of each sense. If the overlap similarity tothe most similar sense is below 0.8 the given wordis not classified. The entire cluster algorithm canthen be summarized as follows:\\\\x95 Target word is w\\\\x95 for each step take the next 30 co-occurrencesof w\\\\x96 Build all possible pairs of the 30 co-occurrences and add w to each to makethem triplets\\\\x96 Compute intersections of co-occurrences of each triplet\\\\x96 Cluster the triplets using their intersec-tions as features together with clustersremaining from previous step? Whenever two clusters are foundto belong together, both the wordsfrom the triplets and the featuresare merged together, increasing theircounts\\\\x95 Cluster results of the loop by using themerged words of the triplets as features\\\\x95 Classify unused words to the resulting clus-ters if possibleIn order to reduce noise, for example introducedby triplets of unrelated words still containing a fewwords, there is a threshold of minimum intersec-tion size which was set to 4. Another parameter139worth mentioning is that after the last clusteringstep all clusters are removed which contain lessthan 8 words. Keeping track of how many timesa given word has \\\\x91hit\\\\x92 a certain cluster (in eachmerging step) enables to add a post-processingstep. In this step a word is removed from a clus-ter if it has \\\\x91hit\\\\x92 another cluster significantly moreoften.There are several issues and open questions thatarise from this entire approach. Most obviously,why to use a particular similarity measure, a par-ticular clustering method or why to merge the vec-tors instead of creating proper centroids. It is pos-sible that another combination of decisions of thiskind would produce better results. However, theoverall observation is that the results are fairly sta-ble with respect to such decisions whereas para-meters such as frequency of the target word, sizeof the corpus, balance of the various senses andothers have a much greater impact.4 EvaluationSchu\\\\xa8tze (1992) introduced a pseudoword-basedevaluation method for WSD algorithms. The ideais to take two arbitrarily chosen words like ba-nana and door and replace all occurrences of ei-ther word by the new pseudoword bananadoor.Then WSD is applied to each sentence and theamount of correctly disambiguated sentences ismeasured. A disambiguation in this case is cor-rect, if the sentence like I ate the banana is as-signed to sense #1 (banana) instead of #2 (door).In other words all sentences where one of the twowords occurs are viewed as one set and the WSDalgorithm is then supposed to sort them correctlyapart. This, in fact, is very similar to the WSItask, which is supposed to sort the set of wordsapart that co-occur with the target word and referto its different meanings. Thus, again it is possibleto take two words, view their co-occurrences asone set and let the WSI algorithm sort them apart.For example, the word banana might have co-occurrences such as apple, fruit, coconut, ... andthe word door co-occurrences such as open, front,locked, .... The WSI algorithm would thereforehave to disambiguate the pseudoword bananadoorwith the co-occurrences apple, open, fruit, front,locked, ....In short, the method merges the co-occurrencesof two words into one set of words. Then, the WSIalgorithm is applied to that set of co-occurrencesand the evaluation measures the result by compar-ing it to the original co-occurrence sets. In order tofind out whether a given sense has been correctlyidentified by the WSI algorithm, its retrieval pre-cision (rP ) - the similarity of the found sensewith the original sense using the overlap measure- can be computed. In the present evaluations, thethreshold of 0.6 was chosen, which means that atleast 60% of words of the found sense must over-lap with the original sense in order to be counted asa correctly found sense. The average numbers ofsimilarity are much higher, ranging between 85%and 95%.It is further informative to measure retrieval re-call (rR) - the amount of words that have beencorrectly retrieved into the correct sense. If, e.g.,two words are merged into a pseudoword and themeaning of each of these two words is representedby 200 co-occurring words, then it could happenthat one of the senses has been correctly found bythe WSI algorithm containing 110 words with anoverlap similarity of 0.91. That means that only100 words representing the original sense were re-trieved, resulting in a 50% retrieval recall. Thisretrieval recall also has an upper bound for tworeasons. The average overlap ratio of the co-occurrences of the word pairs used for the evalua-tion was 3.6%. Another factor lowering the upperbound by an unknown amount is the fact that someof the words are ambiguous. If the algorithm cor-rectly finds different senses of one of the two orig-inal words, then only one of the found senses willbe chosen to represent the original \\\\x91meaning\\\\x92 ofthe original word. All words assigned to the othersense are lost to the other sense.Using terms from information retrieval makessense because this task can be reformulated as fol-lows: Given a set of 400 words and one out of sev-eral word senses, try to retrieve all words belong-ing to that sense (retrieval recall) without retriev-ing any wrong ones (retrieval precision). A senseis then defined as correctly found by the WSI al-gorithm, if its retrieval precision is above 60% andretrieval recall above 25%. The latter number im-plies that at least 50 words have to be retrievedcorrectly since the initial co-occurrence sets con-tained 200 words. This also assumes that 50 wordswould be sufficient to characterize a sense if theWSI algorithm is not only used to evaluate itself.The reason to set the minimum retrieval precisionto any value above 50% is to avoid a too strong140baseline, see below.Using these prerequisites it is possible to defineprecision and recall (based on retrieval precisionand retrieval recall) which will be used to measurethe quality of the WSI algorithm.Precision (P ) is defined as the number of timesthe original co-occurrence sets are properly re-stored divided by the number of different setsfound. Precision has therefore an unknown upperbound below 100%, because any two words cho-sen could be ambiguous themselves. Thus, if thealgorithm finds three meanings of the pseudowordthat might be because one of the two words wasambiguous and had two meanings, and hence pre-cision will only be 66%, although the algorithmoperated flawlessly.Recall (R) is defined as the number of sensesfound divided by the number of words merged tocreate the pseudoword. For example, recall is 60%if five words are used to create the pseudoword,but only three senses were found correctly (ac-cording to retrieval precision and retrieval recall).There is at least one possible baseline for thefour introduced measures. One is an algorithmthat does nothing, resulting in a single set of 400co-occurrences of the pseudo-word. This set hasa retrieval Precision rP of 50% compared to ei-ther of the two original \\\\x91senses\\\\x92 because for any ofthe two senses only half of the \\\\x91retrieved\\\\x92 wordsmatch. This is below the allowed 60% and thusdoes not count as a correctly found sense. Thismeans that also retrieval Recall rR, Recall R areboth 0% and Precision P in such a case (noth-ing correctly retrieved, but also nothing wrong re-trieved) is defined to be 100%.As mentioned in the previous sections, there areseveral parameters that have a strong impact onthe quality of a WSI algorithm. One interestingquestion is, whether the quality of disambigua-tion depends on the type of ambiguity: Wouldthe WSI based on sentence co-occurrences (andhence on the bag-of-words model) produce bet-ter results for two syntactically different senses orfor two senses differing by topic (as predicted bySchu\\\\xa8tze (1992)). This can be simulated by choos-ing two words of different word classes to createthe pseudoword, such as the (dominantly) nouncommittee and the (dominantly) verb accept.Another interesting question concerns the influ-ence of frequency of either the word itself or thesense to be found. The latter, for example, canbe simulated by choosing one high-frequent wordand one low-frequent word, thus representing awell-represented vs. a poorly represented sense.The aim of the evaluation is to test the describedparameters and produce an overall average of pre-cision and recall and at the same time make it com-pletely reproducable by third parties. Thereforethe raw BNCwithout baseform reduction (becauselemmatization introduces additional ambiguity) orPOS-tags was used and nine groups each contain-ing five words were picked semi-randomly (avoid-ing extremely ambiguous words, with respect toWordNet, if possible):\\\\x95 high frequent nouns (Nh): picture, average,blood, committee, economy\\\\x95 medium frequent nouns (Nm): disintegra-tion, substrate, emigration, thirst, saucepan\\\\x95 low frequent nouns (Nl): paratuberculosis,gravitation, pharmacology, papillomavirus,sceptre\\\\x95 high frequent verbs (Vh): avoid, accept, walk,agree, write\\\\x95 medium frequent verbs (Vm): rend, confine,uphold, evoke, varnish\\\\x95 low frequent verbs (Vl): immerse, disengage,memorize, typify, depute\\\\x95 high frequent adjectives (Ah): useful, deep,effective, considerable, traditional\\\\x95 medium frequent adjectives (Am): ferocious,normative, phenomenal, vibrant, inactive\\\\x95 low frequent adjectives (Al): astrological,crispy, unrepresented, homoclinic, bitchyThese nine groups were used to design fourstests, each focussing on a different variable. Thehigh frequent nouns are around 9000 occurrences,medium frequent around 300 and low frequentaround 50.4.1 Influence of word class and frequencyIn the first run of all four tests, sentence co-occurrences were used as features. In the firsttest, all words of equal word class were viewedas one set of 15 words. This results in(152)= 105possibilities to combine two of these words into141a pseudoword and test the results of the WSI al-gorithm. The purpose of this test is to examinewhether there is a tendency for senses of certainword classes to be easier induced. As can be seenfrom Table 1, sense induction of verbs using sen-tence co-occurrences performs worse compared tonouns. This could be explained by the fact thatverbs are less semantically specific and need moresyntactic cues or generalizations - both hardly cov-ered by the underlying bag-of-words model - inorder to be disambiguated properly. At the sametime, nouns and adjectives are much better dis-tinguishable by topical key words. These resultsseem to be in unison with the prediction made bySchu\\\\xa8tze (1992).P R rP rRNhml 86.97% 86.67% 90.94% 64.21%Vhml 78.32% 64.29% 80.23% 55.20%Ahml 88.57% 70.95% 87.96% 65.38%Table 1: Influence of the syntactic class of the in-put word in Test 1. Showing precision P and re-call R, as well as average retrieval precision rPand recall rR.In the second test, all three types of possiblecombinations of the word classes are tested, i.e.pseudowords consisting of a noun and a verb, anouns and an adjective and a verb with an adjec-tive. For each combination there are 15 \\\\xb7 15 = 225possibilities of combining a word from one wordclass with a word from another word class. Thepurpose of this test was to demonstrate possibledifferences between WSI of different word classcombinations. This corresponds to cases when oneword form can be both a nound and a verb, e.g. awalk and to walk or a noun and an adjective, forexample a nice color and color TV. However, theresults in Table 2 show no clear tendencies otherthan perhaps that WSI of adjectival senses fromverb senses seems to be slightly more difficult.P R rP rRN/V 86.58% 77.11% 90.51% 61.87%N/A 90.87% 78.00% 90.36% 66.75%V/A 80.84% 63.56% 81.98% 60.89%Table 2: Influence of the syntactic classes of thesenses to be found in Test 2.The third test was designed to show the in-fluence of frequency of the input word. Allwords of equal frequency are taken as one groupwith(152)= 105 possible combinations. The re-sults in Table 3 show a clear tendency for higher-frequent word combinations to achieve a betterquality of WSI over lower frequency words. Thesteep performance drop in recall becomes imme-diately clear when looking at the retrieval recallof the found senses. This is not surprising, sincewith the low frequency words, each occuring onlyabout 50 times in the BNC, the algorithm runs intothe data sparseness problem that has already beenpointed out as problematic for WSI (Ferret, 2004).P R rP rRhigh 93.65% 78.10% 90.25% 80.70%med. 84.59% 85.24% 89.91% 54.55%low 74.76% 49.52% 71.01% 41.66%Table 3: Influence of frequency of the input wordin Test 3.The fourth test finally shows which influencethe overrepresentation of one sense over anotherhas on WSI. For this purpose, three possible com-binations of frequency classes, high-frequentwith middle, high with low and middle with low-frequent words were created with 15 \\\\xb7 15 = 225possible word pairs. Table 4 demonstrates a steepdrop in recall whenever a low-frequent word ispart of the pseudoword. This reflects the fact thatit is more difficult for the algorithm to find thesense that was represented by the less frequentword. The unusually high precision value for thehigh/low combination can be explained by the factthat in this case mostly only one sense was found(the one of the frequent word). Therefore recall isclose to 50% whereas precision is closer to 100%.P R rP rRh/m 86.43% 79.56% 92.72% 72.08%h/l 91.19% 67.78% 90.85% 74.52%m/l 82.33% 74.00% 85.29% 49.87%Table 4: Influence of different representation ofsenses based on frequency of the two constituentsof the pseudoword in Test 4.Finally it is possible to provide the averages forthe entire test runs comprising 1980 tests. Themacro averages over all tests are P = 85.42%,R = 72.90%, rP = 86.83% and rR = 62.30%,the micro averages are almost the same. Using thesame thresholds but only pairs instead of triplets142results in P = 91.00%, R = 60.40%, rP =83.94% and rR = 62.58%. Or in other words,more often only one sense is retrieved and the F-measures of F = 78.66% for triplets compared toF = 72.61% for pairs confirm an improvement by6% by using triplets.4.2 Window sizeThe second run of all four tests using direct neigh-bors as features failed due to the data sparse-ness problem. There were 17.5 million wordpairs co-occurring significantly within sentencesin the BNC according to the log-likelihood mea-sure used. Even there, words with low frequencyshowed a strong performance loss as compared tothe high-frequent words. Compared to that therewere only 2.3 million word pairs co-occurring di-rectly next to each other. The overall results ofthe second run with macro averages P = 56.01%,R = 40.64%, rP = 54.28% and rR = 26.79%will not be reiterated here in detail because theyare highly inconclusive due to the data sparseness.The inconclusiveness derives from the fact thatcontrary to the results of the first run, the resultshere vary strongly for various parameter settingsand cannot be considered as stable.Although these results are insufficient to showthe influence of context representations on the typeof induced senses as they were supposed to, theyallow several other insights. Firstly, corpus sizedoes obviously matter for WSI as more data wouldprobably have alleviated the sparseness problem.Secondly, while perhaps one context representa-tion might be theoretically superior to another(such as neighbor co-occurrences vs. sentenceco-occurrences), the effect various representationshave on the data richness were by far stronger inthe presented tests.4.3 ExamplesIn the light of rather abstract, pseudoword-basedevaluations some real examples sometimes helpto reduce the abstractness of the presented results.Three words, sheet, line and space were chosen ar-bitrarily and some words representing the inducedsenses are listed below.\\\\x95 sheet\\\\x96 beneath, blank, blanket, blotting, bot-tom, canvas, cardboard\\\\x96 accounts, amount, amounts, asset, as-sets, attributable, balance\\\\x95 line\\\\x96 angle, argument, assembly, axis, bot-tom, boundary, cell, circle, column\\\\x96 lines, link, locomotive, locomotives,loop, metres, mouth, north, parallel\\\\x95 space\\\\x96 astronaut, launch, launched, manned,mission, orbit, rocket, satellite\\\\x96 air, allocated, atmosphere, blank,breathing, buildings, ceiling, confinedThese examples show that the found differentia-tions between senses of words indeed are intuitive.They also show that the found senses are only themost distinguishable ones and many futher sensesare missing even though they do appear in theBNC, some of them even frequently. It seemsthat for finer grained distinctions the bag-of-wordsmodel is not appropriate, although it might proveto be sufficient for other applications such as In-formation Retrieval. Varying contextual represen-tations might prove to be complementary to the ap-proach presented here and enable the detection ofsyntactic differences or collocational usages of aword.5 ConclusionsIt has been shown that the approach presented inthis work enables automatic and knowledge-freeword sense induction on a given corpus with highprecision and sufficient recall values. The inducedsenses of the words are inherently domain-specificto the corpus used. Furthermore, the inducedsenses are only the most apparent ones while thetype of ambiguity matters less than expected. Butthere is a clear preference for topical distinctionsover syntactic ambiguities. The latter effect isdue to the underlying bag-of-words model, hencealternative contextual representations might yielddifferent (as opposed to better/worse) results. Thisbag-of-words limitation also implies some sensesto be found that would be considered as spuriousin other circumstances. For example, the wordchallenger induces 5 senses, three of them de-scribing the opponent in a game. The differencesfound are strong, however, as the senses distin-guished are between a chess-challenger, a GrandPrix challenger and a challenger in boxing, eachhave a large set of specific words distinguishingthe senses.143There are several questions that remain open.As the frequency of a word has a great impacton the possibility to disambiguate it correctly us-ing the presented methods, the question is to whatextent corpus size plays a role in this equation ascompared to balancedness of the corpus and there-fore the senses to be found. Another question isconnected to the limitation of the presented algo-rithm which requires that any sense to be inducedhas to be representable by a rather large amount ofwords. The question then is, whether this (or anyother similar) algorithm can be improved to dis-cern \\\\x91small\\\\x92 senses from random noise. A com-bination with algorithms finding collocational us-ages of words probably offers a feasible solution.The evaluation method employed can be usedfor automatic optimization of the algorithm\\\\x92s ownparameters using genetic algorithms. Moreover, itwould be interesting to employ genetic program-ming in order to let an optimal word sense induc-tion algorithm design itself.ReferencesMichael Barth. 2004. Extraktion von Textele-menten mittels \\\\x94spreading activation\\\\x94 fu\\\\xa8r indikativeTextzusammenfassungen. Master\\\\x92s thesis, Univer-sity of Leipzig.Stefan Bordag. 2003. Sentence co-occurrences assmall-world-graphs: A solution to automatic lexi-cal disambiguation. In Proceedings of CICling-03,LNCS 2588, pages 329\\\\x96333. Springer.James Richard Curran. 2003. From Distributional toSemantic Similarity. Ph.D. thesis, Institute for Com-municating and Collaborative Systems, School ofInformatics. University of Edinburgh.Beate Dorow and Dominic Widdows. 2003. Discover-ing corpus-specific word senses. In Proceedings ofEACL 2003, pages 79\\\\x9682, Budapest, Hungary.Ted E. Dunning. 1993. Accurate methods for the sta-tistics of surprise and coincidence. ComputationalLinguistics, 19(1):61\\\\x9674.Olivier Ferret. 2004. Discovering word senses froma network of lexical cooccurrences. In Proceedingsof Coling 2004, pages 1326\\\\x961332, Geneva, Switzer-land, August.William Gale, Kenneth Ward Church, and DavidYarowsky. 1992. Work on statistical methods forword sense disambiguation. Intelligent Probabilis-tic Approaches to Natural Language, Fall Sympo-sium Series(FS-92-04):54\\\\x9660, March.Susan Gauch and Robert P. Futrelle. 1993. Experi-ments in automatic word class and word sense iden-tification for information retrieval. In Proceedingsof 3rd Annual Symposium on Document Analysisand Information Retrieval, pages 425\\\\x96434.Adam Kilgarriff. 1997. I don\\\\x92t believe in word senses.Computers and the Humanities, 31(2):91\\\\x96113.Christopher D. Manning and Hinrich Schu\\\\xa8tze. 1999.Foundations of Statistical Natural LanguageProcessing. MIT Press.Daniel B. Neill. 2002. Fully automatic word senseinduction by semantic clustering. Master\\\\x92s thesis,Cambridge University.Patrick Pantel and Dekang Lin. 2002. Discoveringword senses from text. In Proceedings of ACMSIGKDD, pages 613\\\\x96619, Edmonton.Amruta Purandare. 2004. Word sense discrimina-tion by clustering similarity contexts. Master\\\\x92s the-sis, Department of Computer Science, University ofMinnesota, Duluth.Reinhard Rapp. 2004. Mining text for word sensesusing independent component analysis. In Pro-ceedings of SIAM International Conference on DataMining 2004.Reinhard Rapp. 2005. A practical solution to the prob-lem of automatic part-of-speech induction from text.In Proceedings of the ACL Interactive Poster andDemonstration Sessions, pages 77\\\\x9680, Ann Arbor,June. ACL.Richard Rohwer and Dayne Freitag. 2004. Towardsfull automation of lexicon construction. In Proceed-ings of HLT-NAACL 04: Computational Lexical Se-mantics Workshop, Boston, MA.Hinrich Schu\\\\xa8tze. 1992. Context space. In WorkingNotes of the AAAI Fall Symposium on ProbabilisticApproaches to Natural Language, pages 113\\\\x96120,Menlo Park, CA. AAAI Press.Hinrich Schu\\\\xa8tze. 1998. Automatic word sense dis-crimination. Computational Linguistics, 24:97\\\\x96124.Goldee Udani, Shachi Dave, Anthony Davis, and TimSibley. 2005. Noun sense induction using websearch results. In Proceedings of 28th ACM SIGIR,pages 657\\\\x96658, Salvador, Brazil.Erik Velldal. 2005. A fuzzy clustering approach toword sense discrimination. In Proceedings of the7th International conference on Terminology andKnowledge Engineering, Copenhagen, Denmark.David Yarowski. 1995. Unsupervised word sensedisambiguation rivaling supervised methods. ACL,33:189\\\\x96196.144'\n",
            "b'Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642\\\\x96646,Dublin, Ireland, August 23-24, 2014.The Meaning Factory: Formal Semantics for Recognizing TextualEntailment and Determining Semantic SimilarityJohannes BjervaUniv. of Groningenj.bjerva@rug.nlJohan BosUniv. of Groningenjohan.bos@rug.nlRob van der GootUniv. of Groningenr.van.der.goot@rug.nlMalvina NissimUniv. of Bolognamalvina.nissim@unibo.itAbstractShared Task 1 of SemEval-2014 com-prised two subtasks on the same datasetof sentence pairs: recognizing textual en-tailment and determining textual similar-ity. We used an existing system based onformal semantics and logical inference toparticipate in the first subtask, reachingan accuracy of 82%, ranking in the top5 of more than twenty participating sys-tems. For determining semantic similar-ity we took a supervised approach using avariety of features, the majority of whichwas produced by our system for recogniz-ing textual entailment. In this subtask oursystem achieved a mean squared error of0.322, the best of all participating systems.1 IntroductionThe recent popularity of employing distributionalapproaches to semantic interpretation has also leadto interesting questions about the relationship be-tween classic formal semantics (including its com-putational adaptations) and statistical semantics.A promising way to provide insight into thesequestions was brought forward as Shared Task 1 inthe SemEval-2014 campaign for semantic evalua-tion (Marelli et al., 2014). In this task, a system isgiven a set of sentence pairs, and has to predict foreach pair whether the sentences are somehow re-lated in meaning. Interestingly, this is done usingtwo different metrics: the first stemming from theformal tradition (contradiction, entailed, neutral),and the second in a distributional fashion (a simi-larity score between 1 and 5). We participated inthis shared task with a system rooted in formal se-mantics. In particular, we were interested in find-ing out whether paraphrasing techniques could in-crease the accuracy of our system, whether mean-ing representations used for textual entailment areThis work is licensed under a Creative Commons Attribution4.0 International Licence. Page numbers and proceedingsfooter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/useful for predicting semantic similarity, and con-versely, whether similarity features could be usedto boost accuracy of recognizing textual entail-ment. In this paper we outline our method andpresent the results for both the textual entailmentand the semantic similarity task.12 Recognizing Textual Entailment2.1 OverviewThe core of our system for recognizing textual en-tailment works as follows: (i) produce a formal se-mantic representation for each sentence for a givensentence pair; (ii) translate these semantic repre-sentations into first-order logic; (iii) use off-the-shelf theorem provers and model builders to checkwhether the first sentence entails the second, orwhether the sentences are contradictory. This isessentially an improved version of the frameworkintroduced by Bos & Markert (2006).To generate background knowledge that couldassist in finding a proof we used the lexicaldatabase WordNet (Fellbaum, 1998). We alsoused a large database of paraphrases (Ganitkevitchet al., 2013) to alter the second sentence in case noproof was found at the first attempt, inspired byBosma & Callison-Burch (2006). The core sys-tem reached high precision on entailment and con-tradiction. To increase recall, we used a classifiertrained on the output from our similarity task sys-tem (see Section 3) to reclassify the \\\\x93neutrals\\\\x94 intopossible entailments.2.2 TechnicalitiesThe semantic parser that we used is Boxer (Bos,2008). It is the last component in the pipeline ofthe C&C tools (Curran et al., 2007), comprisinga tokenizer, POS-tagger, lemmatizer (Minnen et1To reproduce these results in a linux environment (withSWI Prolog) one needs to install the C&C tools (this in-cludes Boxer and the RTE system), the Vampire theoremprover, the two model builders Paradox and Mace-2, and thePPDB-1.0 XL database. Detailed instructions can be found inthe src/scripts/boxer/sick/README folder of theC&C tools.642al., 2001), and a robust parser for CCG (Steed-man, 2001). Boxer produces semantic represen-tations based on Discourse Representation Theory(Kamp and Reyle, 1993). We used the standardtranslation from Discourse Representation Struc-tures to first-order logic, rather than the one basedon modal first-order logic (Bos, 2004), since theshared task data did not contain any sentences withpropositional argument verbs.After conversion to first-order logic, wechecked with the theorem prover Vampire (Ri-azanov and Voronkov, 2002) whether a proofcould be found for the first sentence entailing thesecond, and whether a contradiction could be de-tected for the conjunction of both sentences trans-lated into first-order logic. If neither a proof nora contradiction could be found within 30 seconds,we used the model builder Paradox (Claessen andS\\\\xa8orensson, 2003) to produce a model of the twosentences separately, and one of the two sentencestogether. However, even though Paradox is an ef-ficient piece of software, it does not always returnminimal models with respect to the extensions ofthe non-logical symbols. Therefore, in a secondstep, we asked the model builder Mace-2 (Mc-Cune, 1998) to construct a minimal model for thedomain size established by Paradox. These mod-els are used as features in the similarity task (Sec-tion 3).Background knowledge is important to increaserecall of the theorem prover, but hard to acquireautomatically (Bos, 2013). Besides translating hy-pernym relations of WordNet to first-order logicaxioms, we also reasoned that it would be benefi-cial to have a way of dealing with multi-word ex-pressions. But instead of translating paraphrasesinto axioms, we used them to rephrase the inputsentence in case no proof or contradiction wasfound for the original sentence pair. Given a para-phrase SRC7?TGT, we rephrased the first sen-tence of a pair only if SRC matches with up tofour words, no words of TGT were already in thefirst sentence, and every word of TGT appeared inthe second sentence. The paraphrases themselveswere taken from PPDB-1.0 (Ganitkevitch et al.,2013). In the training phrase we found that the XLversion (comprising o2m, m2o, phrasal, lexical)gave the best results (using a larger version causeda strong decrease in precision, while smaller ver-sions lead to a decrease in recall).We trained a separate classifier in order to re-classify items judged by our RTE system as be-ing neutral. This classifier uses a single feature,namely the relatedness score for each sentencepair. As training material, we used the gold relat-edness scores from the training and trial sets. Forclassification of the test set, we used the related-ness scores obtained from our Semantic Similaritysystem (see Section 3). The classifier is a SupportVector Machine classifier, in the implementationprovided by Scikit-Learn (Pedregosa et al., 2011),based on the commonly used implementation LIB-SVM (Chang and Lin, 2011). We used the imple-mentation\\\\x92s standard parameters.2.3 ResultsWe submitted two runs. The first (primary) runwas produced by a configuration that included re-classifying the \\\\x92neutrals\\\\x92. The second run is with-out the reclassification of the neutrals. After sub-mission we ran a system that did not use the para-phrasing technique in order to measure what in-fluence the PPDB had on our performance. Theresults are summarized in Table 1. In the train-ing phase we got the best results for the configu-ration using the PPDB and reclassication, whichwas submitted as our primary run.Table 1: Results on the entailment task for varioussystem configurations.System Configuration Accuracymost frequent class baseline 56.7?PPDB, ?reclassification 77.6+PPDB, ?reclassification 79.6+PPDB, +reclassification 81.6In sum, our system for recognizing entailmentperformed well reaching 82% accuracy and byfar outperforming the most-frequent class baseline(Table 1). We show some selected examples illus-trating the strengths of our system below.Example 1627 (ENTAILMENT)A man is mixing a few ingredients in a bowlSome ingredients are being mixed in a bowl by a personExample 2709 (CONTRADICTION)There is no person boiling noodlesA woman is boiling noodles in waterExample 9051 (ENTAILMENT)A pair of kids are sticking out blue and green colored tonguesTwo kids are sticking out blue and green colored tonguesA proof for entailment is found for Ex. 1627,because for passive sentences Boxer producesa meaning representation equivalent to their ac-tive variants. A contradiction is detected forEx. 2709 because of the way negation is han-dled by Boxer. Both examples trigger backgroundknowledge from WordNet hyperonyms (man ?person; woman ? person) that is used in the643proofs.2Ex. 9051 shows how paraphrasing helps,here \\\\x93a pair of\\\\x94 7? \\\\x93two\\\\x94.3 Determining Semantic Similarity3.1 OverviewThe Semantic Similarity system follows a super-vised approach to solving the regression problemof determining the similarity between each givensentence pair. The system uses a variety of fea-tures, ranging from simpler ones such as wordoverlap, to more complex ones in the form ofdeep semantic features and features derived from acompositional distributional semantic model. Themajority of these features are derived from themodels from our RTE system (see Section 2).3.2 Technicalities3.2.1 RegressorThe regressor used is a Random Forest Regressorin the implementation provided by Scikit-Learn(Pedregosa et al., 2011). Random forests are ro-bust with respect to noise and do not overfit easily(Breiman, 2001). These two factors make them ahighly suitable choice for our approach, since weare dealing with a relatively large number of weakfeatures, i.e., features which may be seen as indi-vidually containing a rather small amount of infor-mation for the problem at hand.Our parameter settings for the regressor is fol-lows. We used a total of 1000 trees, with a maxi-mum tree depth of 20. At each node in a tree theregressor looked at maximum 3 features in orderto decide on the split. The quality of each suchsplit is determined using mean squared error asmeasure. These parameter values were optimisedwhen training on the training set, with regards toperformance on the trial set.3.2.2 Feature overviewWe used a total of 32 features for our regres-sor. Due to space constraints, we have sub-dividedour features into groups by the model/method in-volved. For all features we compared the outcomeof the original sentence pair with the outcome ofthe paraphrased sentence pairs (see Section 2.2)3.If the paraphrased sentence pair yielded a higherfeature overlap score than the original sentencepair, we utilized the former. In other words, we2In the training data around 20% of the proofs for entail-ment were established with the help of WordNet, but only 4%for detecting contradictions.3In addition to the PPDB we added handling of negations,by removing some negations {not, n\\\\x92t} and substituting oth-ers {no:a, none:some, nobody:somebody}.assume that the sentence pair generated with para-phrases is a good representation of the originalpair, and that similarities found here are an im-provement on the original score.Logical model We used the logical models cre-ated by Paradox and Mace for the two sentencesseparately, as well as a combined model (see Sec-tion 2.2). The features extracted from this modelare the proportion of overlap between the in-stances in the domain, and the proportion of over-lap between the relations in the model.Noun/verb overlap We first extracted and lem-matised all nouns and verbs from the sentencepairs. With these lemmas we calculated two newseparate features, the overlap of the noun lemmasand the overlap of the verb lemmas.Discourse Representation Structure (DRS)The two most interesting pieces of informationwhich easily can be extracted from the DRS mod-els are the agents and patients. We first extractedthe agents for both sentences in a sentence pair,and then computed the overlap between the twolists of agents. Secondly, since all sentences in thecorpus have exactly one patient, we extracted thepatient of each sentence and used this overlap as abinary feature.Wordnet novelty We build one tree containingall WordNet concepts included in the first sen-tence, and one containing all WordNet conceptsof both sentences together. The difference in sizebetween these two trees is used as a feature.RTE The result from our RTE system (entail-ment, neutral or contradiction) is used as a feature.Compositional Distributional Semantic ModelOur CDSM feature is based on word vectors de-rived using a Skip-Gram model (Mikolov et al.,2013a; Mikolov et al., 2013b). We used the pub-licly available word2vec4tool to calculate thesevectors. We trained the tool on a data set con-sisting of the first billion characters of Wikipedia5and the English part of the French-English 109corpus used in the wmt11 translation task6. TheWikipedia section of the data was pre-processedusing a script7which made the text lower case, re-moved tables etc. The second section of the datawas also converted to lower case prior to training.We trained the vectors using the following pa-rameter settings. Vector dimensionality was set4code.google.com/p/word2vec/5mattmahoney.net/dc/enwik9.zip6statmt.org/wmt11/translation-task.html#download7mattmahoney.net/dc/textdata.html644Table 2: Pearson correlation and MSE obtained on the test set for each feature group in isolation.Feature group p [?PPDB] p [+PPDB] MSE [?PPDB] MSE [+PPDB]Logical model 0.649 0.737 0.590 0.476Noun/verb overlap 0.647 0.676 0.592 0.553DRS 0.634 0.667 0.610 0.569Wordnet novelty 0.652 0.651 0.590 0.591RTE 0.621 0.620 0.626 0.627CDSM 0.608 0.609 0.681 0.679IDs 0.493 0.493 0.807 0.807Synset 0.414 0.417 0.891 0.889Word overlap 0.271 0.340 0.944 0.902Sentence length 0.227 0.228 0.971 0.971All with IDs 0.836 0.842 0.308 0.297All without IDs 0.819 0.827 0.336 0.322to 1600 with a context window of 10 words. Theskip-gram model with hierarchical softmax, and anegative sampling of 1e-3 was used.To arrive at the feature used for our regressor,we first calculated the element-wise sum of thevectors of each word in the given sentences. Wethen calculated the cosine distance between thesentences in the sentence pair.IDs One surprisingly helpful feature was eachsentence pair\\\\x92s ID in the corpus.8Since thisfeature clearly is not representative of what onewould have access to in a real-world scenario, itwas not included in the primary run.Synset Overlap We built one set for each sen-tence pair consisting of each possible lemma formof all possible noun synsets for each word. Theproportion of overlap between the two resultingsets was then used as a feature. Given cases whererelatively synonymous words are used (e.g. kidand child), these will often belong to the samesynset, thus resulting in a high overlap score.Synset Distance We first generated each possi-ble word pair consisting of one word from eachsentence. Using these pairings, we calculatedthe maximum path similarity between the nounsynsets available for these words. This calculationis restricted so that each word in the first sentencein each pair is only used once.Word overlap Our word overlap feature wascalculated by first creating one set per sentence,containing each word occurring in that sentence.8We discovered that the ordering of the entire data set wasinformative for the prediction of sentence relatedness. Wehave illustrated this by using the ordering of the sentences(i.e. the sentence IDs) as a feature in our model, and therebyobtaining better results. Relying on such a non-natural order-ing of the sentences would be methodologically flawed, andtherefore this feature was not used in our primary run.The four most common words in the corpus wereused as a stop list, and removed from each set. Theproportion of overlap between the two sets wasthen used as our word overlap feature.Sentence Lengths The difference in length be-tween the sentence pairs proved to be a somewhatuseful feature. Although mildly useful for this par-ticular data set, we do not expect this to be a par-ticularly helpful feature in real world applications.3.3 ResultsWe trained our system on 5000 sentence pairs, andevaluated it on 4927 sentence pairs. Table 2 con-tains our scores for the evaluation, broken up perfeature group. Our relatedness system yielded thehighest scores compared to all other systems inthis shared task, as measured by MSE and Spear-man correlation scores. Although our system per-formed slightly worse as measured by Pearsoncorrelation, there is no significant difference to thescores obtained by the two higher ranked systems.4 ConclusionOur work shows that paraphrasing techniques canbe used to improve the results of a textual entail-ment system. Additionally, the scores from oursemantic similarity measure could be used to im-prove the scores of the textual entailment system.Our work also shows that deep semantic featurescan be used to predict semantic relatedness.AcknowledgementsWe thank Chris Callison-Burch, Juri Ganitkevitch and ElliePavlick for getting the most out of PPDB. We also thank ourcolleagues Valerio Basile, Harm Brouwer, Kilian Evang andNoortje Venhuizen for valuable comments and feedback.645ReferencesJohan Bos and Katja Markert. 2006. Recognisingtextual entailment with robust logical inference. InJoaquin Quinonero-Candela, Ido Dagan, BernardoMagnini, and Florence d\\\\x92Alch\\\\xb4e Buc, editors, Ma-chine Learning Challenges, MLCW 2005, volume3944 of LNAI, pages 404\\\\x96426.Johan Bos. 2004. Computational Semantics in Dis-course: Underspecification, Resolution, and Infer-ence. Journal of Logic, Language and Information,13(2):139\\\\x96157.Johan Bos. 2008. Wide-Coverage Semantic Analy-sis with Boxer. In J. Bos and R. Delmonte, editors,Semantics in Text Processing. STEP 2008 Confer-ence Proceedings, volume 1 of Research in Compu-tational Semantics, pages 277\\\\x96286. College Publi-cations.Johan Bos. 2013. Is there a place for logic in rec-ognizing textual entailment? Linguistic Issues inLanguage Technology, 9(3):1\\\\x9618.Wauter Bosma and Chris Callison-Burch. 2006. Para-phrase substitution for recognizing textual entail-ment. In Proceedings of CLEF.Leo Breiman. 2001. Random forests. Machine learn-ing, 45(1):5\\\\x9632.Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-SVM: a library for support vector machines. ACMTransactions on Intelligent Systems and Technology(TIST), 2(3):27.K. Claessen and N. S\\\\xa8orensson. 2003. New techniquesthat improve mace-style model finding. In P. Baum-gartner and C. Ferm\\\\xa8uller, editors, Model Computa-tion \\\\x96 Principles, Algorithms, Applications (Cade-19 Workshop), pages 11\\\\x9627, Miami, Florida, USA.James Curran, Stephen Clark, and Johan Bos. 2007.Linguistically Motivated Large-Scale NLP withC&C and Boxer. In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 33\\\\x9636, Prague,Czech Republic.Christiane Fellbaum, editor. 1998. WordNet. An Elec-tronic Lexical Database. The MIT Press.Juri Ganitkevitch, Benjamin VanDurme, and ChrisCallison-Burch. 2013. PPDB: The paraphrasedatabase. In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL 2013), Atlanta,Georgia, June. Association for Computational Lin-guistics.Hans Kamp and Uwe Reyle. 1993. From Discourseto Logic; An Introduction to Modeltheoretic Seman-tics of Natural Language, Formal Logic and DRT.Kluwer, Dordrecht.M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,S. Menini, and R. Zamparelli. 2014. Semeval-2014task 1: Evaluation of compositional distributionalsemantic models on full sentences through seman-tic relatedness and textual entailment. In Proceed-ings of SemEval 2014: International Workshop onSemantic Evaluation.W. McCune. 1998. Automatic Proofs and Counterex-amples for Some Ortholattice Identities. Informa-tion Processing Letters, 65(6):285\\\\x96291.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013a. Efficient estimation of wordrepresentations in vector space. arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean. 2013b. Distributed representa-tions of words and phrases and their compositional-ity. In Advances in Neural Information ProcessingSystems, pages 3111\\\\x963119.Guido Minnen, John Carroll, and Darren Pearce. 2001.Applied morphological processing of english. Jour-nal of Natural Language Engineering, 7(3):207\\\\x96223.F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B. Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE. Duchesnay. 2011. Scikit-learn: Machine learn-ing in Python. Journal of Machine Learning Re-search, 12:2825\\\\x962830.A. Riazanov and A. Voronkov. 2002. The Design andImplementation of Vampire. AI Communications,15(2\\\\x963):91\\\\x96110.Mark Steedman. 2001. The Syntactic Process. TheMIT Press.646'\n",
            "b'Proceedings of the EACL 2009 Demonstrations Session, pages 41\\\\x9644,Athens, Greece, 3 April 2009. c\\\\xa92009 Association for Computational LinguisticsParsing, Projecting & Prototypes: RepurposingLinguistic Data on the WebWilliam D. LewisMicrosoft ResearchRedmond, WA 98052wilewis@microsoft.comFei XiaUniversity of WashingtonSeattle, WA 98195fxia@u.washington.edu1 IntroductionUntil very recently, most NLP tasks (e.g., parsing, tag-ging, etc.) have been confined to a very limited numberof languages, the so-called majority languages. Now,as the field moves into the era of developing tools forResource Poor Languages (RPLs)\\\\x97a vast majority ofthe world\\\\x92s 7,000 languages are resource poor\\\\x97thediscipline is confronted not only with the algorithmicchallenges of limited data, but also the sheer difficultyof locating data in the first place. In this demo, wepresent a resource which taps the large body of linguis-tically annotated data on the Web, data which can be re-purposed for NLP tasks. Because the field of linguisticshas as its mandate the study of human language\\\\x97infact, the study of all human languages\\\\x97and has whole-heartedly embraced the Web as a means for dissemi-nating linguistic knowledge, the consequence is that alarge quantity of analyzed language data can be foundon the Web. In many cases, the data is richly annotatedand exists for many languages for which there wouldotherwise be very limited annotated data. The resource,the Online Database of INterlinear text (ODIN), makesthis data available and provides additional annotationand structure, making the resource useful to the Com-putational Linguistic audience.In this paper, after a brief discussion of the previouswork on ODIN, we report our recent work on extend-ing ODIN by applying machine learning methods tothe task of data extraction and language identification,and on using ODIN to \\\\x93discover\\\\x94 linguistic knowledge.Then we outline a plan for the demo presentation.2 Background and Previous work onODINODIN is a collection of Interlinear Glossed Text (IGT)harvested from scholarly documents. In this section,we describe the original ODIN system (Lewis, 2006),and the IGT enrichment algorithm (Xia and Lewis,2007). These serve as the starting point for our currentwork, which will be discussed in the next section.2.1 Interlinear Glossed Text (IGT)In recent years, a large part of linguistic scholarly dis-course has migrated to the Web, whether it be in theform of papers informally posted to scholars\\\\x92 websites,or electronic editions of highly respected journals. In-cluded in many papers are snippets of language datathat are included as part of this linguistic discourse.The language data is often represented as InterlinearGlossed Text (IGT), an example of which is shown in(1).(1) Rhoddodd yr athro lyfr i\\\\x92r bachgen ddoegave-3sg the teacher book to-the boy yesterday\\\\x93The teacher gave a book to the boy yesterday\\\\x94(Bailyn, 2001)The canonical form of an IGT consists of three lines:a language line for the language in question, a glossline that contains a word-by-word or morpheme-by-morpheme gloss, and a translation line, usually in En-glish. The grammatical annotations such as 3sg on thegloss line are called grams.2.2 The Original ODIN SystemODIN was built in three steps. First, linguistic docu-ments that may contain instances of IGT are harvestedfrom the Web using metacrawls. Metacrawling in-volves throwing queries against an existing search en-gine, such as Google and Live Search.Second, IGT instances in the retrieved documentsare identified using regular expression \\\\x93templates\\\\x94, ef-fectively looking for text that resembles IGT. An exam-ple RegEx template is shown in (2), which matches anythree-line instance (e.g., the IGT instance in (1)) suchthat the first line starts with an example number (e.g.,(1)) and the third line starts with a quotation mark.(2) \\\\\\\\s*\\\\\\\\(\\\\\\\\d+\\\\\\\\).*\\\\\\\\s*.*\\\\\\\\s*\\\\\\\\[\\\\x91\\\\x92\"].*The third step is to determine the language of thelanguage line in an IGT instance. Our original work inlanguage ID relied on TextCat, an implementation of(Cavnar and Trenkle, 1994).As of January 2008 (the time we started our currentwork), ODIN had 41,581 instances of IGT for 731 lan-guages extracted from nearly 3,000 documents.11For a thorough discussion about how ODIN was origi-nally constructed, see (Lewis, 2006).412.3 Enriching IGT dataSince the language line in IGT data does not come withannotations (e.g., POS tags, phrase structures), Xia andLewis (2007) proposed to enrich the original IGT andthen extract syntactic information (e.g., context-freerules) to bootstrap NLP tools such as POS taggers andparsers. The enrichment algorithm has three steps: (1)parse the English translation with an English parser, (2)align the language line and the English translation viathe gloss line, and (3) project syntactic structure fromEnglish to the language line. The algorithm was testedon 538 IGTs from seven languages and the word align-ment accuracy was 94.1% and projection accuracy (i.e.,the percentage of correct links in the projected depen-dency structures) was 81.5%.3 Our recent workWe extend the previous work in three areas: (1) im-proving IGT detection and language identification, (2)testing the usefulness of the enriched IGT by answer-ing typological questions, and (3) enhancing ODIN\\\\x92ssearch facility by allowing structural and \\\\x93construc-tion\\\\x94 searches.23.1 IGT detectionThe canonical form of IGT, as presented in Section 2.1,consists of three parts and each part is on a single line.However, many IGT instances, 53.6% of instances inODIN, do not follow the canonical format for variousreasons. For instance, some IGT instances are missinggloss or translation lines as they can be recovered fromcontext (e.g., other neighboring examples or the textsurrounding the instance); other IGT instances havemultiple translations or language lines (e.g., one part inthe native script, and another in a Latin transliteration).Because of the irregular structure of IGT instances,the regular expression templates used in the originalODIN system performed poorly. We apply machinelearning methods to the task. In particular, we treat theIGT detection task as a sequence labeling problem: wetrain a classifier to tag each line with a pre-defined tagset,3 use the learner to tag new documents, and con-vert the best tag sequence into a span sequence. Whentrained on 41 documents (with 1573 IGT instances) andtested on 10 documents (with 447 instances), the F-score for exact match (i.e., two spans match iff theyare identical) is 88.4%, and for partial match (i.e., twospans match iff they overlap) is 95.4%.4 In comparison,the F-score of the RegEx approach on the same test setis 51.4% for exact match and 74.6% for partial match.2By constructions, we mean linguistically salient con-structions, such as actives, passives, relative clauses, invertedword orders, etc., in particular those we feel would be of themost benefit to linguists and computational linguists alike.3The tagset extends the standard BIO tagging scheme.4The result is produced by a Maximum Entropy learner.The results by SVM and CRF learners are similar. The detailswere reported in (Xia and Lewis, 2008).Table 1: The language distribution of the IGTs inODINRange of # of # of IGT % of IGTIGT instances languages instances instances> 10000 3 36,691 19.391000-9999 37 97,158 51.34100-999 122 40,260 21.2710-99 326 12,822 6.781-9 838 2,313 1.22total 1326 189,244 1003.2 Language IDThe language ID task here is very different from a typ-ical language ID task. For instance, the number of lan-guages in ODIN is more than a thousand and could po-tentially reach several thousand as more data is added.Furthermore, for most languages in ODIN, our trainingdata contains few to no instances of IGT. Because ofthese properties, applying existing language ID algo-rithms to the task does not produce satisfactory results.As IGTs are part of a document, there are oftenvarious cues in the document (e.g., language names)that can help predict the language ID of the IGT in-stances. We designed a new algorithm that treats thelanguage ID task as a pronoun resolution task, whereIGT instances are \\\\x93pronouns\\\\x94, language names are \\\\x93an-tecedents\\\\x94, and finding the language name of an IGTis the same as linking a pronoun (i.e., the IGT) to itsantecedent (i.e., the language name). The algorithmoutperforms existing, general-purpose language iden-tification algorithms significantly. The detail of the al-gorithm and experimental results is described in (Xia etal., 2009).Running the new IGT detection on the original threethousand ODIN documents, the number of IGT in-stances increases from 41,581 to 189,244. We then ranthe new language ID algorithm on the IGTs, and Table1 shows the language distribution of the IGTs in ODINaccording to the output of the algorithm. For instance,the third row says that 122 languages each have 100 to999 IGT instances, and the 40,260 instances in this binaccount for 21.27% of all instances in ODIN.53.3 Answering typological questionsLinguistic typology is the study of the classificationof languages, where a typology is an organization oflanguages by an enumerated list of logically possibletypes, most often identified by one or more structuralfeatures. One of the most well known and well studiedtypological types, or parameters, is that of canonicalword order, made famous by Joseph Greenberg (Green-berg, 1963).5Some IGTs are marked by the authors of the crawleddocuments as ungrammatical (usually with an asterisk \\\\x93*\\\\x94at the beginning of the language line). Those IGTs are keptin ODIN too because they could be useful to other linguists,the same reason that they were included in the original docu-ments.42In (Lewis and Xia, 2008), we described a meansfor automatically discovering the answers to a numberof computationally salient typological questions, suchas the canonical order of constituents (e.g., sententialword order, order of constituents in noun phrases) orthe existence of particular constituents in a language(e.g., definite or indefinite determiners). In these ex-periments, we tested not only the potential of IGT toprovide knowledge that could be useful to NLP, butalso for IGT to overcome biases inherent to the op-portunistic nature of its collection: (1) What we callthe IGT-bias, that is, the bias produced by the fact thatIGT examples are used by authors to demonstrate a par-ticular fact about a language, causing the collection ofIGT for a language to suffer from a potential lack ofrepresentativeness. (2) What we call the English-bias,an English-centrism in the examples brought on by thefact that most IGT examples provide a translation inEnglish, which can potentially affect subsequent en-richment of IGT data, such as through structural pro-jection. In one experiment, we automatically found theanswer to the canonical word order question for about100 languages, and the accuracy was 99% for all thelanguages with at least 40 IGT instances.6 In anotherexperiment, our system answered 13 typological ques-tions for 10 languages with an accuracy of 90%. Thediscovered knowledge can then be used for subsequentgrammar and tool development work.The knowledge we capture in IGT instances\\\\x97boththe native annotations provided by the linguists them-selves, as well as the answers to a variety of typologicalquestions discovered in IGT\\\\x97we use to populate lan-guage profiles. These profiles are a recent addition tothe ODIN site, and are available for those languageswhere sufficient data exists. Following is an exampleprofile:<Profile><language code=\"WBP\">Warlpiri</language><ontologyNamespace prefix=\"gold\">http://linguistic-ontology.org/gold.owl#</ontologyNamespace><feature=\"word_order\"><value>SVO</value></feature><feature=\"det_order\"><value>DT-NN</value></feature><feature=\"case\"><value>gold:DativeCase</value><value>gold:ErgativeCase</value><value>gold:NominativeCase</value>. . .</Profile>3.4 Enhancing ODIN\\\\x92s Value to ComputationalLinguistics: Search and Language ProfilesODIN provides a variety of ways to search across itsdata, in particular, search by language name or code,language family, and even by annotations and their re-lated concepts. Once data is discovered that fits theparticular pattern that a user is interested in, he/she can6Some IGT instances are not sentences and therefore arenot useful for answering this question. Further, those in-stances marked as ungrammatical (usually with an asterisk\\\\x93*\\\\x94) are ignored for this and all the typological questions.either display the data (where sufficient citation infor-mation exists and where the data is relatively clean) orlocate documents in which the data exists. Additionalsearch facilities allow users to search across poten-tially linguistically salient structures and return resultsin the form of language profiles. Although languageprofiles are by no means complete\\\\x97they are subjectto the availability of data to fill in the answers withinthe profiles\\\\x97they provide a summary of automaticallyavailable knowledge about that language as found inIGT (or enriched IGT).4 The Demo PresentationOur focus in this demonstration will be on the queryfeatures of ODIN. In addition, however, we will alsogive some background on how ODIN was built, showhow we see the data in ODIN being used by both thelinguistic and NLP communities, and present the kindof information available in language profiles. The fol-lowing is our plan for the demo:\\\\x95 Very brief discussion on the methods used to buildODIN (as discussed in Section 2.2, 3.1, and 3.2)\\\\x95 An overview of the IGT enrichment algorithm (asdiscussed in Section 2.3).\\\\x95 A presentation of ODIN\\\\x92s search facility andthe results that can be returned, in partic-ular language profiles (as discussed in Sec-tion 3.3-3.4). ODIN\\\\x92s current website ishttp://uakari.ling.washington.edu/odin. Userscan also search ODIN using the OLAC7 searchinterfaces at the LDC8 and LinguistList.9 Somesearch examples are given below.4.1 Example 1: Search by Language NameThe opening screen for ODIN allows the user to searchthe ODIN database by clicking a specific languagename in the left-hand frame, or by typing all or partof a name (finding closest matches). Once a languageis selected, our search tool will list all the documentsthat have data for the language in question. The usercan then click on any of those documents, and searchtool will return the IGT instances found in those doc-uments. Following linguistic custom and fair use re-strictions, only instances of data that have citations aredisplayed. An example is shown in Figure 1. Search bylanguage and name is by far the most popular search inODIN, given the hundreds of queries executed per day.4.2 Example 2: Search by LinguisticConstructionsThis type of query looks either at enriched data in theEnglish translation, or at the projected structures in the7Open Language Archives Community8http://www.language-archives.org/tools/search/9LinguistList has graciously offered to host ODIN, and itis being migrated to http://odin.linguistlist.org. Completionof this migration is expected sometime in April 2009.43Figure 1: IGT instances in a documenttarget language data. Figure 2 shows the list of linguis-tic constructions that are currently covered.Suppose the user clicks on \\\\x93Word Order: VSO\\\\x94,the search tool will retrieve all the languages in ODINthat have VSO order according to the PCFGs extractedfrom the projected phrase structures (Figure 3). Theuser can then click on the Data link for any language inthe list to retrieve the IGT instances in that language.Figure 2: List of linguistic constructions that are cur-rently supported5 ConclusionIn this paper, we briefly discussed our work on im-proving the ODIN system, testing the usefulness ofthe ODIN data for linguistic study, and enhancing thesearch facility. While IGT data collected off the Web isinherently noisy, we show that even a sample size of 40IGT instances is large enough to ensure 99% accuracyin predicting Word Order. In the future, we plan to con-tinue our efforts to collect more data for ODIN, in orderto make it a more useful resource to the linguistic andcomputational linguistic audiences. Likewise, we willFigure 3: Languages in ODIN Determined to be VSOfurther extend the search interface to allow more so-phisticated queries that tap the full breadth of languagesthat exist in ODIN, and give users greater access to theenriched annotations and projected structures that canbe found only in ODIN.ReferencesJohn Frederick Bailyn. 2001. Inversion, Dislocation and Op-tionality in Russian. In Gerhild Zybatow, editor, CurrentIssues in Formal Slavic Linguistics.W. B. Cavnar and J. M. Trenkle. 1994. N-gram-based textcategorization. In Proceedings of Third Annual Sympo-sium on Document Analysis and Information Retrieval,pages 161\\\\x96175, Las Vegas, April.Joseph H. Greenberg. 1963. Some universals of grammarwith particular reference to the order of meaningful el-ements. In Joseph H. Greenberg, editor, Universals ofLanguage, pages 73\\\\x96113. MIT Press, Cambridge, Mas-sachusetts.William D. Lewis and Fei Xia. 2008. Automatically Identi-fying Computationally Relevant Typological Features. InProceedings of The Third International Joint Conferenceon Natural Language Processing (IJCNLP), Hyderabad,January.William D. Lewis. 2006. ODIN: A Model for Adapting andEnriching Legacy Infrastructure. In Proceedings of the e-Humanities Workshop, Amsterdam. Held in cooperationwith e-Science 2006: 2nd IEEE International Conferenceon e-Science and Grid Computing.Fei Xia and William D. Lewis. 2007. Multilingual struc-tural projection across interlinearized text. In Proceedingsof the North American Association of Computational Lin-guistics (NAACL) conference.Fei Xia and William D. Lewis. 2008. Repurposing Theoret-ical Linguistic Data for Tool Development and Search. InProceedings of The Third International Joint Conferenceon Natural Language Processing (IJCNLP), Hyderabad,January.Fei Xia, William D. Lewis, and Hoifung Poon. 2009. Lan-guage ID in the Context of Harvesting Language Data offthe Web. In Proceedings of The 12th Conference of the Eu-ropean Chapter of the Association of Computational Lin-guistics (EACL), Athens, Greece, April.44'\n",
            "b'Proceedings of the 10th Conference on Parsing Technologies, pages 121\\\\x96132,Prague, Czech Republic, June 2007. c\\\\xa92007 Association for Computational LinguisticsOn the Complexity of Non-Projective Data-Driven Dependency ParsingRyan McDonaldGoogle Inc.76 Ninth AvenueNew York, NY 10028ryanmcd@google.comGiorgio SattaUniversity of Paduavia Gradenigo 6/AI-35131 Padova, Italysatta@dei.unipd.itAbstractIn this paper we investigate several non-projective parsing algorithms for depen-dency parsing, providing novel polynomialtime solutions under the assumption thateach dependency decision is independent ofall the others, called here the edge-factoredmodel. We also investigate algorithms fornon-projective parsing that account for non-local information, and present several hard-ness results. This suggests that it is unlikelythat exact non-projective dependency pars-ing is tractable for any model richer than theedge-factored model.1 IntroductionDependency representations of natural language area simple yet flexible mechanism for encoding wordsand their syntactic dependencies through directedgraphs. These representations have been thoroughlystudied in descriptive linguistics (Tesnie`re, 1959;Hudson, 1984; Sgall et al., 1986; Mel\\\\xb4c?uk, 1988) andhave been applied in numerous language process-ing tasks. Figure 1 gives an example dependencygraph for the sentence Mr. Tomash will remain as adirector emeritus, which has been extracted from thePenn Treebank (Marcus et al., 1993). Each edge inthis graph represents a single syntactic dependencydirected from a word to its modifier. In this rep-resentation all edges are labeled with the specificsyntactic function of the dependency, e.g., SBJ forsubject and NMOD for modifier of a noun. To sim-plify computation and some important definitions,an artificial token is inserted into the sentence as theleft most word and will always represent the root ofthe dependency graph. We assume all dependencygraphs are directed trees originating out of a singlenode, which is a common constraint (Nivre, 2005).The dependency graph in Figure 1 is an exam-ple of a nested or projective graph. Under the as-sumption that the root of the graph is the left mostword of the sentence, a projective graph is one wherethe edges can be drawn in the plane above the sen-tence with no two edges crossing. Conversely, anon-projective dependency graph does not satisfythis property. Figure 2 gives an example of a non-projective graph for a sentence that has also beenextracted from the Penn Treebank. Non-projectivityarises due to long distance dependencies or in lan-guages with flexible word order. For many lan-guages, a significant portion of sentences requirea non-projective dependency analysis (Buchholz etal., 2006). Thus, the ability to learn and infer non-projective dependency graphs is an important prob-lem in multilingual language processing.Syntactic dependency parsing has seen a num-ber of new learning and inference algorithms whichhave raised state-of-the-art parsing accuracies formany languages. In this work we focus on data-drivenmodels of dependency parsing. These modelsare not driven by any underlying grammar, but in-stead learn to predict dependency graphs based ona set of parameters learned solely from a labeledcorpus. The advantage of these models is that theynegate the need for the development of grammarswhen adapting the model to new languages.One interesting class of data-driven models are121Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees. Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al.,2005a). Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al., 2005b). The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and neighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and Pereira, 2006). How-ever, in the data-driven parsing setting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al., 2005a).The goal of this work is to further our currentunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting. We start byinvestigating and extending the edge-factored modelof McDonald et al. (2005b). In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence. To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm \\\\x96 none of which havepreviously been known to have exact non-projectiveimplementations.We then switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions. For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case. For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods. A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related WorkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al.,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No\\\\xb4va\\\\xb4k, 2005; McDon-ald et al., 2005b). These approaches can often beclassified into two broad categories. In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorithms (Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005). In the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, asis the case for edge-factored models (Paskin, 2001;McDonald et al., 2005a; McDonald et al., 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004). Theoretical studies ofnote include the work of Neuhaus and Bo\\\\xa8ker (1997)showing that the recognition problem for a mini-122mal dependency grammar is hard. In addition, thework of Kahane et al. (1998) provides a polynomialparsing algorithm for a constrained class of non-projective structures. Non-projective dependencyparsing can be related to certain parsing problemsdefined for phrase structure representations, as forinstance immediate dominance CFG parsing (Bartonet al., 1987) and shake-and-bake translation (Brew,1992).Independently of this work, Koo et al. (2007) andSmith and Smith (2007) showed that the Matrix-Tree Theorem can be used to train edge-factoredlog-linear models of dependency parsing. Both stud-ies constructed implementations that compare favor-ably with the state-of-the-art. The work of Meila?and Jaakkola (2000) is also of note. In that studythey use the Matrix Tree Theorem to develop atractable bayesian learning algorithms for tree beliefnetworks, which in many ways are closely relatedto probabilistic dependency parsing formalisms andthe problems we address here.2 PreliminariesLet L = {l1, . . . , l|L|} be a set of permissible syn-tactic edge labels and x = x0x1 \\\\xb7 \\\\xb7 \\\\xb7xn be a sen-tence such that x0=root. From this sentence we con-struct a complete labeled directed graph (digraph)Gx = (Vx, Ex) such that,\\\\x95 Vx = {0, 1, . . . , n}\\\\x95 Ex = {(i, j)k | ? i, j ? Vx and 1 ? k ? |L|}Gx is a graph where each word in the sentence is anode, and there is a directed edge between every pairof nodes for every possible label. By its definition,Gx is a multi-digraph, which is a digraph that mayhave more than one edge between any two nodes.Let (i, j)k represent the kth edge from i to j. Gx en-codes all possible labeled dependencies between thewords of x. Thus every possible dependency graphof x must be a subgraph of Gx.Let i ?+ j be a relation that is true if and onlyif there is a non-empty directed path from node i tonode j in some graph under consideration. A di-rected spanning tree1 of a graph G, that originates1A directed spanning tree is commonly referred to as a ar-borescence in the graph theory literature.out of node 0, is any subgraph T = (VT , ET ) suchthat,\\\\x95 VT = Vx and ET ? Ex\\\\x95 ?j ? VT , 0 ?+ j if and only if j 6= 0\\\\x95 If (i, j)k ? ET , then (i?, j)k?/? ET , ?i? 6= iand/or k? 6= k.Define T (G) as the set of all directed spanning treesfor a graph G. As McDonald et al. (2005b) noted,there is a one-to-one correspondence between span-ning trees of Gx and labeled dependency graphsof x, i.e., T (Gx) is exactly the set of all possibleprojective and non-projective dependency graphs forsentence x. Throughout the rest of this paper, wewill refer to any T ? T (Gx) as a valid dependencygraph for a sentence x. Thus, by definition, everyvalid dependency graph must be a tree.3 Edge-factored ModelsIn this section we examine the class of models thatassume each dependency decision is independent.Within this setting, every edge in an induced graphGx for a sentence x will have an associated weightwkij ? 0 that maps the kth directed edge from nodei to node j to a real valued numerical weight. Theseweights represents the likelihood of a dependencyoccurring from word wi to word wj with label lk.Define the weight of a spanning tree T = (VT , ET )as the product of the edge weightsw(T ) =?(i,j)k?ETwkijIt is easily shown that this formulation includesthe projective model of Paskin (2001) and the non-projective model of McDonald et al. (2005b).The definition of wkij depends on the context inwhich it is being used. For example, in the work ofMcDonald et al. (2005b) it is simply a linear classi-fier that is a function of the words in the dependency,the label of the dependency, and any contextual fea-tures of the words in the sentence. In a generativeprobabilistic model (such as Paskin (2001)) it couldrepresent the conditional probability of a word wjbeing generated with a label lk given that the wordbeing modified is wi (possibly with some other in-formation such as the orientation of the dependency123or the number of words betweenwi andwj). We willattempt to make any assumptions about the formwkijclear when necessary.For the remainder of this section we discuss threecrucial problems for learning and inference whileshowing that each can be computed tractably for thenon-projective case.3.1 Finding the ArgmaxThe first problem of interest is finding the highestweighted tree for a given input sentence xT = argmaxT?T (Gx)?(i,j)k?ETwkijMcDonald et al. (2005b) showed that this can besolved in O(n2) for unlabeled parsing using theChu-Liu-Edmonds algorithm for standard digraphs(Chu and Liu, 1965; Edmonds, 1967). Unlike mostexact projective parsing algorithms, which use effi-cient bottom-up chart parsing algorithms, the Chu-Liu-Edmonds algorithm is greedy in nature. It be-gins by selecting the single best incoming depen-dency edge for each node j. It then post-processesthe resulting graph to eliminate cycles and then con-tinues recursively until a spanning tree (or validdependency graph) results (see McDonald et al.(2005b) for details).The algorithm is trivially extended to the multi-digraph case for use in labeled dependency parsing.First we note that if the maximum directed spanningtree of a multi-digraph Gx contains any edge (i, j)k,then we must have k = k? = argmaxk wkij . Oth-erwise we could simply substitute (i, j)k?in placeof (i, j)k and obtain a higher weighted tree. There-fore, without effecting the solution to the argmaxproblem, we can delete all edges in Gx that do notsatisfy this property. The resulting digraph is nolonger a multi-digraph and the Chu-Liu-Edmondsalgorithm can be applied directly. The new runtimeis O(|L|n2).As a side note, the k-best argmax problem for di-graphs can be solved in O(kn2) (Camerini et al.,1980). This can also be easily extended to the multi-digraph case for labeled parsing.3.2 Partition FunctionA common step in many learning algorithms is tocompute the sum over the weight of all the possi-ble outputs for a given input x. This value is oftenreferred to as the partition function due to its sim-ilarity with a value by the same name in statisticalmechanics. We denote this value as Zx,Zx =?T?T (Gx)w(T ) =?T?T (Gx)?(i,j)k?ETwki,jTo compute this sum it is possible to use the MatrixTree Theorem for multi-digraphs,Matrix Tree Theorem (Tutte, 1984): Let G be amulti-digraph with nodes V = {0, 1, . . . , n} andedges E. Define (Laplacian) matrix Q as a (n +1)\\\\xd7(n+1) matrix indexed from 0 to n. For all i andj, define:Qjj =?i6=j,(i,j)k?Ewkij & Qij =?i6=j,(i,j)k?E?wkijIf the ith row and column are removed from Q toproduce the matrixQi, then the sum of the weights ofall directed spanning trees rooted at node i is equalto |Qi| (the determinant of Qi).Thus, if we construct Q for a graph Gx, then the de-terminant of the matrix Q0 is equivalent to Zx. Thedeterminant of an n\\\\xd7n matrix can be calculated innumerous ways, most of which takeO(n3) (Cormenet al., 1990). The most efficient algorithms for cal-culating the determinant of a matrix use the fact thatthe problem is no harder than matrix multiplication(Cormen et al., 1990). Matrix multiplication cur-rently has known O(n2.38) implementations and ithas been widely conjectured that it can be solved inO(n2) (Robinson, 2005). However, most algorithmswith sub-O(n3) running times require constants thatare large enough to negate any asymptotic advantagefor the case of dependency parsing. As a result, inthis work we use O(n3) as the runtime for comput-ing Zx.Since it takes O(|L|n2) to construct the matrix Q,the entire runtime to compute Zx is O(n3 + |L|n2).3.3 Edge ExpectationsAnother important problem for various learningparadigms is to calculate the expected value of eachedge for an input sentence x,?(i, j)k?x =?T?T (Gx)w(T )\\\\xd7 I((i, j)k, T )124Input: x = x0x1 \\\\xb7 \\\\xb7 \\\\xb7xn1. Construct Q O(|L|n2)2. for j : 1 .. n O(n)3. Q?jj = Qjj and Q?ij = Qij , 0 ? ?i ? n O(n)4. Qjj = 1 and Qij = 0, 0 ? ?i ? n O(n)5. for i : 0 .. n & i 6= j O(n)6. Qij = ?1 O(1)7. Zx = |Q0| O(n3)8. ?(i, j)k?x = wkijZx, ?1 ? k ? |L| O(|L|)9. end for10. Qjj = Q?jj and Qij = Q?ij , 0 ? ?i ? n O(n)11. end forFigure 3: Algorithm to calculate ?(i, j)k?x inO(n5 + |L|n2).where I((i, j)k, T ) is an indicator function that isone when the edge (i, j)k is in the tree T .To calculate the expectation for the edge (i, j)k,we can simply eliminate all edges (i?, j)k?6= (i, j)kfrom Gx and calculate Zx. Zx will now be equalto the sum of the weights of all trees that con-tain (i, j)k. A naive implementation to computethe expectation of all |L|n2 edges takes O(|L|n5 +|L|2n4), since calculating Zx takes O(n3 + |L|n2)for a single edge. However, we can reduce this con-siderably by constructing Q a single time and onlymaking modifications to it when necessary. An al-gorithm is given in Figure 3.3 that has a runtime ofO(n5 + |L|n2). This algorithm works by first con-structing Q. It then considers edges from the node ito the node j. Now, assume that there is only a singleedge from i to j and that that edge has a weight of 1.Furthermore assume that this edge is the only edgedirected into the node j. In this case Q should bemodified so that Qjj = 1, Qij = ?1, and Qi?j = 0,?i? 6= i, j (by the Matrix Tree Theorem). The valueof Zx under this new Q will be equivalent to theweight of all trees containing the single edge from ito j with a weight of 1. For a specific edge (i, j)k itsexpectation is simplywkijZx, since we can factor outthe weight 1 edge from i to j in all the trees that con-tribute to Zx and multiply through the actual weightfor the edge. The algorithm then reconstructs Q andcontinues.Following the work of Koo et al. (2007) and Smithand Smith (2007), it is possible to compute all ex-pectations in O(n3 + |L|n2) through matrix inver-sion. To make this paper self contained, we reporthere their algorithm adapted to our notation. First,consider the equivalence,? logZx?wkij=? logZx?Zx?Zx?wkij=1Zx?T?T (Gx)w(T )wkij\\\\xd7 I((i, j)k, T )As a result, we can re-write the edge expectations as,?(i, j)k? = Zxwkij? logZx?wkij= Zxwkij? log |Q0|?wkijUsing the chain rule, we get,? log |Q0|?wkij=?i?,j??1? log |Q0|?(Q0)i?j??(Q0)i?j??wkijWe assume the rows and columns of Q0 are in-dexed from 1 so that the indexes of Q and Q0 co-incide. To calculate ?(i, j)k? when i, j > 0, we canuse the fact that ? log |X|/Xij = (X?1)ji and that?(Q0)i?j?/?wkij is non zero only when i? = i andj? = j or i? = j? = j to get,?(i, j)k? = Zxwkij [((Q0)?1)jj ? ((Q0)?1)ji]When i = 0 and j > 0 the only non zero term ofthis sum is when i? = j? = j and so?(0, j)k? = Zxwk0j((Q0)?1)jjZx and (Q0)?1 can both be calculated a single time,each taking O(n3). Using these values, each expec-tation is computed in O(1). Coupled with with thefact that we need to construct Q and compute theexpectation for all |L|n2 possible edges, in total ittakes O(n3 + |L|n2) time to compute all edge ex-pectations.3.4 Comparison with Projective ParsingProjective dependency parsing algorithms are wellunderstood due to their close connection to phrase-based chart parsing algorithms. The work of Eis-ner (1996) showed that the argmax problem for di-graphs could be solved in O(n3) using a bottom-up dynamic programming algorithm similar to CKY.Paskin (2001) presented an O(n3) inside-outside al-gorithm for projective dependency parsing using theEisner algorithm as its backbone. Using this al-gorithm it is trivial to calculate both Zx and each125Projective Non-Projectiveargmax O(n3 + |L|n2) O(|L|n2)Zx O(n3 + |L|n2) O(n3 + |L|n2)?(i, j)k?x O(n3 + |L|n2) O(n3 + |L|n2)Table 1: Comparison of runtime for non-projectiveand projective algorithms.edge expectation. Crucially, the nested property ofprojective structures allows edge expectations to becomputed in O(n3) from the inside-outside values.It is straight-forward to extend the algorithms of Eis-ner (1996) and Paskin (2001) to the labeled caseadding only a factor of O(|L|n2).Table 1 gives an overview of the computationalcomplexity for the three problems considered herefor both the projective and non-projective case. Wesee that the non-projective case compares favorablyfor all three problems.4 ApplicationsTo motivate the algorithms from Section 3, wepresent some important situations where each cal-culation is required.4.1 Inference Based LearningMany learning paradigms can be defined asinference-based learning. These include the per-ceptron (Collins, 2002) and its large-margin vari-ants (Crammer and Singer, 2003; McDonald et al.,2005a). In these settings, a models parameters areiteratively updated based on the argmax calculationfor a single or set of training instances under thecurrent parameter settings. The work of McDon-ald et al. (2005b) showed that it is possible to learna highly accurate non-projective dependency parserfor multiple languages using the Chu-Liu-Edmondsalgorithm for unlabeled parsing.4.2 Non-Projective Min-Risk DecodingIn min-risk decoding the goal is to find the depen-dency graph for an input sentence x, that on averagehas the lowest expected risk,T = argminT?T (Gx)?T ??T (Gx)w(T ?)R(T, T ?)where R is a risk function measuring the error be-tween two graphs. Min-risk decoding has beenstudied for both phrase-structure parsing and depen-dency parsing (Titov and Henderson, 2006). In thatwork, as is common with many min-risk decodingschemes, T (Gx) is not the entire space of parsestructures. Instead, this set is usually restricted toa small number of possible trees that have been pre-selected by some baseline system. In this subsectionwe show that when the risk function is of a specificform, this restriction can be dropped. The result isan exact min-risk decoding procedure.Let R(T, T ?) be the Hamming distance betweentwo dependency graphs for an input sentence x =x0x1 \\\\xb7 \\\\xb7 \\\\xb7xn,R(T, T ?) = n ??(i,j)k?ETI((i, j)k, T ?)This is a common definition of risk between twographs as it corresponds directly to labeled depen-dency parsing accuracy (McDonald et al., 2005a;Buchholz et al., 2006). Some algebra reveals,T = argminT?T (Gx)XT ??T (Gx)w(T ?)R(T, T ?)= argminT?T (Gx)XT ??T (Gx)w(T ?)[n ?X(i,j)k?ETI((i, j)k, T ?)]= argminT?T (Gx)?XT ??T (Gx)w(T ?)X(i,j)k?ETI((i, j)k, T ?)= argminT?T (Gx)?X(i,j)k?ETXT ??T (Gx)w(T ?)I((i, j)k, T ?)= argmaxT?T (Gx)X(i,j)k?ETXT ??T (Gx)w(T ?)I((i, j)k, T ?)= argmaxT?T (Gx)Y(i,j)k?ETePT ??T (Gx)w(T ?)I((i,j)k,T ?)= argmaxT?T (Gx)Y(i,j)k?ETe?(i,j)k?xBy setting the edge weights to wkij = e?(i,j)k?x wecan directly solve this problem using the edge ex-pectation algorithm described in Section 3.3 and theargmax algorithm described in Section 3.1.4.3 Non-Projective Log-Linear ModelsConditional Random Fields (CRFs) (Lafferty et al.,2001) are global discriminative learning algorithmsfor problems with structured output spaces, such asdependency parsing. For dependency parsing, CRFswould define the conditional probability of a depen-dency graph T for a sentence x as a globally nor-126malized log-linear model,p(T |x) =?(i,j)k?ETew\\\\xb7f(i,j,k)?T ??T (Gx)?(i,j)k?ET ?ew\\\\xb7f(i,j,k)=?(i,j)k?ETwkij?T ??T (Gx)?(i,j)k?ET ?wkij=w(T )ZxHere, the weights wkij are potential functions overeach edge defined as an exponentiated linear classi-fier with weight vector w ? RN and feature vectorf(i, j, k) ? RN , where fu(i, j, k) ? R represents asingle dimension of the vector f. The denominator,which is exactly the sum over all graph weights, is anormalization constant forcing the conditional prob-ability distribution to sum to one.CRFs set the parameters w to maximize the log-likelihood of the conditional probability over a train-ing set of examples T = {(x?, T?)}|T |?=1,w = argmaxw??log p(T?|x?)This optimization can be solved through a vari-ety of iterative gradient based techniques. Manyof these require the calculation of feature expecta-tions over the training set under model parametersfor the previous iteration. First, we note that thefeature functions factor over edges, i.e., fu(T ) =?(i,j)k?ETfu(i, j, k). Because of this, we can useedge expectations to compute the expectation of ev-ery feature fu. Let ?fu?x? represent the expectationof feature fu for the training instance x?,?fu?x? =XT?T (Gx? )p(T |x?)fu(T )=XT?T (Gx? )p(T |x?)X(i,j)k?ETfu(i, j, k)=XT?T (Gx? )w(T )ZxX(i,j)k?ETfu(i, j, k)=1ZxX(i,j)k?Ex?XT?T (Gx)w(T )I((i, j)k, T )fu(i, j, k)=1ZxX(i,j)k?Ex??(i, j)k?x?fu(i, j, k)Thus, we can calculate the feature expectation pertraining instance using the algorithms for comput-ing Zx and edge expectations. Using this, we cancalculate feature expectations over the entire train-ing set,?fu?T =??p(x?)?fu?x?where p(x?) is typically set to 1/|T |.4.4 Non-projective Generative Parsing ModelsA generative probabilistic dependency model oversome alphabet ? consists of parameters pkx,y asso-ciated with each dependency from word x ? ? toword y ? ? with label lk ? L. In addition, we im-pose 0 ? pkx,y ? 1 and the normalization conditions?y,k pkx,y = 1 for each x ? ?. We define a gen-erative probability model p over trees T ? T (Gx)and a sentence x = x0x1 \\\\xb7 \\\\xb7 \\\\xb7xn conditioned on thesentence length, which is always known,p(x, T |n) = p(x|T, n)p(T |n)=?(i,j)k?ETpkxi,xj p(T |n)We assume that p(T |n) = ? is uniform. This modelis studied specifically by Paskin (2001). In thismodel, one can view the sentence as being generatedrecursively in a top-down process. First, a tree isgenerated from the distribution p(T |n). Then start-ing at the root of the tree, every word generates all ofits modifiers independently in a recursive breadth-first manner. Thus, pkx,y represents the probabilityof the word x generating its modifier y with labellk. This distribution is usually smoothed and is of-ten conditioned on more information including theorientation of x relative to y (i.e., to the left/right)and distance between the two words. In the super-vised setting this model can be trained with maxi-mum likelihood estimation, which amounts to sim-ple counts over the data. Learning in the unsuper-vised setting requires EM and is discussed in Sec-tion 4.4.2.Another generative dependency model of interestis that given by Klein and Manning (2004). In thismodel the sentence and tree are generated jointly,which allows one to drop the assumption that p(T |n)is uniform. This requires the addition to the modelof parameters px,STOP for each x ? ?, with the nor-malization condition px,STOP +?y,k pkx,y = 1. It ispossible to extend the model of Klein and Manning127(2004) to the non-projective case. However, the re-sulting distribution will be over multisets of wordsfrom the alphabet instead of strings. The discus-sion in this section is stated for the model in Paskin(2001); a similar treatment can be developed for themodel in Klein and Manning (2004).4.4.1 Language ModelingA generative model of dependency structuremight be used to determine the probability of a sen-tence x by marginalizing out all possible depen-dency trees,p(x|n) =?T?T (Gx)p(x, T |n)=?T?T (Gx)p(x|T, n)p(T |n)= ??T?T (Gx)?(i,j)k?ETpkxi,xj = ?ZxThis probability can be used directly as a non-projective syntactic language model (Chelba et al.,1997) or possibly interpolated with a separate n-gram model.4.4.2 Unsupervised LearningIn unsupervised learning we train our model ona sample of unannotated sentences X = {x?}|X |?=1.Let |x?| = n? and p(T |n?) = ??. We choose theparameters that maximize the log-likelihood|X |??=1log(p(x?|n?)) ==|X |??=1log(?T?T (Gx? )p(x?|T, n?)) +|X |??=1log(??),viewed as a function of the parameters and subjectto the normalization conditions, i.e.,?y,k pkx,y = 1and pkx,y ? 0.Let x?i be the ith word of x?. By solving theabove constrained optimization problem with theusual Lagrange multipliers method one getspkx,y ==?|X |?=11Zx??i : x?i = x,j : x?j = y?(i, j)k?x??|X |?=11Zx??y?,k??i : x?i = x,j? : x?j? = y??(i, j?)k??x?,where for each x? the expectation ?(i, j)k?x? is de-fined as in Section 3, but with the weight w(T ) re-placed by the probability distribution p(x?|T, n?).The above |L| \\\\xb7 |?|2 relations represent a non-linear system of equations. There is no closed formsolution in the general case, and one adopts the ex-pectation maximization (EM) method, which is aspecialization of the standard fixed-point iterationmethod for the solution of non-linear systems. Westart with some initial assignment of the parametersand at each iteration we use the induced distribu-tion p(x?|T, n?) to compute a refined value for theparameters themselves. We are always guaranteedthat the Kullback-Liebler divergence between twoapproximated distributions computed at successiveiterations does not increase, which implies the con-vergence of the method to some local maxima (withthe exception of saddle points).Observe that at each iteration we can computequantities ?(i, j)k?x? and Zx? in polynomial timeusing the algorithms from Section 3 with pkx?i,x?jin place of wki,j . Furthermore, under some standardconditions the fixed-point iteration method guaran-tees a constant number of bits of precision gain forthe parameters at each iteration, resulting in overallpolynomial time computation in the size of the inputand in the required number of bits for the precision.As far as we know, this is the first EM learning algo-rithm for the model in Paskin (2001) working in thenon-projective case. The projective case has beeninvestigated in Paskin (2001).5 Beyond Edge-factored ModelsWe have shown that several computational problemsrelated to parsing can be solved in polynomial timefor the class of non-projective dependency modelswith the assumption that dependency relations aremutually independent. These independence assump-tions are unwarranted, as it has already been estab-lished that modeling non-local information such asarity and nearby parsing decisions improves the ac-curacy of dependency models (Klein and Manning,2002; McDonald and Pereira, 2006).In the spirit of our effort to understand the natureof exact non-projective algorithms, we examine de-pendency models that introduce arity constraints aswell as permit edge decisions to be dependent on a128limited neighbourhood of other edges in the graph.Both kinds of models can no longer be considerededge-factored, since the likelihood of a dependencyoccurring in a particular analysis is now dependenton properties beyond the edge itself.5.1 ArityOne feature of the edge-factored models is that norestriction is imposed on the arity of the nodes in thedependency trees. As a consequence, these modelscan generate dependency trees of unbounded arity.We show below that this is a crucial feature in thedevelopment of the complexity results we have ob-tained in the previous sections.Let us assume a graph G(?)x = (Vx, Ex) definedas before, but with the additional condition that eachnode i ? Vx is associated with an integer value?(i) ? 0. T (G(?)x ) is now defined as the set of alldirected spanning trees for G(?)x rooted in node 0,such that every node i ? Vx has arity smaller than orequal to ?(i). We now introduce a construction thatwill be used to establish several hardness results forthe computational problems discussed in this paper.Recall that a Hamiltonian path in a directed graphG is a directed path that visits all of the nodes of Gexactly once.Let G be some directed graph with set of nodesV = {1, 2, . . . , n}. We construct a target graphG(?)x = (Vx, Ex) with Vx = V ? {0} (0 the rootnode) and |L| = 1. For each i, j ? Vx with i 6= j,we add an edge (i, j)1 to Ex. We set w1i,j = 1 ifthere is an edge from i to j in G, or else if i or jis the root node 0, and w1i,j = 0 otherwise. Fur-thermore, we set ?(i) = 1 for each i ? Vx. Thisconstruction can be clearly carried out in log-space.Note that each T ? T (G(?)x ) must be a monadictree with weight equal to either 0 or 1. It is not dif-ficult to see that if w(T ) = 1, then when we removethe root node 0 from T we obtain a Hamiltonian pathin G. Conversely, each Hamiltonian path in G canbe extended to a spanning tree T ? T (G(?)x ) withw(T ) = 1, by adding the root node 0.Using the above observations, it can be shown thatthe solution of the argmax problem for G(?)x pro-vides some Hamiltonian directed path in G. The lat-ter search problem is FNP-hard, and is unlikely tobe solved in polynomial time. Furthermore, quan-tity Zx provides the count of the Hamiltonian di-rected paths in G, and for each i ? V , the expecta-tion ?(0, i)1?x provides the count of the Hamiltoniandirected paths in G starting from node i. Both thesecounting problems are #P-hard, and very unlikely tohave polynomial time solutions.This result helps to relate the hardness of data-driven models to the commonly known hardnessresults in the grammar-driven literature given byNeuhaus and Bo\\\\xa8ker (1997). In that work, an arityconstraint is included in their minimal grammar.5.2 Vertical and Horizontal MarkovizationIn general, we would like to say that every depen-dency decision is dependent on every other edge ina graph. However, modeling dependency parsing insuch a manner would be a computational nightmare.Instead, we would like to make a Markov assump-tion over the edges of the tree, in a similar way thata Markov assumption can be made for sequentialclassification problems in order to ensure tractablelearning and inference.Klein and Manning (2003) distinguish betweentwo kinds of Markovization for unlexicalized CFGparsing. The first is vertical Markovization, whichmakes the generation of a non-terminal dependenton other non-terminals that have been generated atdifferent levels in the phrase-structure tree. Thesecond is horizontal Markovization, which makesthe generation of a non-terminal dependent on othernon-terminals that have been generated at the samelevel in the tree.For dependency parsing there are analogous no-tions of vertical and horizontal Markovization for agiven edge (i, j)k. First, let us define the vertical andhorizontal neighbourhoods of (i, j)k. The verticalneighbourhood includes all edges in any path fromthe root to a leaf that passes through (i, j)k. The hor-izontal neighbourhood contains all edges (i, j?)k?.Figure 4 graphically displays the vertical and hor-izontal neighbourhoods for an edge in the depen-dency graph from Figure 1.Vertical and horizontal Markovization essentiallyallow the score of the graph to factor over a largerscope of edges, provided those edges are in the samevertical or horizontal neighbourhood. A dth orderfactorization is one in which the score factors onlyover the d nearest edges in the neighbourhoods. In129Figure 4: Vertical and Horizontal neighbourhood forthe edge from will to remain.McDonald and Pereira (2006), it was shown thatnon-projective dependency parsing with horizontalMarkovization is FNP-hard. In this study we com-plete the picture and show that vertical Markoviza-tion is also FNP-hard.Consider a first-order vertical Markovization inwhich the score for a dependency graph factors overpairs of vertically adjacent edges2,w(T ) =?(h,i)k,(i,j)k??ETkhiwk?ijwhere khiwk?ij is the weight of including both edges(h, i)k and (i, j)k?in the dependency graph. Notethat this formulation does not include any contribu-tions from dependencies that have no vertically adja-cent neighbours, i.e., any edge (0, i)k such that thereis no edge (i, j)k?in the graph. We can easily rec-tify this by inserting a second root node, say 0?, andincluding the weights k0?0wk?0i . To ensure that onlyvalid dependency graphs get a weight greater thanzero, we can set khiwk?ij = 0 if i = 0? and k0?iwk?ij = 0if i 6= 0.Now, consider the NP-complete 3D-matchingproblem (3DM). As input we are given three setsof size m, call them A, B and C, and a set S ?A\\\\xd7B \\\\xd7C. The 3DM problem asks if there is a setS? ? S such that |S?| = m and for any two tuples(a, b, c), (a?, b?, c?) ? S? it is the case that a 6= a?,b 6= b?, and c 6= c?.2McDonald and Pereira (2006) define this as a second-orderMarkov assumption. This is simply a difference in terminologyand does not represent any meaningful distinction.We can reduce the 3D-matching problem to thefirst-order vertical Markov parsing problem by con-structing a graph G = (V,E), such that L =A ? B ? C, V = {0?, 0} ? A ? B ? C and E ={(i, j)k | i, j ? V, k ? L}. The set E contains mul-tiple edges between ever pair of nodes, each edgetaking on a label representing a single element ofthe set A ? B ? C. Now, define k0?0wk?0a = 1, for alla ? A and k, k? ? A ? B ? C, and b0awcab = 1, forall a ? A and b ? B and c ? C, and cabwcbc = 1, forall (a, b, c) ? S. All other weights are set to zero.We show below that there exists a bijection be-tween the set of valid 3DMs for S and the set of non-zero weighted dependency graphs in T (G). First, itis easy to show that for any 3DM S?, there is a rep-resentative dependency graph that has a weight of1. This graph simply consists of the edges (0, a)b,(a, b)c, and (b, c)c, for all (a, b, c) ? S?, plus an ar-bitrarily labeled edge from 0? to 0.To prove the reverse, consider a graph with weight1. This graph must have a weight 1 edge into thenode a of the form (0, a)b since the graph must bespanning. By the definition of the weight function,in any non-zero weighted tree, a must have a sin-gle outgoing edge, and that edge must be directedinto the node b. Let\\\\x92s say that this edge is (a, b)c.Then again by the weighting function, in any non-zero weighted graph, b must have a single outgoingedge that is directed into c, in particular the edge(b, c)c. Thus, for any node a, there is a single pathdirected out of it to a single leaf c ? C. We canthen state that the only non-zero weighted depen-dency graph is one where each a ? A, b ? B andc ? C occurs in exactly one ofm disjoint paths fromthe root of the form 0 ? a ? b ? c. This is be-cause the label of the single edge going into node awill determine exactly the node b that the one outgo-ing edge from a must go into. The label of that edgedetermines exactly the single outgoing edge from binto some node c. Now, since the weighting func-tion ensures that the only non-zero weighted pathsinto any leaf node c correspond directly to elementsof S, each of the m disjoint paths represent a singletuple in a 3DM. Thus, if there is a non-zero weightedgraph in T (G), then it must directly correspond to avalid 3DM, which concludes the proof.Note that any dth order Markovization can be em-bedded into a d + 1th Markovization. Thus, this re-130sult also holds for any arbitrary Markovization.6 DiscussionIn this paper we have shown that many importantlearning and inference problems can be solved effi-ciently for non-projective edge-factored dependencymodels by appealing to the Matrix Tree Theoremfor multi-digraphs. These results extend the workof McDonald et al. (2005b) and help to further ourunderstanding of when exact non-projective algo-rithms can be employed. When this analysis is cou-pled with the projective parsing algorithms of Eisner(1996) and Paskin (2001) we begin to get a clear pic-ture of the complexity for data-driven dependencyparsing within an edge-factored framework. To fur-ther justify the algorithms presented here, we out-lined a few novel learning and inference settings inwhich they are required.However, for the non-projective case, movingbeyond edge-factored models will almost certainlylead to intractable parsing problems. We have pro-vided further evidence for this by proving the hard-ness of incorporating arity constraints and hori-zontal/vertical edge Markovization, both of whichincorporate information unavailable to an edge-factored model. The hardness results providedhere are also of interest since both arity constraintsand Markovization can be incorporated efficientlyin the projective case through the straight-forwardaugmentation of the underlying chart parsing algo-rithms used in the projective edge-factored models.This highlights a fundamental difference betweenthe nature of projective parsing algorithms and non-projective parsing algorithms. On the projectiveside, all algorithms use a bottom-up chart parsingframework to search the space of nested construc-tions. On the non-projective side, algorithms areeither greedy-recursive in nature (i.e., the Chu-Liu-Edmonds algorithm) or based on the calculation ofthe determinant of a matrix (i.e., the partition func-tion and edge expectations).Thus, the existence of bottom-up chart parsingalgorithms for projective dependency parsing pro-vides many advantages. As mentioned above, itpermits simple augmentation techniques to incorpo-rate non-local information such as arity constraintsand Markovization. It also ensures the compatibilityof projective parsing algorithms with many impor-tant natural language processing methods that workwithin a bottom-up chart parsing framework, includ-ing information extraction (Miller et al., 2000) andsyntax-based machine translation (Wu, 1996).The complexity results given here suggest thatpolynomial chart-parsing algorithms do not existfor the non-projective case. Otherwise we shouldbe able to augment them and move beyond edge-factored models without encountering intractability\\\\x96 just like the projective case. An interesting lineof research is to investigate classes of non-projectivestructures that can be parsed with chart-parsing algo-rithms and how these classes relate to the languagesparsable by other syntactic formalisms.AcknowledgmentsThanks to Ben Taskar for pointing out the work ofMeila? and Jaakkola (2000). Thanks to David Smith,Noah Smith and Michael Collins for making draftsof their EMNLP papers available.ReferencesG. E. Barton, R. C. Berwick, and E. S. Ristad. 1987.Computational Complexity and Natural Language.MIT Press, Cambridge, MA.C. Brew. 1992. Letting the cat out of the bag: Generationfor Shake-and-Bake MT. In Proc. COLING.S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006. CoNLL-X shared task on multilingual depen-dency parsing. In Proc. CoNLL.P. M. Camerini, L. Fratta, and F. Maffioli. 1980. The kbest spanning arborescences of a network. Networks,10(2):91\\\\x96110.C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-pur, L. Mangu, H. Printz, E.S. Ristad, R. Rosenfeld,A. Stolcke, and D. Wu. 1997. Structure and per-formance of a dependency language model. In Eu-rospeech.Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-cence of a directed graph. Science Sinica, 14:1396\\\\x961400.M. Collins. 2002. Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms. In Proc. EMNLP.T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-troduction to Algorithms. MIT Press/McGraw-Hill.131K. Crammer and Y. Singer. 2003. Ultraconservative on-line algorithms for multiclass problems. JMLR.J. Edmonds. 1967. Optimum branchings. Journal of Re-search of the National Bureau of Standards, 71B:233\\\\x96240.J. Eisner. 1996. Three new probabilistic models for de-pendency parsing: An exploration. In Proc. COLING.K. Hall and V. No\\\\xb4va\\\\xb4k. 2005. Corrective modeling fornon-projective dependency parsing. In Proc. IWPT.H. Hirakawa. 2006. Graph branch algorithm: An opti-mum tree search method for scored dependency graphwith arc co-occurrence constraints. In Proc. ACL.R. Hudson. 1984. Word Grammar. Blackwell.S. Kahane, A. Nasr, and O Rambow. 1998. Pseudo-projectivity: A polynomially parsable non-projectivedependency grammar. In Proc. ACL.D. Klein and C.D. Manning. 2002. Fast exact natu-ral language parsing with a factored model. In Proc.NIPS.D. Klein and C. Manning. 2003. Accurate unlexicalizedparsing. In Proc. ACL.D. Klein and C. Manning. 2004. Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency. In Proc. ACL.T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.Structured prediction models via the matrix-tree theo-rem. In Proc. EMNLP.J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data. In Proc. ICML.M. Marcus, B. Santorini, and M. Marcinkiewicz.1993. Building a large annotated corpus of En-glish: The Penn Treebank. Computational Linguistics,19(2):313\\\\x96330.R. McDonald and F. Pereira. 2006. Online learning ofapproximate dependency parsing algorithms. In ProcEACL.R. McDonald, K. Crammer, and F. Pereira. 2005a. On-line large-margin training of dependency parsers. InProc. ACL.R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.Non-projective dependency parsing using spanningtree algorithms. In Proc. HLT/EMNLP.M. Meila? and T. Jaakkola. 2000. Tractable Bayesianlearning of tree belief networks. In Proc. UAI.I.A. Mel\\\\xb4c?uk. 1988. Dependency Syntax: Theory andPractice. State University of New York Press.S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.2000. A novel use of statistical parsing to extract in-formation from text. In Proc NAACL, pages 226\\\\x96233.P. Neuhaus and N. Bo\\\\xa8ker. 1997. The complexityof recognition of linguistically adequate dependencygrammars. In Proc. ACL.J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-dency parsing. In Proc. ACL.J. Nivre and M. Scholz. 2004. Deterministic dependencyparsing of english text. In Proc. COLING.J. Nivre. 2005. Dependency grammar and dependencyparsing. Technical Report MSI report 05133, Va\\\\xa8xjo\\\\xa8University: School of Mathematics and Systems Engi-neering.M.A. Paskin. 2001. Cubic-time parsing and learning al-gorithms for grammatical bigram models. TechnicalReport UCB/CSD-01-1148, Computer Science Divi-sion, University of California Berkeley.S. Riedel and J. Clarke. 2006. Incremental integer linearprogramming for non-projective dependency parsing.In Proc. EMNLP.S. Robinson. 2005. Toward an optimal algorithm formatrix multiplication. News Journal of the Society forIndustrial and Applied Mathematics, 38(9).P. Sgall, E. Hajic?ova\\\\xb4, and J. Panevova\\\\xb4. 1986. The Mean-ing of the Sentence in Its Pragmatic Aspects. Reidel.D.A. Smith and N.A. Smith. 2007. Probabilistic modelsof nonprojective dependency trees. In Proc. EMNLP.L. Tesnie`re. 1959. E\\\\xb4le\\\\xb4ments de syntaxe structurale. Edi-tions Klincksieck.I. Titov and J. Henderson. 2006. Bayes risk minimiza-tion in natural language parsing. University of Genevatechnical report.W.T. Tutte. 1984. Graph Theory. Cambridge UniversityPress.W. Wang and M. P. Harper. 2004. A statistical constraintdependency grammar (CDG) parser. In Workshop onIncremental Parsing: Bringing Engineering and Cog-nition Together (ACL).D. Wu. 1996. A polynomial-time algorithm for statisti-cal machine translation. In Proc. ACL.H. Yamada and Y. Matsumoto. 2003. Statistical depen-dency analysis with support vector machines. In Proc.IWPT.132'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-9b0595160a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtFiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3fce2e2b7f11>\u001b[0m in \u001b[0;36mtfidf\u001b[0;34m(docList)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtFiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#dicList=wordList(str(data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/encodings/unicode_escape.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_escape_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'unicodeescape' codec can't decode bytes in position 25387-25388: malformed \\N character escape"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mow2odeKHDAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct a plot that shows how IDF-K relation changes as base of logarithm changes from 10 to -1."
      ]
    },
    {
      "metadata": {
        "id": "U1K1OAcfHDAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### yout code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zxxcf1vWHDAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct a plot Term Frequency weight transformation such as this one\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "BPkMQK-gHDAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdhcmYmXHDAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### construct plot of BM25 as shown here\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "iI0UaMuSHDAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyesJy4EHDA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### write code for SVM and run the following querry, your must show the top 5 documents ranked according to the score.\n",
        "    1 Text Mining\n",
        "    2 LDA\n",
        "    3 topic modelling\n",
        "    4 Natural language Processing\n",
        "    5 generative models\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wTrhPWvGHDA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}